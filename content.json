{"meta":{"title":"Innovation Louke","subtitle":"Welcome Strangers","description":null,"author":"InnovationLou","url":"https://blog.innnovation.cn","root":"/"},"pages":[{"title":"404","date":"2021-06-08T02:59:13.000Z","updated":"2021-06-08T02:59:40.297Z","comments":true,"path":"404/index.html","permalink":"https://blog.innnovation.cn/404/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-06-07T08:11:58.671Z","updated":"2021-06-07T08:11:58.671Z","comments":true,"path":"about/index.html","permalink":"https://blog.innnovation.cn/about/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-06-07T08:09:54.964Z","updated":"2021-06-07T08:09:54.964Z","comments":true,"path":"categories/index.html","permalink":"https://blog.innnovation.cn/categories/index.html","excerpt":"","text":""},{"title":"相册","date":"2021-06-08T04:07:10.803Z","updated":"2021-06-08T04:07:10.803Z","comments":true,"path":"galleries/index.html","permalink":"https://blog.innnovation.cn/galleries/index.html","excerpt":"","text":"123123123test"},{"title":"movies","date":"2021-06-08T03:29:11.000Z","updated":"2021-06-08T03:29:11.453Z","comments":true,"path":"movies/index.html","permalink":"https://blog.innnovation.cn/movies/index.html","excerpt":"","text":""},{"title":"music","date":"2021-06-08T03:29:00.000Z","updated":"2021-06-08T03:29:00.579Z","comments":true,"path":"music/index.html","permalink":"https://blog.innnovation.cn/music/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-06-07T08:09:15.925Z","updated":"2021-06-07T08:09:15.925Z","comments":true,"path":"tags/index.html","permalink":"https://blog.innnovation.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"分布式事务","slug":"分布式事务","date":"2021-07-15T16:00:00.000Z","updated":"2021-07-16T06:53:16.881Z","comments":true,"path":"2021/07/16/fen-bu-shi-shi-wu/","link":"","permalink":"https://blog.innnovation.cn/2021/07/16/fen-bu-shi-shi-wu/","excerpt":"","text":"分布式事务1.什么是分布式事务要了解分布式事务，必须先了解本地事务。 1.1.本地事务本地事务，是指传统的单机数据库事务，必须具备ACID原则： 原子性（A） 所谓的原子性就是说，在整个事务中的所有操作，要么全部完成，要么全部不做，没有中间状态。对于事务在执行中发生错误，所有的操作都会被回滚，整个事务就像从没被执行过一样。 一致性（C） 事务的执行必须保证系统的一致性，在事务开始之前和事务结束以后，数据库的完整性没有被破坏，就拿转账为例，A有500元，B有500元，如果在一个事务里A成功转给B50元，那么不管发生什么，那么最后A账户和B账户的数据之和必须是1000元。 隔离性（I） 所谓的隔离性就是说，事务与事务之间不会互相影响，一个事务的中间状态不会被其他事务感知。数据库保证隔离性包括四种不同的隔离级别： ​ Read Uncommitted（读取未提交内容） ​ Read Committed（读取提交内容） ​ Repeatable Read（可重读） ​ Serializable（可串行化） 持久性（D） 所谓的持久性，就是说一旦事务提交了，那么事务对数据所做的变更就完全保存在了数据库中，即使发生停电，系统宕机也是如此。 因为在传统项目中，项目部署基本是单点式：即单个服务器和单个数据库。这种情况下，数据库本身的事务机制就能保证ACID的原则，这样的事务就是本地事务。 概括来讲，单个服务与单个数据库的架构中，产生的事务都是本地事务。 其中原子性和持久性就要靠undo和redo 日志来实现。 1.2.undo和redo本小节参考内容：mysqlops 在数据库系统中，既有存放数据的文件，也有存放日志的文件。日志在内存中也是有缓存Log buffer，也有磁盘文件log file。 MySQL中的日志文件，有这么两种与事务有关：undo日志与redo日志。 1.2.1.undo日志数据库事务具备原子性（Atomicity），如果事务执行失败，需要把数据回滚。 事务同时还具备持久性**(Durability)**，事务对数据所做的变更就完全保存在了数据库，不能因为故障而丢失。 原子性可以利用undo日志来实现。 Undo Log的原理很简单，为了满足事务的原子性，在操作任何数据之前，首先将数据备份到Undo Log。然后进行数据的修改。如果出现了错误或者用户执行了ROLLBACK语句，系统可以利用Undo Log中的备份将数据恢复到事务开始之前的状态。 数据库写入数据到磁盘之前，会把数据先缓存在内存中，事务提交时才会写入磁盘中。 用Undo Log实现原子性和持久化的事务的简化过程： 假设有A、B两个数据，值分别为1,2。 A. 事务开始. B. 记录A=1到undo log. C. 修改A=3. D. 记录B=2到undo log. E. 修改B=4. F. 将undo log写到磁盘。 G. 将数据写到磁盘。 H. 事务提交 如何保证持久性？ 事务提交前，会把修改数据到磁盘前，也就是说只要事务提交了，数据肯定持久化了。 如何保证原子性？ 每次对数据库修改，都会把修改前数据记录在undo log，那么需要回滚时，可以读取undo log，恢复数据。 若系统在G和H之间崩溃 此时事务并未提交，需要回滚。而undo log已经被持久化，可以根据undo log来恢复数据 若系统在G之前崩溃 此时数据并未持久化到硬盘，依然保持在事务之前的状态 缺陷：每个事务提交前将数据和Undo Log写入磁盘，这样会导致大量的磁盘IO，因此性能很低。 如果能够将数据缓存一段时间，就能减少IO提高性能。但是这样就会丧失事务的持久性。因此引入了另外一种机制来实现持久化，即Redo Log. 1.2.2.redo日志和Undo Log相反，Redo Log记录的是新数据的备份。在事务提交前，只要将Redo Log持久化即可，不需要将数据持久化，减少了IO的次数。 先来看下基本原理： Undo + Redo事务的简化过程 假设有A、B两个数据，值分别为1,2 A. 事务开始. B. 记录A=1到undo log buffer. C. 修改A=3. D. 记录A=3到redo log buffer. E. 记录B=2到undo log buffer. F. 修改B=4. G. 记录B=4到redo log buffer. H. 将undo log写入磁盘 I. 将redo log写入磁盘 J. 事务提交 安全和性能问题 如何保证原子性？ 如果在事务提交前故障，通过undo log日志恢复数据。如果undo log都还没写入，那么数据就尚未持久化，无需回滚 如何保证持久化？ 大家会发现，这里并没有出现数据的持久化。因为数据已经写入redo log，而redo log持久化到了硬盘，因此只要到了步骤I以后，事务是可以提交的。 内存中的数据库数据何时持久化到磁盘？ 因为redo log已经持久化，因此数据库数据写入磁盘与否影响不大，不过为了避免出现脏数据（内存中与磁盘不一致），事务提交后也会将内存数据刷入磁盘（也可以按照固设定的频率刷新内存数据到磁盘中）。 redo log何时写入磁盘 redo log会在事务提交之前，或者redo log buffer满了的时候写入磁盘 这里存在两个问题： 问题1：之前是写undo和数据库数据到硬盘，现在是写undo和redo到磁盘，似乎没有减少IO次数 数据库数据写入是随机IO，性能很差 redo log在初始化时会开辟一段连续的空间，写入是顺序IO，性能很好 实际上undo log并不是直接写入磁盘，而是先写入到redo log buffer中，当redo log持久化时，undo log就同时持久化到硬盘了。 因此事务提交前，只需要对redo log持久化即可。 另外，redo log并不是写入一次就持久化一次，redo log在内存中也有自己的缓冲池：redo log buffer。每次写redo log都是写入到buffer，在提交时一次性持久化到磁盘，减少IO次数。 问题2：redo log 数据是写入内存buffer中，当buffer满或者事务提交时，将buffer数据写入磁盘。 redo log中记录的数据，有可能包含尚未提交事务，如果此时数据库崩溃，那么如何完成数据恢复？ 数据恢复有两种策略： 恢复时，只重做已经提交了的事务 恢复时，重做所有事务包括未提交的事务和回滚了的事务。然后通过Undo Log回滚那些未提交的事务 Inodb引擎采用的是第二种方案，因此undo log要在 redo log前持久化 1.2.3.总结最后总结一下： undo log 记录更新前数据，用于保证事务原子性 redo log 记录更新后数据，用于保证事务的持久性 redo log有自己的内存buffer，先写入到buffer，事务提交时写入磁盘 redo log持久化之后，意味着事务是可提交的 1.3.分布式事务分布式事务，就是指不是在单个服务或单个数据库架构下，产生的事务： 跨数据源的分布式事务 跨服务的分布式事务 综合情况 1）跨数据源随着业务数据规模的快速发展，数据量越来越大，单库单表逐渐成为瓶颈。所以我们对数据库进行了水平拆分，将原单库单表拆分成数据库分片，于是就产生了跨数据库事务问题。 2）跨服务在业务发展初期，“一块大饼”的单业务系统架构，能满足基本的业务需求。但是随着业务的快速发展，系统的访问量和业务复杂程度都在快速增长，单系统架构逐渐成为业务发展瓶颈，解决业务系统的高耦合、可伸缩问题的需求越来越强烈。 如下图所示，按照面向服务（SOA）的架构的设计原则，将单业务系统拆分成多个业务系统，降低了各系统之间的耦合度，使不同的业务系统专注于自身业务，更有利于业务的发展和系统容量的伸缩。 3）分布式系统的数据一致性问题在数据库水平拆分、服务垂直拆分之后，一个业务操作通常要跨多个数据库、服务才能完成。在分布式网络环境下，我们无法保障所有服务、数据库都百分百可用，一定会出现部分服务、数据库执行成功，另一部分执行失败的问题。 当出现部分业务操作成功、部分业务操作失败时，业务数据就会出现不一致。 例如电商行业中比较常见的下单付款案例，包括下面几个行为： 创建新订单 扣减商品库存 从用户账户余额扣除金额 完成上面的操作需要访问三个不同的微服务和三个不同的数据库。 在分布式环境下，肯定会出现部分操作成功、部分操作失败的问题，比如：订单生成了，库存也扣减了，但是 用户账户的余额不足，这就造成数据不一致。 订单的创建、库存的扣减、账户扣款在每一个服务和数据库内是一个本地事务，可以保证ACID原则。 但是当我们把三件事情看做一个事情事，要满足保证“业务”的原子性，要么所有操作全部成功，要么全部失败，不允许出现部分成功部分失败的现象，这就是分布式系统下的事务了。 此时ACID难以满足，这是分布式事务要解决的问题 2.解决分布式事务的思路为什么分布式系统下，事务的ACID原则难以满足？ 这得从CAP定理和BASE理论说起。 2.1.CAP定理本小节内容摘自：CAP 定理的含义 什么是CAP定理呢？ 1998年，加州大学的计算机科学家 Eric Brewer 提出，分布式系统有三个指标。 Consistency（一致性） Availability（可用性） Partition tolerance （分区容错性） 它们的第一个字母分别是 C、A、P。 Eric Brewer 说，这三个指标不可能同时做到。这个结论就叫做 CAP 定理。 2.1.1.Partition tolerance先看 Partition tolerance，中文叫做”分区容错”。 大多数分布式系统都分布在多个子网络。每个子网络就叫做一个区（partition）。分区容错的意思是，区间通信可能失败。比如，一台服务器放在上海，另一台服务器放在北京，这就是两个区，它们之间可能因网络问题无法通信。 如图： 上图中，G1 和 G2 是两台跨区的服务器。G1 向 G2 发送一条消息，G2 可能无法收到。系统设计的时候，必须考虑到这种情况。 一般来说，分布式系统，分区容错无法避免，因此可以认为 CAP 的 P 总是成立。根据CAP 定理，剩下的 C 和 A 无法同时做到。 2.1.2.ConsistencyConsistency 中文叫做”一致性”。意思是，写操作之后的读操作，必须返回该值。举例来说，某条记录是 v0，用户向 G1 发起一个写操作，将其改为 v1。 接下来，用户的读操作就会得到 v1。这就叫一致性。 问题是，用户有可能向 G2 发起读操作，由于 G2 的值没有发生变化，因此返回的是 v0。G1 和 G2 读操作的结果不一致，这就不满足一致性了。 为了让 G2 也能变为 v1，就要在 G1 写操作的时候，让 G1 向 G2 发送一条消息，要求 G2 也改成 v1。 这样的话，用户向 G2 发起读操作，也能得到 v1。 2.1.3.Availability Availability 中文叫做”可用性”，意思是只要收到用户的请求，服务器就必须给出回应（对和错不论）。 用户可以选择向 G1 或 G2 发起读操作。不管是哪台服务器，只要收到请求，就必须告诉用户，到底是 v0 还是 v1，否则就不满足可用性。 2.1.4.Consistency 和 Availability 的矛盾一致性和可用性，为什么不可能同时成立？ 答案很简单，因为可能通信失败（即出现分区容错）。 如果保证 G2 的一致性，那么 G1 必须在写操作时，锁定 G2 的读操作和写操作。只有数据同步后，才能重新开放读写。锁定期间，G2 不能读写，没有可用性不。 如果保证 G2 的可用性，那么势必不能锁定 G2，所以一致性不成立。 综上所述，G2 无法同时做到一致性和可用性。系统设计时只能选择一个目标。如果追求一致性，那么无法保证所有节点的可用性；如果追求所有节点的可用性，那就没法做到一致性。 2.1.5.几点疑问 怎样才能同时满足CA？ 除非是单点架构 何时要满足CP？ 对一致性要求高的场景。例如我们的Zookeeper就是这样的，在服务节点间数据同步时，服务对外不可用。 何时满足AP？ 对可用性要求较高的场景。例如Eureka，必须保证注册中心随时可用，不然拉取不到服务就可能出问题。 2.2.Base理论BASE是三个单词的缩写： Basically Available（基本可用） Soft state（软状态） Eventually consistent（最终一致性） 而我们解决分布式事务，就是根据上述理论来实现。 还以上面的下单减库存和扣款为例： 订单服务、库存服务、用户服务及他们对应的数据库就是分布式应用中的三个部分。 CP方式：现在如果要满足事务的强一致性，就必须在订单服务数据库锁定的同时，对库存服务、用户服务数据资源同时锁定。等待三个服务业务全部处理完成，才可以释放资源。此时如果有其他请求想要操作被锁定的资源就会被阻塞，这样就是满足了CP。 这就是强一致，弱可用 AP方式：三个服务的对应数据库各自独立执行自己的业务，执行本地事务，不要求互相锁定资源。但是这个中间状态下，我们去访问数据库，可能遇到数据不一致的情况，不过我们需要做一些后补措施，保证在经过一段时间后，数据最终满足一致性。 这就是高可用，但弱一致（最终一致）。 由上面的两种思想，延伸出了很多的分布式事务解决方案： XA TCC 可靠消息最终一致 AT 2.4.分阶段提交2.4.1DTP和XA分布式事务的解决手段之一，就是两阶段提交协议（2PC：Two-Phase Commit） 那么到底什么是两阶段提交协议呢？ 1994 年，X/Open 组织（即现在的 Open Group ）定义了分布式事务处理的DTP 模型。该模型包括这样几个角色： 应用程序（ AP ）：我们的微服务 事务管理器（ TM ）：全局事务管理者 资源管理器（ RM ）：一般是数据库 通信资源管理器（ CRM ）：是TM和RM间的通信中间件 在该模型中，一个分布式事务（全局事务）可以被拆分成许多个本地事务，运行在不同的AP和RM上。每个本地事务的ACID很好实现，但是全局事务必须保证其中包含的每一个本地事务都能同时成功，若有一个本地事务失败，则所有其它事务都必须回滚。但问题是，本地事务处理过程中，并不知道其它事务的运行状态。因此，就需要通过CRM来通知各个本地事务，同步事务执行的状态。 因此，各个本地事务的通信必须有统一的标准，否则不同数据库间就无法通信。XA就是 X/Open DTP中通信中间件与TM间联系的接口规范，定义了用于通知事务开始、提交、终止、回滚等接口，各个数据库厂商都必须实现这些接口。 2.4.2.二阶段提交参考：漫话分布式系统共识协议: 2PC/3PC篇 二阶提交协议就是根据这一思想衍生出来的，将全局事务拆分为两个阶段来执行： 阶段一：准备阶段，各个本地事务完成本地事务的准备工作。 阶段二：执行阶段，各个本地事务根据上一阶段执行结果，进行提交或回滚。 这个过程中需要一个协调者（coordinator），还有事务的参与者（voter）。 1）正常情况 投票阶段：协调组询问各个事务参与者，是否可以执行事务。每个事务参与者执行事务，写入redo和undo日志，然后反馈事务执行成功的信息（agree） 提交阶段：协调组发现每个参与者都可以执行事务（agree），于是向各个事务参与者发出commit指令，各个事务参与者提交事务。 2）异常情况 当然，也有异常的时候： 投票阶段：协调组询问各个事务参与者，是否可以执行事务。每个事务参与者执行事务，写入redo和undo日志，然后反馈事务执行结果，但只要有一个参与者返回的是Disagree，则说明执行失败。 提交阶段：协调组发现有一个或多个参与者返回的是Disagree，认为执行失败。于是向各个事务参与者发出abort指令，各个事务参与者回滚事务。 3）缺陷 二阶段提交的问题： 单点故障问题 2PC的缺点在于不能处理fail-stop形式的节点failure. 比如下图这种情况. 假设coordinator和voter3都在Commit这个阶段crash了, 而voter1和voter2没有收到commit消息. 这时候voter1和voter2就陷入了一个困境. 因为他们并不能判断现在是两个场景中的哪一种: (1)上轮全票通过然后voter3第一个收到了commit的消息并在commit操作之后crash了 (2)上轮voter3反对所以干脆没有通过. 阻塞问题 在准备阶段、提交阶段，每个事物参与者都会锁定本地资源，并等待其它事务的执行结果，阻塞时间较长，资源锁定时间太久，因此执行的效率就比较低了。 面对二阶段提交的上述缺点，后来又演变出了三阶段提交，但是依然没有完全解决阻塞和资源锁定的问题，而且引入了一些新的问题，因此实际使用的场景较少。 2.4.3.使用场景对事务有强一致性要求，对事务执行效率不敏感，并且不希望有太多代码侵入。 2.5.TCCTCC模式可以解决2PC中的资源锁定和阻塞问题，减少资源锁定时间。 2.5.1.基本原理它本质是一种补偿的思路。事务运行过程包括三个方法， Try：资源的检测和预留； Confirm：执行的业务操作提交；要求 Try 成功 Confirm 一定要能成功； Cancel：预留资源释放。 执行分两个阶段： 准备阶段（try）：资源的检测和预留； 执行阶段（confirm/cancel）：根据上一步结果，判断下面的执行方法。如果上一步中所有事务参与者都成功，则这里执行confirm。反之，执行cancel 粗看似乎与两阶段提交没什么区别，但其实差别很大： try、confirm、cancel都是独立的事务，不受其它参与者的影响，不会阻塞等待它人 try、confirm、cancel由程序员在业务层编写，锁粒度有代码控制 2.5.2.实例我们以之前的下单业务中的扣减余额为例来看下三个不同的方法要怎么编写，假设账户A原来余额是100，需要余额扣减30元。如图： 一阶段（Try）：余额检查，并冻结用户部分金额，此阶段执行完毕，事务已经提交 检查用户余额是否充足，如果充足，冻结部分余额 在账户表中添加冻结金额字段，值为30，余额不变 二阶段 提交（Confirm）：真正的扣款，把冻结金额从余额中扣除，冻结金额清空 修改冻结金额为0，修改余额为100-30 = 70元 补偿（Cancel）：释放之前冻结的金额，并非回滚 余额不变，修改账户冻结金额为0 2.5.3.优势和缺点 优势 TCC执行的每一个阶段都会提交本地事务并释放锁，并不需要等待其它事务的执行结果。而如果其它事务执行失败，最后不是回滚，而是执行补偿操作。这样就避免了资源的长期锁定和阻塞等待，执行效率比较高，属于性能比较好的分布式事务方式。 缺点 代码侵入：需要人为编写代码实现try、confirm、cancel，代码侵入较多 开发成本高：一个业务需要拆分成3个步骤，分别编写业务实现，业务编写比较复杂 安全性考虑：cancel动作如果执行失败，资源就无法释放，需要引入重试机制，而重试可能导致重复执行，还要考虑重试时的幂等问题 2.5.4.使用场景 对事务有一定的一致性要求（最终一致） 对性能要求较高 开发人员具备较高的编码能力和幂等处理经验 2.6.可靠消息服务这种实现方式的思路，其实是源于ebay，其基本的设计思想是将远程分布式事务拆分成一系列的本地事务。 2.6.1.基本原理一般分为事务的发起者A和事务的其它参与者B： 事务发起者A执行本地事务 事务发起者A通过MQ将需要执行的事务信息发送给事务参与者B 事务参与者B接收到消息后执行本地事务 如图： 这个过程有点像你去学校食堂吃饭： 拿着钱去收银处，点一份红烧牛肉面，付钱 收银处给你发一个小票，还有一个号牌，你别把票弄丢！ 你凭小票和号牌一定能领到一份红烧牛肉面，不管需要多久 几个注意事项： 事务发起者A必须确保本地事务成功后，消息一定发送成功 MQ必须保证消息正确投递和持久化保存 事务参与者B必须确保消息最终一定能消费，如果失败需要多次重试 事务B执行失败，会重试，但不会导致事务A回滚 那么问题来了，我们如何保证消息发送一定成功？如何保证消费者一定能收到消息？ 2.6.2.本地消息表为了避免消息发送失败或丢失，我们可以把消息持久化到数据库中。实现时有简化版本和解耦合版本两种方式。 1）简化版本原理图： 事务发起者： 开启本地事务 执行事务相关业务 发送消息到MQ 把消息持久化到数据库，标记为已发送 提交本地事务 事务接收者： 接收消息 开启本地事务 处理事务相关业务 修改数据库消息状态为已消费 提交本地事务 额外的定时任务 定时扫描表中超时未消费消息，重新发送 优点： 与tcc相比，实现方式较为简单，开发成本低。 缺点： 数据一致性完全依赖于消息服务，因此消息服务必须是可靠的。 需要处理被动业务方的幂等问题 被动业务失败不会导致主动业务的回滚，而是重试被动的业务 事务业务与消息发送业务耦合、业务数据与消息表要在一起 2）独立消息服务为了解决上述问题，我们会引入一个独立的消息服务，来完成对消息的持久化、发送、确认、失败重试等一系列行为，大概的模型如下： 一次消息发送的时序图： 事务发起者A的基本执行步骤： 开启本地事务 通知消息服务，准备发送消息（消息服务将消息持久化，标记为准备发送） 执行本地业务， 执行失败则终止，通知消息服务，取消发送（消息服务修改订单状态） 执行成功则继续，通知消息服务，确认发送（消息服务发送消息、修改订单状态） 提交本地事务 消息服务本身提供下面的接口： 准备发送：把消息持久化到数据库，并标记状态为准备发送 取消发送：把数据库消息状态修改为取消 确认发送：把数据库消息状态修改为确认发送。尝试发送消息，成功后修改状态为已发送 确认消费：消费者已经接收并处理消息，把数据库消息状态修改为已消费 定时任务：定时扫描数据库中状态为确认发送的消息，然后询问对应的事务发起者，事务业务执行是否成功，结果： 业务执行成功：尝试发送消息，成功后修改状态为已发送 业务执行失败：把数据库消息状态修改为取消 事务参与者B的基本步骤： 接收消息 开启本地事务 执行业务 通知消息服务，消息已经接收和处理 提交事务 优点： 解除了事务业务与消息相关业务的耦合 缺点： 实现起来比较复杂 2.6.3.RocketMQ事务消息RocketMQ本身自带了事务消息，可以保证消息的可靠性，原理其实就是自带了本地消息表，与我们上面讲的思路类似。 2.6.4.RabbitMQ的消息确认RabbitMQ确保消息不丢失的思路比较奇特，并没有使用传统的本地表，而是利用了消息的确认机制： 生产者确认机制：确保消息从生产者到达MQ不会有问题 消息生产者发送消息到RabbitMQ时，可以设置一个异步的监听器，监听来自MQ的ACK MQ接收到消息后，会返回一个回执给生产者： 消息到达交换机后路由失败，会返回失败ACK 消息路由成功，持久化失败，会返回失败ACK 消息路由成功，持久化成功，会返回成功ACK 生产者提前编写好不同回执的处理方式 失败回执：等待一定时间后重新发送 成功回执：记录日志等行为 消费者确认机制：确保消息能够被消费者正确消费 消费者需要在监听队列的时候指定手动ACK模式 RabbitMQ把消息投递给消费者后，会等待消费者ACK，接收到ACK后才删除消息，如果没有接收到ACK消息会一直保留在服务端，如果消费者断开连接或异常后，消息会投递给其它消费者。 消费者处理完消息，提交事务后，手动ACK。如果执行过程中抛出异常，则不会ACK，业务处理失败，等待下一条消息 经过上面的两种确认机制，可以确保从消息生产者到消费者的消息安全，再结合生产者和消费者两端的本地事务，即可保证一个分布式事务的最终一致性。 2.6.5.消息事务的优缺点总结上面的几种模型，消息事务的优缺点如下： 优点： 业务相对简单，不需要编写三个阶段业务 是多个本地事务的结合，因此资源锁定周期短，性能好 缺点： 代码侵入 依赖于MQ的可靠性 消息发起者可以回滚，但是消息参与者无法引起事务回滚 事务时效性差，取决于MQ消息发送是否及时，还有消息参与者的执行情况 针对事务无法回滚的问题，有人提出说可以再事务参与者执行失败后，再次利用MQ通知消息服务，然后由消息服务通知其他参与者回滚。那么，恭喜你，你利用MQ和自定义的消息服务再次实现了2PC 模型，又造了一个大轮子 2.7.AT模式2019年 1 月份，Seata 开源了 AT 模式。AT 模式是一种无侵入的分布式事务解决方案。可以看做是对TCC或者二阶段提交模型的一种优化，解决了TCC模式中的代码侵入、编码复杂等问题。 在 AT 模式下，用户只需关注自己的“业务 SQL”，用户的 “业务 SQL” 作为一阶段，Seata 框架会自动生成事务的二阶段提交和回滚操作。 可以参考Seata的官方文档。 2.7.1.基本原理先来看一张流程图： 有没有感觉跟TCC的执行很像，都是分两个阶段： 一阶段：执行本地事务，并返回执行结果 二阶段：根据一阶段的结果，判断二阶段做法：提交或回滚 但AT模式底层做的事情可完全不同，而且第二阶段根本不需要我们编写，全部有Seata自己实现了。也就是说：我们写的代码与本地事务时代码一样，无需手动处理分布式事务。 那么，AT模式如何实现无代码侵入，如何帮我们自动实现二阶段代码的呢？ 一阶段 在一阶段，Seata 会拦截“业务 SQL”，首先解析 SQL 语义，找到“业务 SQL”要更新的业务数据，在业务数据被更新前，将其保存成“before image”，然后执行“业务 SQL”更新业务数据，在业务数据更新之后，再将其保存成“after image”，最后获取全局行锁，提交事务。以上操作全部在一个数据库事务内完成，这样保证了一阶段操作的原子性。 这里的before image和after image类似于数据库的undo和redo日志，但其实是用数据库模拟的。 二阶段提交 二阶段如果是提交的话，因为“业务 SQL”在一阶段已经提交至数据库， 所以 Seata 框架只需将一阶段保存的快照数据和行锁删掉，完成数据清理即可。 二阶段回滚： 二阶段如果是回滚的话，Seata 就需要回滚一阶段已经执行的“业务 SQL”，还原业务数据。回滚方式便是用“before image”还原业务数据；但在还原前要首先要校验脏写，对比“数据库当前业务数据”和 “after image”，如果两份数据完全一致就说明没有脏写，可以还原业务数据，如果不一致就说明有脏写，出现脏写就需要转人工处理。 不过因为有全局锁机制，所以可以降低出现脏写的概率。 AT 模式的一阶段、二阶段提交和回滚均由 Seata 框架自动生成，用户只需编写“业务 SQL”，便能轻松接入分布式事务，AT 模式是一种对业务无任何侵入的分布式事务解决方案。 2.7.2.详细架构和流程Seata中的几个基本概念： TC（Transaction Coordinator） - 事务协调者 维护全局和分支事务的状态，驱动全局事务提交或回滚（TM之间的协调者）。 TM（Transaction Manager） - 事务管理器 定义全局事务的范围：开始全局事务、提交或回滚全局事务。 RM（Resource Manager） - 资源管理器 管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。 我们看下面的一个架构图 TM：业务模块中全局事务的开启者 向TC开启一个全局事务 调用其它微服务 RM：业务模块执行者中，包含RM部分，负责向TC汇报事务执行状态 执行本地事务 向TC注册分支事务，并提交本地事务执行结果 TM：结束对微服务的调用，通知TC，全局事务执行完毕，事务一阶段结束 TC：汇总各个分支事务执行结果，决定分布式事务是提交还是回滚； TC 通知所有 RM 提交/回滚 资源，事务二阶段结束。 一阶段： TM开启全局事务，并向TC声明全局事务，包括全局事务XID信息 TM所在服务调用其它微服务 微服务，主要有RM来执行 查询before_image 执行本地事务 查询after_image 生成undo_log并写入数据库 向TC注册分支事务，告知事务执行结果 获取全局锁（阻止其它全局事务并发修改当前数据） 释放本地锁（不影响其它业务对数据的操作） 待所有业务执行完毕，事务发起者（TM）会尝试向TC提交全局事务 二阶段： TC统计分支事务执行情况，根据结果判断下一步行为 分支都成功：通知分支事务，提交事务 有分支执行失败：通知执行成功的分支事务，回滚数据 分支事务的RM 提交事务：直接清空before_image和after_image信息，释放全局锁 回滚事务： 校验after_image，判断是否有脏写 如果没有脏写，回滚数据到before_image，清除before_image和after_image 如果有脏写，请求人工介入 2.7.3.工作机制详见Seata的官方文档：https://seata.io/zh-cn/docs/overview/what-is-seata.html 场景 以一个示例来说明整个 AT 分支的工作过程。 业务表：product Field Type Key id bigint(20) PRI name varchar(100) since varchar(100) AT 分支事务的业务逻辑： update product set name = 'GTS' where name = 'TXC'; 一阶段 过程： 解析 SQL：得到 SQL 的类型（UPDATE），表（product），条件（where name = ‘TXC’）等相关的信息。 查询前镜像：根据解析得到的条件信息，生成查询语句，定位数据。 select id, name, since from product where name = 'TXC'; 得到前镜像： id name since 1 TXC 2014 执行业务 SQL：更新这条记录的 name 为 ‘GTS’。 查询后镜像：根据前镜像的结果，通过 主键 定位数据。 select id, name, since from product where id = 1`; 得到后镜像： id name since 1 GTS 2014 插入回滚日志：把前后镜像数据以及业务 SQL 相关的信息组成一条回滚日志记录，插入到 UNDO_LOG 表中。 &#123; \"branchId\": 641789253, \"undoItems\": [&#123; \"afterImage\": &#123; \"rows\": [&#123; \"fields\": [&#123; \"name\": \"id\", \"type\": 4, \"value\": 1 &#125;, &#123; \"name\": \"name\", \"type\": 12, \"value\": \"GTS\" &#125;, &#123; \"name\": \"since\", \"type\": 12, \"value\": \"2014\" &#125;] &#125;], \"tableName\": \"product\" &#125;, \"beforeImage\": &#123; \"rows\": [&#123; \"fields\": [&#123; \"name\": \"id\", \"type\": 4, \"value\": 1 &#125;, &#123; \"name\": \"name\", \"type\": 12, \"value\": \"TXC\" &#125;, &#123; \"name\": \"since\", \"type\": 12, \"value\": \"2014\" &#125;] &#125;], \"tableName\": \"product\" &#125;, \"sqlType\": \"UPDATE\" &#125;], \"xid\": \"xid:xxx\" &#125; 提交前，向 TC 注册分支：申请 product 表中，主键值等于 1 的记录的 全局锁 。 本地事务提交：业务数据的更新和前面步骤中生成的 UNDO LOG 一并提交。 将本地事务提交的结果上报给 TC。 二阶段-回滚 收到 TC 的分支回滚请求，开启一个本地事务，执行如下操作。 通过 XID 和 Branch ID 查找到相应的 UNDO LOG 记录。 数据校验：拿 UNDO LOG 中的后镜与当前数据进行比较，如果有不同，说明数据被当前全局事务之外的动作做了修改。这种情况，需要根据配置策略来做处理，详细的说明在另外的文档中介绍。 根据 UNDO LOG 中的前镜像和业务 SQL 的相关信息生成并执行回滚的语句： update product set name = 'TXC' where id = 1; 提交本地事务。并把本地事务的执行结果（即分支事务回滚的结果）上报给 TC。 二阶段-提交 收到 TC 的分支提交请求，把请求放入一个异步任务的队列中，马上返回提交成功的结果给 TC。 异步任务阶段的分支提交请求将异步和批量地删除相应 UNDO LOG 记录。 2.7.4.优缺点优点： 与2PC相比：每个分支事务都是独立提交，不互相等待，减少了资源锁定和阻塞时间 与TCC相比：二阶段的执行操作全部自动化生成，无代码侵入，开发成本低 缺点： 与TCC相比，需要动态生成二阶段的反向补偿操作，执行性能略低于TCC 2.8.Saga模式Saga 模式是 Seata 即将开源的长事务解决方案，将由蚂蚁金服主要贡献。 其理论基础是Hector &amp; Kenneth 在1987年发表的论文Sagas。 Seata官网对于Saga的指南：https://seata.io/zh-cn/docs/user/saga.html 基本模型在 Saga 模式下，分布式事务内有多个参与者，每一个参与者都是一个冲正补偿服务，需要用户根据业务场景实现其正向操作和逆向回滚操作。 分布式事务执行过程中，依次执行各参与者的正向操作，如果所有正向操作均执行成功，那么分布式事务提交。如果任何一个正向操作执行失败，那么分布式事务会去退回去执行前面各参与者的逆向回滚操作，回滚已提交的参与者，使分布式事务回到初始状态。 Saga 模式下分布式事务通常是由事件驱动的，各个参与者之间是异步执行的，Saga 模式是一种长事务解决方案。 适用场景： 业务流程长、业务流程多 参与者包含其它公司或遗留系统服务，无法提供 TCC 模式要求的三个接口 优势： 一阶段提交本地事务，无锁，高性能 事件驱动架构，参与者可异步执行，高吞吐 补偿服务易于实现 缺点： 不保证隔离性（应对方案见用户文档） 3.Seata3.1.介绍Seata（Simple Extensible Autonomous Transaction Architecture，简单可扩展自治事务框架）是 2019 年 1 月份蚂蚁金服和阿里巴巴共同开源的分布式事务解决方案。Seata 开源半年左右，目前已经有接近一万 star，社区非常活跃。我们热忱欢迎大家参与到 Seata 社区建设中，一同将 Seata 打造成开源分布式事务标杆产品。 Seata：https://github.com/seata/seata 3.1.1. Seata 产品模块如下图所示，Seata 中有三大模块，分别是 TM、RM 和 TC。 其中 TM 和 RM 是作为 Seata 的客户端与业务系统集成在一起，TC 作为 Seata 的服务端独立部署。 3.1.2.Seata支持的事务模型Seata 会有 4 种分布式事务解决方案，分别是 AT 模式、TCC 模式、Saga 模式和 XA 模式。 3.2.AT模式实践Seata中比较常用的是AT模式，这里我们拿AT模式来做演示，看看如何在SpringCloud微服务中集成Seata. 我们假定一个用户购买商品的业务逻辑。整个业务逻辑由3个微服务提供支持： 仓储服务：对给定的商品扣除仓储数量。 订单服务：根据采购需求创建订单。 帐户服务：从用户帐户中扣除余额。 流程图： 订单服务在下单时，同时调用库存服务和用户服务，此时就会发生跨服务和跨数据源的分布式事务问题。 3.2.1.准备数据执行资料中提供的seata_demo.sql文件，导入数据。 其中包含4张表。 Order表： CREATE TABLE &#96;order_tbl&#96; ( &#96;id&#96; int(11) NOT NULL AUTO_INCREMENT, &#96;user_id&#96; varchar(255) DEFAULT NULL COMMENT &#39;用户id&#39;, &#96;commodity_code&#96; varchar(255) DEFAULT NULL COMMENT &#39;商品码&#39;, &#96;count&#96; int(11) unsigned DEFAULT &#39;0&#39; COMMENT &#39;购买数量&#39;, &#96;money&#96; int(11) unsigned DEFAULT &#39;0&#39; COMMENT &#39;总金额&#39;, PRIMARY KEY (&#96;id&#96;) USING BTREE ) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 ROW_FORMAT&#x3D;COMPACT; 商品库存表： CREATE TABLE &#96;storage_tbl&#96; ( &#96;id&#96; int(11) NOT NULL AUTO_INCREMENT, &#96;commodity_code&#96; varchar(255) DEFAULT NULL COMMENT &#39;商品码&#39;, &#96;count&#96; int(11) unsigned DEFAULT &#39;0&#39; COMMENT &#39;商品库存&#39;, PRIMARY KEY (&#96;id&#96;) USING BTREE, UNIQUE KEY &#96;commodity_code&#96; (&#96;commodity_code&#96;) USING BTREE ) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;2 DEFAULT CHARSET&#x3D;utf8 ROW_FORMAT&#x3D;COMPACT; 用户账户表： CREATE TABLE &#96;account_tbl&#96; ( &#96;id&#96; int(11) NOT NULL AUTO_INCREMENT, &#96;user_id&#96; varchar(255) DEFAULT NULL COMMENT &#39;用户id&#39;, &#96;money&#96; int(11) unsigned DEFAULT &#39;0&#39; COMMENT &#39;用户余额&#39;, PRIMARY KEY (&#96;id&#96;) USING BTREE ) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;2 DEFAULT CHARSET&#x3D;utf8 ROW_FORMAT&#x3D;COMPACT; 还有用来记录Seata中的事务日志表undo_log，其中会包含after_image和before_image数据，用于数据回滚： CREATE TABLE &#96;undo_log&#96; ( &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT, &#96;branch_id&#96; bigint(20) NOT NULL, &#96;xid&#96; varchar(100) NOT NULL, &#96;context&#96; varchar(128) NOT NULL, &#96;rollback_info&#96; longblob NOT NULL, &#96;log_status&#96; int(11) NOT NULL, &#96;log_created&#96; datetime NOT NULL, &#96;log_modified&#96; datetime NOT NULL, &#96;ext&#96; varchar(100) DEFAULT NULL, PRIMARY KEY (&#96;id&#96;) USING BTREE, UNIQUE KEY &#96;ux_undo_log&#96; (&#96;xid&#96;,&#96;branch_id&#96;) USING BTREE ) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 ROW_FORMAT&#x3D;COMPACT; 3.2.2.引入Demo工程我们先准备基本的项目环境，实现下单的业务代码 导入项目项目结构如下： 结构说明： account-service：用户服务，提供操作用户账号余额的功能，端口8083 eureka-server：注册中心，端口8761 order-service：订单服务，提供根据数据创建订单的功能，端口8082 storage-service：仓储服务，提供扣减商品库存功能，端口8081 测试事务接下来，我们来测试下分布式事务的现象。 下单的接口是： 请求方式：POST 请求路径：/order 请求参数：form表单，包括： userId：用户id commodityCode：商品码 count：购买数量 money：话费金额 返回值类型：long，订单的id 原始数据库数据： 余额： 库存： 其它两张表为空。 正常下单 此时启动项目，尝试下单，目前商品库存为10，用户余额为1000，因此只要数据不超过这两个值应该能正常下单。 查看数据库数据： 余额： 库存： 订单： 异常下单 这次，我们把money参数设置为1200，这样就超过了余额最大值，理论上所有数据都应该回滚： 看下用户余额： 因为扣款失败，因此这里没有扣减 来看下库存数据： 这说明扣减库存依然成功，并未回滚！ 接下来，我们引入Seata，看看能不能解决这个问题。 3.2.3.准备TC服务在之前讲解Seata原理的时候，我们就聊过，其中包含重要的3个角色： TC：事务协调器 TM：事务管理器 RM：资源管理器 其中，TC是一个独立的服务，负责协调各个分支事务，而TM和RM通过jar包的方式，集成在各个事务参与者中。 因此，首先我们需要搭建一个独立的TC服务。 1）安装首先去官网下载TC的服务端安装包，GitHub的地址：https://github.com/seata/seata/releases 其目录结构如下： 包括： bin：启动脚本 conf：配置文件 lib：依赖项 2）配置Seata的核心配置主要是两部分： 注册中心的配置：在$&#123;seata_home&#125;/conf/目录中，一般是registry.conf文件 当前服务的配置，两种配置方式： 通过分布式服务的统一配置中心，例如Zookeeper 通过本地文件 我们先看registry.conf，内容是JSON风格 registry &#123; # 指定注册中心类型，这里使用eureka类型 type = \"eureka\" # 各种注册中心的配置。。这里省略，只保留了eureka和Zookeeper eureka &#123; serviceUrl = \"http://localhost:8761/eureka\" application = \"seata_tc_server\" weight = \"1\" &#125; zk &#123; cluster = \"default\" serverAddr = \"127.0.0.1:2181\" session.timeout = 6000 connect.timeout = 2000 &#125; &#125; config &#123; # 配置文件方式，可以支持 file、nacos 、apollo、zk、consul、etcd3 type = \"file\" nacos &#123; serverAddr = \"localhost\" namespace = \"\" group = \"SEATA_GROUP\" &#125; zk &#123; serverAddr = \"127.0.0.1:2181\" session.timeout = 6000 connect.timeout = 2000 &#125; file &#123; name = \"file.conf\" &#125; &#125; 这个文件主要配置两个内容： 注册中心的类型及地址，本例我们选择eureka做注册中心 eureka.serviceUrl：是eureka的地址，例如http://localhost:8761/eureka application：是TC注册到eureka时的服务名称，例如seata_tc_server 配置中心的类型及地址，本例我们选择本地文件做配置，就是当前目录的file.conf文件 再来看file.conf文件： ## transaction log store, only used in seata-server store &#123; ## store mode: file、db mode = \"file\" ## file store property file &#123; ## store location dir dir = \"sessionStore\" # branch session size , if exceeded first try compress lockkey, still exceeded throws exceptions maxBranchSessionSize = 16384 # globe session size , if exceeded throws exceptions maxGlobalSessionSize = 512 # file buffer size , if exceeded allocate new buffer fileWriteBufferCacheSize = 16384 # when recover batch read size sessionReloadReadSize = 100 # async, sync flushDiskMode = async &#125; ## database store property db &#123; ## the implement of javax.sql.DataSource, such as DruidDataSource(druid)/BasicDataSource(dbcp) etc. datasource = \"dbcp\" ## mysql/oracle/h2/oceanbase etc. dbType = \"mysql\" driverClassName = \"com.mysql.jdbc.Driver\" url = \"jdbc:mysql://127.0.0.1:3306/seata_demo\" user = \"root\" password = \"123\" minConn = 1 maxConn = 10 globalTable = \"global_table\" branchTable = \"branch_table\" lockTable = \"lock_table\" queryLimit = 100 &#125; &#125; 关键配置： store：TC的服务端数据存储配置 mode：数据存储方式，支持两种：file和db file：将数据存储在本地文件中，性能比较好，但不支持水平扩展 db：将数据保存在指定的数据库中，需要指定数据库连接信息 如果用文件作为存储介质，不需要其它配置了，直接运行即可。 但是如果使用db作为存储介质，还需要在数据库中创建3张表： CREATE TABLE IF NOT EXISTS `global_table` ( `xid` VARCHAR(128) NOT NULL, `transaction_id` BIGINT, `status` TINYINT NOT NULL, `application_id` VARCHAR(32), `transaction_service_group` VARCHAR(32), `transaction_name` VARCHAR(128), `timeout` INT, `begin_time` BIGINT, `application_data` VARCHAR(2000), `gmt_create` DATETIME, `gmt_modified` DATETIME, PRIMARY KEY (`xid`), KEY `idx_gmt_modified_status` (`gmt_modified`, `status`), KEY `idx_transaction_id` (`transaction_id`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8; -- the table to store BranchSession data CREATE TABLE IF NOT EXISTS `branch_table` ( `branch_id` BIGINT NOT NULL, `xid` VARCHAR(128) NOT NULL, `transaction_id` BIGINT, `resource_group_id` VARCHAR(32), `resource_id` VARCHAR(256), `branch_type` VARCHAR(8), `status` TINYINT, `client_id` VARCHAR(64), `application_data` VARCHAR(2000), `gmt_create` DATETIME, `gmt_modified` DATETIME, PRIMARY KEY (`branch_id`), KEY `idx_xid` (`xid`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8; -- the table to store lock data CREATE TABLE IF NOT EXISTS `lock_table` ( `row_key` VARCHAR(128) NOT NULL, `xid` VARCHAR(96), `transaction_id` BIGINT, `branch_id` BIGINT NOT NULL, `resource_id` VARCHAR(256), `table_name` VARCHAR(32), `pk` VARCHAR(36), `gmt_create` DATETIME, `gmt_modified` DATETIME, PRIMARY KEY (`row_key`), KEY `idx_branch_id` (`branch_id`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8; 3）启动进入$&#123;seata_home&#125;/bin/目录中: 如果是linux环境（要有JRE），执行seata-server.sh 如果是windows环境，执行seata-server.bat 3.2.4.改造Order服务接下来是微服务的改造，不管是哪一个微服务，只要是事务的参与者，步骤基本一致。 1）引入依赖我们在父工程seata-demo中已经对依赖做了管理： &lt;alibaba.seata.version>2.1.0.RELEASE&lt;/alibaba.seata.version> &lt;seata.version>1.1.0&lt;/seata.version> 因此，我们在项目order-service的pom文件中，引入依赖坐标即可： &lt;dependency> &lt;groupId>com.alibaba.cloud&lt;/groupId> &lt;artifactId>spring-cloud-alibaba-seata&lt;/artifactId> &lt;version>$&#123;alibaba.seata.version&#125;&lt;/version> &lt;exclusions> &lt;exclusion> &lt;artifactId>seata-all&lt;/artifactId> &lt;groupId>io.seata&lt;/groupId> &lt;/exclusion> &lt;/exclusions> &lt;/dependency> &lt;dependency> &lt;artifactId>seata-all&lt;/artifactId> &lt;groupId>io.seata&lt;/groupId> &lt;version>$&#123;seata.version&#125;&lt;/version> &lt;/dependency> 2）添加配置文件首先在application.yml中添加一行配置： spring: cloud: alibaba: seata: tx-service-group: test_tx_group # 定义事务组的名称 这里是定义事务组的名称，接下来会用到。 然后是在resources目录下放两个配置文件：file.conf和registry.conf 其中，registry.conf与TC服务端的一样，此处不再讲解。 我们来看下file.conf transport &#123; # tcp udt unix-domain-socket type = \"TCP\" #NIO NATIVE server = \"NIO\" #enable heartbeat heartbeat = true # the client batch send request enable enableClientBatchSendRequest = true #thread factory for netty threadFactory &#123; bossThreadPrefix = \"NettyBoss\" workerThreadPrefix = \"NettyServerNIOWorker\" serverExecutorThread-prefix = \"NettyServerBizHandler\" shareBossWorker = false clientSelectorThreadPrefix = \"NettyClientSelector\" clientSelectorThreadSize = 1 clientWorkerThreadPrefix = \"NettyClientWorkerThread\" # netty boss thread size,will not be used for UDT bossThreadSize = 1 #auto default pin or 8 workerThreadSize = \"default\" &#125; shutdown &#123; # when destroy server, wait seconds wait = 3 &#125; serialization = \"seata\" compressor = \"none\" &#125; service &#123; vgroup_mapping.test_tx_group = \"seata_tc_server\" #only support when registry.type=file, please don't set multiple addresses seata_tc_server.grouplist = \"127.0.0.1:8091\" #degrade, current not support enableDegrade = false #disable seata disableGlobalTransaction = false &#125; client &#123; rm &#123; asyncCommitBufferLimit = 10000 lock &#123; retryInterval = 10 retryTimes = 30 retryPolicyBranchRollbackOnConflict = true &#125; reportRetryCount = 5 tableMetaCheckEnable = false reportSuccessEnable = false &#125; tm &#123; commitRetryCount = 5 rollbackRetryCount = 5 &#125; undo &#123; dataValidation = true logSerialization = \"jackson\" logTable = \"undo_log\" &#125; log &#123; exceptionRate = 100 &#125; &#125; 配置详情： transport：与TC交互的一些配置 heartbeat：client和server通信心跳检测开关 enableClientBatchSendRequest：客户端事务消息请求是否批量合并发送 service：TC的地址配置，用于获取TC的地址 vgroup_mapping.test_tx_group = &quot;seata_tc_server&quot;： test_tx_group：是事务组名称，要与application.yml中配置一致， seata_tc_server：是TC服务端在注册中心的id，将来通过注册中心获取TC地址 enableDegrade：服务降级开关，默认关闭。如果开启，当业务重试多次失败后会放弃全局事务 disableGlobalTransaction：全局事务开关，默认false。false为开启，true为关闭 default.grouplist：这个当注册中心为file的时候，才用到 client：客户端配置 rm：资源管理器配 asynCommitBufferLimit：二阶段提交默认是异步执行，这里指定异步队列的大小 lock：全局锁配置 retryInterval：校验或占用全局锁重试间隔，默认10，单位毫秒 retryTimes：校验或占用全局锁重试次数，默认30次 retryPolicyBranchRollbackOnConflict：分支事务与其它全局回滚事务冲突时锁策略，默认true，优先释放本地锁让回滚成功 reportRetryCount：一阶段结果上报TC失败后重试次数，默认5次 tm：事务管理器配置 commitRetryCount：一阶段全局提交结果上报TC重试次数，默认1 rollbackRetryCount：一阶段全局回滚结果上报TC重试次数，默认1 undo：undo_log的配置 dataValidation：是否开启二阶段回滚镜像校验，默认true logSerialization：undo序列化方式，默认Jackson logTable：自定义undo表名，默认是undo_log log：日志配置 exceptionRate：出现回滚异常时的日志记录频率，默认100，百分之一概率。回滚失败基本是脏数据，无需输出堆栈占用硬盘空间 3）代理DataSourceSeata的二阶段执行是通过拦截sql语句，分析语义来指定回滚策略，因此需要对DataSource做代理。我们在项目的cn.itcast.order.config包中，添加一个配置类： package cn.itcast.order.config; import com.baomidou.mybatisplus.extension.spring.MybatisSqlSessionFactoryBean; import io.seata.rm.datasource.DataSourceProxy; import org.apache.ibatis.session.SqlSessionFactory; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import javax.sql.DataSource; @Configuration public class DataSourceProxyConfig &#123; @Bean public SqlSessionFactory sqlSessionFactoryBean(DataSource dataSource) throws Exception &#123; // 订单服务中引入了mybatis-plus，所以要使用特殊的SqlSessionFactoryBean MybatisSqlSessionFactoryBean sqlSessionFactoryBean = new MybatisSqlSessionFactoryBean(); // 代理数据源 sqlSessionFactoryBean.setDataSource(new DataSourceProxy(dataSource)); // 生成SqlSessionFactory return sqlSessionFactoryBean.getObject(); &#125; &#125; 注意，这里因为订单服务使用了mybatis-plus这个框架（这是一个mybatis集成框架，自动生成单表Sql），因此我们需要用mybatis-plus的MybatisSqlSessionFactoryBean代替SqlSessionFactoryBean 如果用的是原生的mybatis，请使用SqlSessionFactoryBean。 4）添加事务注解给事务发起者order_service的OrderServiceImpl中的createOrder()方法添加@GlobalTransactional注解，开启全局事务： 重新启动即可。 3.2.5.改造Storage、Account服务与OrderService类似，这里也要经过下面的步骤： 引入依赖：与order-service一致，略 添加配置文件：与order-service一致，略 代理DataSource，我们的storage-service和account-service都没有用mybatis-plus，所以配置要使用SqlSessionFactory： package cn.itcast.order.config; import io.seata.rm.datasource.DataSourceProxy; import org.apache.ibatis.session.SqlSessionFactory; import org.mybatis.spring.SqlSessionFactoryBean; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import javax.sql.DataSource; @Configuration public class DataSourceProxyConfig &#123; @Bean public SqlSessionFactory sqlSessionFactoryBean(DataSource dataSource) throws Exception &#123; // 因为使用的是mybatis，这里定义SqlSessionFactoryBean SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean(); // 配置数据源代理 sqlSessionFactoryBean.setDataSource(new DataSourceProxy(dataSource)); return sqlSessionFactoryBean.getObject(); &#125; &#125; 另外，事务注解可以使用@Transactionnal，而不是@GlobalTransactional，事务发起者才需要添加@GlobalTransactional。 3.2.6.测试重启所有微服务后，我们再次测试。 目前数据情况：用户余额900，库存为6. 我们试试扣款1200元，那么扣款失败，理论上来说所有数据都会回滚. 看下用户余额： 因为扣款失败，因此这里没有扣减 来看下库存数据： 减库存依然是6，成功回滚，说明分布式事务生效了！","categories":[{"name":"Java","slug":"Java","permalink":"https://blog.innnovation.cn/categories/Java/"},{"name":"分布式","slug":"Java/分布式","permalink":"https://blog.innnovation.cn/categories/Java/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.innnovation.cn/tags/Java/"},{"name":"分布式","slug":"分布式","permalink":"https://blog.innnovation.cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"事务","slug":"事务","permalink":"https://blog.innnovation.cn/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"Seata","slug":"Seata","permalink":"https://blog.innnovation.cn/tags/Seata/"}]},{"title":"操作系统-如何划分和组织内存","slug":"操作系统-如何划分与组织内存","date":"2021-07-11T16:00:00.000Z","updated":"2021-07-12T06:43:52.944Z","comments":true,"path":"2021/07/12/cao-zuo-xi-tong-ru-he-hua-fen-yu-zu-zhi-nei-cun/","link":"","permalink":"https://blog.innnovation.cn/2021/07/12/cao-zuo-xi-tong-ru-he-hua-fen-yu-zu-zhi-nei-cun/","excerpt":"","text":"分段，分页？第一点，从表示方式和状态确定角度考虑。段的长度大小不一，用什么数据结构表示一个段，如何确定一个段已经分配还是空闲呢？而页的大小固定，我们只需用位图就能表示页的分配与释放。 比方说，位图中第 1 位为 1，表示第一个页已经分配；位图中第 2 位为 0，表示第二个页是空闲，每个页的开始地址和大小都是固定的。 第二点，从内存碎片的利用看，由于段的长度大小不一，更容易产生内存碎片，例如内存中有 A 段（内存地址：0～5000）、 B 段（内存地址：5001～8000）、C 段（内存地址：8001～9000），这时释放了 B 段，然后需要给 D 段分配内存空间，且 D 段长度为 5000。你立马就会发现 A 段和 C 段之间的空间（B 段）不能满足，只能从 C 段之后的内存空间开始分配，随着程序运行，这些情况会越来越多。段与段之间存在着不大不小的空闲空间，内存总的空闲空间很多，但是放不下一个新段。而页的大小固定，分配最小单位是页，页也会产生碎片，比如我需要请求分配 4 个页，但在内存中从第 1～3 个页是空闲的，第 4 个页是分配出去了，第 5 个页是空闲的。这种情况下，我们通过修改页表的方式，就能让连续的虚拟页面映射到非连续的物理页面。 第三点，从内存和硬盘的数据交换效率考虑，当内存不足时，操作系统希望把内存中的一部分数据写回硬盘，来释放内存。这就涉及到内存和硬盘交换数据，交换单位是段还是页？如果是段的话，其大小不一，A 段有 50MB，B 段有 1KB，A、B 段写回硬盘的时间也不同，有的段需要时间长，有的段需要时间短，硬盘的空间分配也会有上面第二点同样的问题，这样会导致系统性能抖动。如果每次交换一个页，则没有这些问题。 段最大的问题是使得虚拟内存地址空间，难于实施 综上，我们自然选择分页模式来管理内存，其实现在所有的商用操作系统都使用了分页模式管理内存。我们用 4KB 作为页大小，这也正好对应 x86 CPU 长模式下 MMU 4KB 的分页方式。 页内存表示形式我们使用分页模型来管理内存。首先是把物理内存空间分成 4KB 大小页，这页表示从地址 x 开始到 x+0xFFF 这一段的物理内存空间，x 必须是 0x1000 对齐的。这一段 x+0xFFF 的内存空间，称为内存页。在逻辑上的结构图如下所示： 上图这是一个接近真实机器的情况，不过一定不要忘记前面的内存布局示图，真实的物理内存地址空间不是连续的，这中间可能有空洞，可能是显存，也可能是外设的寄存器。 真正的物理内存空间布局信息来源于 e820map_t 结构数组，之前的初始化中，我们已经将其转换成 phymmarge_t 结构数组了，由 kmachbsp-&gt;mb_e820expadr 指向。 我们需要页的状态、页的地址、页的分配记数、页的类型、页的链表，你自然就会想到，这些信息可以用一个 C 语言结构体封装起来 //内存空间地址描述符标志 typedef struct s_MSADFLGS &#123; u32_t mf_olkty:2; //挂入链表的类型 u32_t mf_lstty:1; //是否挂入链表 u32_t mf_mocty:2; //分配类型，被谁占用了，内核还是应用或者空闲 u32_t mf_marty:3; //属于哪个区 u32_t mf_uindx:24; //分配计数 &#125;__attribute__((packed)) msadflgs_t; //物理地址和标志 typedef struct s_PHYADRFLGS &#123; u64_t paf_alloc:1; //分配位 u64_t paf_shared:1; //共享位 u64_t paf_swap:1; //交换位 u64_t paf_cache:1; //缓存位 u64_t paf_kmap:1; //映射位 u64_t paf_lock:1; //锁定位 u64_t paf_dirty:1; //脏位 u64_t paf_busy:1; //忙位 u64_t paf_rv2:4; //保留位 u64_t paf_padrs:52; //页物理地址位 &#125;__attribute__((packed)) phyadrflgs_t; //内存空间地址描述符 typedef struct s_MSADSC &#123; list_h_t md_list; //链表 spinlock_t md_lock; //保护自身的自旋锁 msadflgs_t md_indxflgs; //内存空间地址描述符标志 phyadrflgs_t md_phyadrs; //物理地址和标志 void* md_odlink; //相邻且相同大小msadsc的指针 &#125;__attribute__((packed)) msadsc_t; 内存区内存区和内存页不同，内存区只是一个逻辑上的概念，并不是硬件上必需的，就是说就算没有内存区，也毫不影响硬件正常工作；但是没有内存页是绝对不行的。 硬件区，它占用物理内存低端区域，地址区间为 0~32MB。从名字就能看出来，这个内存区域是给硬件使用的，我们不是使用虚拟地址吗？虚拟地址不是和物理地址无关吗，一个虚拟可以映射到任一合法的物理地址。 但凡事总有例外，虚拟地址主要依赖于 CPU 中的 MMU，但有很多外部硬件能直接和内存交换数据，常见的有 DMA，并且它只能访问低于 24MB 的物理内存。这就导致了我们很多内存页不能随便分配给这些设备，但是我们只要规定硬件区分配内存页就好，这就是硬件区的作用。 内核区，内核也要使用内存，但是内核同样也是运行在虚拟地址空间，就需要有一段物理内存空间和内核的虚拟地址空间是线性映射关系。 应用区，这个区域主是给应用用户态程序使用。应用程序使用虚拟地址空间，一开始并不会为应用一次性分配完所需的所有物理内存，而是按需分配，即应用用到一页就分配一个页。 如果访问到一个没有与物理内存页建立映射关系的虚拟内存页，这时候 CPU 就会产生缺页异常。最终这个缺页异常由操作系统处理，操作系统会分配一个物理内存页，并建好映射关系。 这是因为这种情况往往分配的是单个页面，所以为了给单个页面提供快捷的内存请求服务，就需要把离散的单页、或者是内核自身需要建好页表才可以访问的页面，统统收归到用户区。 但是我们要如何表示一个内存区呢？和先前物理内存页面一样，我们需要定义一个数据结构，来表示一个内存区的开始地址和结束地址，里面有多少个物理页面，已经分配了多少个物理页面，剩下多少等等。 #define MA_TYPE_HWAD 1 #define MA_TYPE_KRNL 2 #define MA_TYPE_PROC 3 #define MA_HWAD_LSTART 0 #define MA_HWAD_LSZ 0x2000000 #define MA_HWAD_LEND (MA_HWAD_LSTART+MA_HWAD_LSZ-1) #define MA_KRNL_LSTART 0x2000000 #define MA_KRNL_LSZ (0x40000000-0x2000000) #define MA_KRNL_LEND (MA_KRNL_LSTART+MA_KRNL_LSZ-1) #define MA_PROC_LSTART 0x40000000 #define MA_PROC_LSZ (0xffffffff-0x40000000) #define MA_PROC_LEND (MA_PROC_LSTART+MA_PROC_LSZ) typedef struct s_MEMAREA &#123; list_h_t ma_list; //内存区自身的链表 spinlock_t ma_lock; //保护内存区的自旋锁 uint_t ma_stus; //内存区的状态 uint_t ma_flgs; //内存区的标志 uint_t ma_type; //内存区的类型 sem_t ma_sem; //内存区的信号量 wait_l_head_t ma_waitlst; //内存区的等待队列 uint_t ma_maxpages; //内存区总的页面数 uint_t ma_allocpages; //内存区分配的页面数 uint_t ma_freepages; //内存区空闲的页面数 uint_t ma_resvpages; //内存区保留的页面数 uint_t ma_horizline; //内存区分配时的水位线 adr_t ma_logicstart; //内存区开始地址 adr_t ma_logicend; //内存区结束地址 uint_t ma_logicsz; //内存区大小 //还有一些结构我们这里不关心。后面才会用到 &#125;memarea_t； 关于内存区的数据结构我们就设计好了，但是这仍然不能让我们高效地分配内存，因为我们没有把内存区数据结构和内存页面数据结构关联起来，如果我们现在要分配内存页依然要遍历扫描 msadsc_t 结构数组，这和扫描位图没有本质的区别。 组织内存页照我们之前对 msadsc_t 结构的定义，组织内存页就是组织 msadsc_t 结构，而 msadsc_t 结构中就有一个链表，你大概已经猜到了，我们组织 msadsc_t 结构正是通过另一个数据结构中的链表，将 msadsc_t 结构串连在其中的。 我们需要更加科学合理地组织 msadsc_t 结构，下面我们来定义一个挂载 msadsc_t 结构的数据结构，它其中需要锁、状态、msadsc_t 结构数量，挂载 msadsc_t 结构的链表、和一些统计数据。 typedef struct s_BAFHLST &#123; spinlock_t af_lock; //保护自身结构的自旋锁 u32_t af_stus; //状态 uint_t af_oder; //页面数的位移量 uint_t af_oderpnr; //oder对应的页面数比如 oder为2那就是1&lt;&lt;2=4 uint_t af_fobjnr; //多少个空闲msadsc_t结构，即空闲页面 uint_t af_mobjnr; //此结构的msadsc_t结构总数，即此结构总页面 uint_t af_alcindx; //此结构的分配计数 uint_t af_freindx; //此结构的释放计数 list_h_t af_frelst; //挂载此结构的空闲msadsc_t结构 list_h_t af_alclst; //挂载此结构已经分配的msadsc_t结构 &#125;bafhlst_t; 有了 bafhlst_t 数据结构，我们只是有了挂载 msadsc_t 结构的地方，这并没有做到科学合理。但是，如果我们把多个 bafhlst_t 数据结构组织起来，形成一个 bafhlst_t 结构数组，并且把这个 bafhlst_t 结构数组放在一个更高的数据结构中，这个数据结构就是内存分割合并数据结构——memdivmer_t，那情况就不一样了。 #define MDIVMER_ARR_LMAX 52 typedef struct s_MEMDIVMER &#123; spinlock_t dm_lock; //保护自身结构的自旋锁 u32_t dm_stus; //状态 uint_t dm_divnr; //内存分配次数 uint_t dm_mernr; //内存合并次数 bafhlst_t dm_mdmlielst[MDIVMER_ARR_LMAX];//bafhlst_t结构数组 bafhlst_t dm_onemsalst; //单个的bafhlst_t结构 &#125;memdivmer_t; 那问题来了，内存不是只有两个标准操作吗，这里我们为什么要用分割和合并呢？这其实取意于我们的内存分配、释放算法，对这个算法而言分配内存就是分割内存，而释放内存就是合并内存。 如果 memdivmer_t 结构中 dm_mdmlielst 数组只是一个数组，那是没有意义的。我们正是要通过 dm_mdmlielst 数组，来划分物理内存地址不连续的 msadsc_t 结构。dm_mdmlielst 数组中第 0 个元素挂载单个 msadsc_t 结构，它们的物理内存地址可能对应于 0x1000，0x3000，0x5000。 dm_mdmlielst 数组中第 1 个元素挂载两个连续的 msadsc_t 结构，它们的物理内存地址可能对应于 0x8000～0x9FFF，0xA000～0xBFFF；dm_mdmlielst 数组中第 2 个元素挂载 4 个连续的 msadsc_t 结构，它们的物理内存地址可能对应于 0x100000～0x103FFF，0x104000～0x107FFF…… 依次类推，dm_mdmlielst 数组挂载连续 msadsc_t 结构的数量等于用 1 左移其数组下标，如数组下标为 3，那结果就是 8（1&lt;&lt;3）个连续的 msadsc_t 结构。 我们并不在意其中第一个 msadsc_t 结构对应的内存物理地址从哪里开始，但是第一个 msadsc_t 结构与最后一个 msadsc_t 结构，它们之间的内存物理地址是连续的。 每个内存区 memarea_t 结构中包含一个内存分割合并 memdivmer_t 结构，而在 memdivmer_t 结构中又包含 dm_mdmlielst 数组。在 dm_mdmlielst 数组中挂载了多个 msadsc_t 结构。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.innnovation.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"OS","slug":"OS","permalink":"https://blog.innnovation.cn/tags/OS/"}]},{"title":"操作系统-如何管理内存对象","slug":"操作系统-如何管理内存对象","date":"2021-07-11T16:00:00.000Z","updated":"2021-07-12T07:12:43.389Z","comments":true,"path":"2021/07/12/cao-zuo-xi-tong-ru-he-guan-li-nei-cun-dui-xiang/","link":"","permalink":"https://blog.innnovation.cn/2021/07/12/cao-zuo-xi-tong-ru-he-guan-li-nei-cun-dui-xiang/","excerpt":"","text":"","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.innnovation.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"OS","slug":"OS","permalink":"https://blog.innnovation.cn/tags/OS/"}]},{"title":"操作系统-内存页面初始化","slug":"操作系统-内存页面初始化","date":"2021-07-11T16:00:00.000Z","updated":"2021-07-12T06:58:10.063Z","comments":true,"path":"2021/07/12/cao-zuo-xi-tong-nei-cun-ye-mian-chu-shi-hua/","link":"","permalink":"https://blog.innnovation.cn/2021/07/12/cao-zuo-xi-tong-nei-cun-ye-mian-chu-shi-hua/","excerpt":"","text":"初始化在 hal 层初始化中，初始化了从二级引导器中获取的内存布局信息，也就是那个 e820map_t 数组，并把这个数组转换成了 phymmarge_t 结构数组，还对它做了排序。 物理内存管理器初始化的大总管——init_memmgr 函数，并在 init_halmm 函数中调用 //cosmos/hal/x86/halmm.c中 //hal层的内存初始化函数 void init_halmm() &#123; init_phymmarge(); init_memmgr(); return; &#125; //Cosmos物理内存管理器初始化 void init_memmgr() &#123; //初始化内存页结构msadsc_t //初始化内存区结构memarea_t return; &#125; 在 init_memmgr 函数中应该要完成内存页结构 msadsc_t 和内存区结构 memarea_t 的初始化 内存页结构初始化内存页结构的初始化，其实就是初始化 msadsc_t 结构对应的变量。因为一个 msadsc_t 结构体变量代表一个物理内存页，而物理内存由多个页组成，所以最终会形成一个 msadsc_t 结构体数组。 这会让我们的工作变得简单，我们只需要找一个内存地址，作为 msadsc_t 结构体数组的开始地址，当然这个内存地址必须是可用的，而且之后内存空间足以存放 msadsc_t 结构体数组。 然后，我们要扫描 phymmarge_t 结构体数组中的信息，只要它的类型是可用内存，就建立一个 msadsc_t 结构体，并把其中的开始地址作为第一个页面地址。 接着，要给这个开始地址加上 0x1000，如此循环，直到其结束地址。当这个 phymmarge_t 结构体的地址区间，它对应的所有 msadsc_t 结构体都建立完成之后，就开始下一个 phymmarge_t 结构体。依次类推，最后，我们就能建好所有可用物理内存页面对应的 msadsc_t 结构体。 void write_one_msadsc(msadsc_t *msap, u64_t phyadr) &#123; //对msadsc_t结构做基本的初始化，比如链表、锁、标志位 msadsc_t_init(msap); //这是把一个64位的变量地址转换成phyadrflgs_t*类型方便取得其中的地址位段 phyadrflgs_t *tmp = (phyadrflgs_t *)(&amp;phyadr); //把页的物理地址写入到msadsc_t结构中 msap->md_phyadrs.paf_padrs = tmp->paf_padrs; return; &#125; u64_t init_msadsc_core(machbstart_t *mbsp, msadsc_t *msavstart, u64_t msanr) &#123; //获取phymmarge_t结构数组开始地址 phymmarge_t *pmagep = (phymmarge_t *)phyadr_to_viradr((adr_t)mbsp->mb_e820expadr); u64_t mdindx = 0; //扫描phymmarge_t结构数组 for (u64_t i = 0; i &lt; mbsp->mb_e820exnr; i++) &#123; //判断phymmarge_t结构的类型是不是可用内存 if (PMR_T_OSAPUSERRAM == pmagep[i].pmr_type) &#123; //遍历phymmarge_t结构的地址区间 for (u64_t start = pmagep[i].pmr_saddr; start &lt; pmagep[i].pmr_end; start += 4096) &#123; //每次加上4KB-1比较是否小于等于phymmarge_t结构的结束地址 if ((start + 4096 - 1) &lt;= pmagep[i].pmr_end) &#123; //与当前地址为参数写入第mdindx个msadsc结构 write_one_msadsc(&amp;msavstart[mdindx], start); mdindx++; &#125; &#125; &#125; &#125; return mdindx; &#125; void init_msadsc() &#123; u64_t coremdnr = 0, msadscnr = 0; msadsc_t *msadscvp = NULL; machbstart_t *mbsp = &amp;kmachbsp; //计算msadsc_t结构数组的开始地址和数组元素个数 if (ret_msadsc_vadrandsz(mbsp, &amp;msadscvp, &amp;msadscnr) == FALSE) &#123; system_error(\"init_msadsc ret_msadsc_vadrandsz err\\n\"); &#125; //开始真正初始化msadsc_t结构数组 coremdnr = init_msadsc_core(mbsp, msadscvp, msadscnr); if (coremdnr != msadscnr) &#123; system_error(\"init_msadsc init_msadsc_core err\\n\"); &#125; //将msadsc_t结构数组的开始的物理地址写入kmachbsp结构中 mbsp->mb_memmappadr = viradr_to_phyadr((adr_t)msadscvp); //将msadsc_t结构数组的元素个数写入kmachbsp结构中 mbsp->mb_memmapnr = coremdnr; //将msadsc_t结构数组的大小写入kmachbsp结构中 mbsp->mb_memmapsz = coremdnr * sizeof(msadsc_t); //计算下一个空闲内存的开始地址 mbsp->mb_nextwtpadr = PAGE_ALIGN(mbsp->mb_memmappadr + mbsp->mb_memmapsz); return; &#125; 其中的 ret_msadsc_vadrandsz 函数也是遍历 phymmarge_t 结构数组，计算出有多大的可用内存空间，可以分成多少个页面，需要多少个 msadsc_t 结构。 内存区结构初始化我们将整个物理地址空间在逻辑上分成了三个区，分别是：硬件区、内核区、用户区，这就要求我们要在内存中建立三个 memarea_t 结构体的实例变量。 我们只需要在内存中找个空闲空间，存放这三个 memarea_t 结构体就行。相比建立 msadsc_t 结构数组这更为简单，因为 memarea_t 结构体是顶层结构，并不依赖其它数据结构，只是对其本身进行初始化就好了. void bafhlst_t_init(bafhlst_t *initp, u32_t stus, uint_t oder, uint_t oderpnr) &#123; //初始化bafhlst_t结构体的基本数据 knl_spinlock_init(&amp;initp->af_lock); initp->af_stus = stus; initp->af_oder = oder; initp->af_oderpnr = oderpnr; initp->af_fobjnr = 0; initp->af_mobjnr = 0; initp->af_alcindx = 0; initp->af_freindx = 0; list_init(&amp;initp->af_frelst); list_init(&amp;initp->af_alclst); list_init(&amp;initp->af_ovelst); return; &#125; void memdivmer_t_init(memdivmer_t *initp) &#123; //初始化medivmer_t结构体的基本数据 knl_spinlock_init(&amp;initp->dm_lock); initp->dm_stus = 0; initp->dm_divnr = 0; initp->dm_mernr = 0; //循环初始化memdivmer_t结构体中dm_mdmlielst数组中的每个bafhlst_t结构的基本数据 for (uint_t li = 0; li &lt; MDIVMER_ARR_LMAX; li++) &#123; bafhlst_t_init(&amp;initp->dm_mdmlielst[li], BAFH_STUS_DIVM, li, (1UL &lt;&lt; li)); &#125; bafhlst_t_init(&amp;initp->dm_onemsalst, BAFH_STUS_ONEM, 0, 1UL); return; &#125; void memarea_t_init(memarea_t *initp) &#123; //初始化memarea_t结构体的基本数据 list_init(&amp;initp->ma_list); knl_spinlock_init(&amp;initp->ma_lock); initp->ma_stus = 0; initp->ma_flgs = 0; initp->ma_type = MA_TYPE_INIT; initp->ma_maxpages = 0; initp->ma_allocpages = 0; initp->ma_freepages = 0; initp->ma_resvpages = 0; initp->ma_horizline = 0; initp->ma_logicstart = 0; initp->ma_logicend = 0; initp->ma_logicsz = 0; //初始化memarea_t结构体中的memdivmer_t结构体 memdivmer_t_init(&amp;initp->ma_mdmdata); initp->ma_privp = NULL; return; &#125; bool_t init_memarea_core(machbstart_t *mbsp) &#123; //获取memarea_t结构开始地址 u64_t phymarea = mbsp->mb_nextwtpadr; //检查内存空间够不够放下MEMAREA_MAX个memarea_t结构实例变量 if (initchkadr_is_ok(mbsp, phymarea, (sizeof(memarea_t) * MEMAREA_MAX)) != 0) &#123; return FALSE; &#125; memarea_t *virmarea = (memarea_t *)phyadr_to_viradr((adr_t)phymarea); for (uint_t mai = 0; mai &lt; MEMAREA_MAX; mai++) &#123; //循环初始化每个memarea_t结构实例变量 memarea_t_init(&amp;virmarea[mai]); &#125; //设置硬件区的类型和空间大小 virmarea[0].ma_type = MA_TYPE_HWAD; virmarea[0].ma_logicstart = MA_HWAD_LSTART; virmarea[0].ma_logicend = MA_HWAD_LEND; virmarea[0].ma_logicsz = MA_HWAD_LSZ; //设置内核区的类型和空间大小 virmarea[1].ma_type = MA_TYPE_KRNL; virmarea[1].ma_logicstart = MA_KRNL_LSTART; virmarea[1].ma_logicend = MA_KRNL_LEND; virmarea[1].ma_logicsz = MA_KRNL_LSZ; //设置应用区的类型和空间大小 virmarea[2].ma_type = MA_TYPE_PROC; virmarea[2].ma_logicstart = MA_PROC_LSTART; virmarea[2].ma_logicend = MA_PROC_LEND; virmarea[2].ma_logicsz = MA_PROC_LSZ; //将memarea_t结构的开始的物理地址写入kmachbsp结构中 mbsp->mb_memznpadr = phymarea; //将memarea_t结构的个数写入kmachbsp结构中 mbsp->mb_memznnr = MEMAREA_MAX; //将所有memarea_t结构的大小写入kmachbsp结构中 mbsp->mb_memznsz = sizeof(memarea_t) * MEMAREA_MAX; //计算下一个空闲内存的开始地址 mbsp->mb_nextwtpadr = PAGE_ALIGN(phymarea + sizeof(memarea_t) * MEMAREA_MAX); return TRUE; &#125; //初始化内存区 void init_memarea() &#123; //真正初始化内存区 if (init_memarea_core(&amp;kmachbsp) == FALSE) &#123; system_error(\"init_memarea_core fail\"); &#125; return; &#125; 在 init_memarea_core 函数的开始，我们调用了 memarea_t_init 函数，对 MEMAREA_MAX 个 memarea_t 结构进行了基本的初始化。 然后，在 memarea_t_init 函数中又调用了 memdivmer_t_init 函数，而在 memdivmer_t_init 函数中又调用了 bafhlst_t_init 函数，这保证了那些被包含的数据结构得到了初始化。 最后，我们给三个区分别设置了类型和地址空间。 处理初始内存占用问题目前我们的内存中已经有很多数据了，有 os 内核本身的执行文件，有字体文件，有 MMU 页表，有打包的内核映像文件，还有刚刚建立的内存页和内存区的数据结构，这些数据都要占用实际的物理内存 再回头看看我们建立内存页结构 msadsc_t，所有的都是空闲状态，而它们每一个都表示一个实际的物理内存页。 假如在这种情况下，对调用内存分配接口进行内存分配，它按既定的分配算法查找空闲的 msadsc_t 结构，那它一定会找到内核占用的内存页所对应的 msadsc_t 结构，并把这个内存页分配出去，然后得到这个页面的程序对其进行改写。这样内核数据就会被覆盖，这种情况是我们绝对不能允许的。 所以，我们要把这些已经占用的内存页面所对应的 msadsc_t 结构标记出来，标记成已分配，这样内存分配算法就不会找到它们了。 要解决这个问题，我们只要给出被占用内存的起始地址和结束地址，然后从起始地址开始查找对应的 msadsc_t 结构，再把它标记为已经分配，最后直到查找到结束地址为止。 //搜索一段内存地址空间所对应的msadsc_t结构 u64_t search_segment_occupymsadsc(msadsc_t *msastart, u64_t msanr, u64_t ocpystat, u64_t ocpyend) &#123; u64_t mphyadr = 0, fsmsnr = 0; msadsc_t *fstatmp = NULL; for (u64_t mnr = 0; mnr &lt; msanr; mnr++) &#123; if ((msastart[mnr].md_phyadrs.paf_padrs &lt;&lt; PSHRSIZE) == ocpystat) &#123; //找出开始地址对应的第一个msadsc_t结构，就跳转到step1 fstatmp = &amp;msastart[mnr]; goto step1; &#125; &#125; step1: fsmsnr = 0; if (NULL == fstatmp) &#123; return 0; &#125; for (u64_t tmpadr = ocpystat; tmpadr &lt; ocpyend; tmpadr += PAGESIZE, fsmsnr++) &#123; //从开始地址对应的第一个msadsc_t结构开始设置，直到结束地址对应的最后一个masdsc_t结构 mphyadr = fstatmp[fsmsnr].md_phyadrs.paf_padrs &lt;&lt; PSHRSIZE; if (mphyadr != tmpadr) &#123; return 0; &#125; if (MF_MOCTY_FREE != fstatmp[fsmsnr].md_indxflgs.mf_mocty || 0 != fstatmp[fsmsnr].md_indxflgs.mf_uindx || PAF_NO_ALLOC != fstatmp[fsmsnr].md_phyadrs.paf_alloc) &#123; return 0; &#125; //设置msadsc_t结构为已经分配，已经分配给内核 fstatmp[fsmsnr].md_indxflgs.mf_mocty = MF_MOCTY_KRNL; fstatmp[fsmsnr].md_indxflgs.mf_uindx++; fstatmp[fsmsnr].md_phyadrs.paf_alloc = PAF_ALLOC; &#125; //进行一些数据的正确性检查 u64_t ocpysz = ocpyend - ocpystat; if ((ocpysz &amp; 0xfff) != 0) &#123; if (((ocpysz >> PSHRSIZE) + 1) != fsmsnr) &#123; return 0; &#125; return fsmsnr; &#125; if ((ocpysz >> PSHRSIZE) != fsmsnr) &#123; return 0; &#125; return fsmsnr; &#125; bool_t search_krloccupymsadsc_core(machbstart_t *mbsp) &#123; u64_t retschmnr = 0; msadsc_t *msadstat = (msadsc_t *)phyadr_to_viradr((adr_t)mbsp->mb_memmappadr); u64_t msanr = mbsp->mb_memmapnr; //搜索BIOS中断表占用的内存页所对应msadsc_t结构 retschmnr = search_segment_occupymsadsc(msadstat, msanr, 0, 0x1000); if (0 == retschmnr) &#123; return FALSE; &#125; //搜索内核栈占用的内存页所对应msadsc_t结构 retschmnr = search_segment_occupymsadsc(msadstat, msanr, mbsp->mb_krlinitstack &amp; (~(0xfffUL)), mbsp->mb_krlinitstack); if (0 == retschmnr) &#123; return FALSE; &#125; //搜索内核占用的内存页所对应msadsc_t结构 retschmnr = search_segment_occupymsadsc(msadstat, msanr, mbsp->mb_krlimgpadr, mbsp->mb_nextwtpadr); if (0 == retschmnr) &#123; return FALSE; &#125; //搜索内核映像文件占用的内存页所对应msadsc_t结构 retschmnr = search_segment_occupymsadsc(msadstat, msanr, mbsp->mb_imgpadr, mbsp->mb_imgpadr + mbsp->mb_imgsz); if (0 == retschmnr) &#123; return FALSE; &#125; return TRUE; &#125; //初始化搜索内核占用的内存页面 void init_search_krloccupymm(machbstart_t *mbsp) &#123; //实际初始化搜索内核占用的内存页面 if (search_krloccupymsadsc_core(mbsp) == FALSE) &#123; system_error(\"search_krloccupymsadsc_core fail\\n\"); &#125; return; &#125; 这三个函数逻辑很简单，由 init_search_krloccupymm 函数入口，search_krloccupymsadsc_core 函数驱动，由 search_segment_occupymsadsc 函数完成实际的工作。由于初始化阶段各种数据占用的开始、结束地址和大小，这些信息都保存在 machbstart_t 类型的 kmachbsp 变量中，所以函数与 machbstart_t 类型的指针为参数。其实 phymmarge_t、msadsc_t、memarea_t 这些结构的实例变量和 MMU 页表，它们所占用的内存空间已经涵盖在了内核自身占用的内存空间。 合并内存页到内存区1.确定内存页属于哪个区，即标定一系列 msadsc_t 结构是属于哪个 memarea_t 结构的。 2.把特定的内存页合并，然后挂载到特定的内存区下的 memdivmer_t 结构中的 dm_mdmlielst 数组中。 我们先来做第一件事，这件事比较简单，我们只要遍历每个 memarea_t 结构，遍历过程中根据特定的 memarea_t 结构，然后去扫描整个 msadsc_t 结构数组，最后依次对比 msadsc_t 的物理地址，看它是否落在 memarea_t 结构的地址区间中。如果是，就把这个 memarea_t 结构的类型值写入 msadsc_t 结构中，这样就一个一个打上了标签，遍历 memarea_t 结构结束之后，每个 msadsc_t 结构就只归属于某一个 memarea_t 结构了。 //给msadsc_t结构打上标签 uint_t merlove_setallmarflgs_onmemarea(memarea_t *mareap, msadsc_t *mstat, uint_t msanr) &#123; u32_t muindx = 0; msadflgs_t *mdfp = NULL; //获取内存区类型 switch (mareap->ma_type)&#123; case MA_TYPE_HWAD: muindx = MF_MARTY_HWD &lt;&lt; 5;//硬件区标签 mdfp = (msadflgs_t *)(&amp;muindx); break; case MA_TYPE_KRNL: muindx = MF_MARTY_KRL &lt;&lt; 5;//内核区标签 mdfp = (msadflgs_t *)(&amp;muindx); break; case MA_TYPE_PROC: muindx = MF_MARTY_PRC &lt;&lt; 5;//应用区标签 mdfp = (msadflgs_t *)(&amp;muindx); break; &#125; u64_t phyadr = 0; uint_t retnr = 0; //扫描所有的msadsc_t结构 for (uint_t mix = 0; mix &lt; msanr; mix++) &#123; if (MF_MARTY_INIT == mstat[mix].md_indxflgs.mf_marty) &#123; //获取msadsc_t结构对应的地址 phyadr = mstat[mix].md_phyadrs.paf_padrs &lt;&lt; PSHRSIZE; //和内存区的地址区间比较 if (phyadr >= mareap->ma_logicstart &amp;&amp; ((phyadr + PAGESIZE) - 1) &lt;= mareap->ma_logicend) &#123; //设置msadsc_t结构的标签 mstat[mix].md_indxflgs.mf_marty = mdfp->mf_marty; retnr++; &#125; &#125; &#125; return retnr; &#125; bool_t merlove_mem_core(machbstart_t *mbsp) &#123; //获取msadsc_t结构的首地址 msadsc_t *mstatp = (msadsc_t *)phyadr_to_viradr((adr_t)mbsp->mb_memmappadr); //获取msadsc_t结构的个数 uint_t msanr = (uint_t)mbsp->mb_memmapnr, maxp = 0; //获取memarea_t结构的首地址 memarea_t *marea = (memarea_t *)phyadr_to_viradr((adr_t)mbsp->mb_memznpadr); uint_t sretf = ~0UL, tretf = ~0UL; //遍历每个memarea_t结构 for (uint_t mi = 0; mi &lt; (uint_t)mbsp->mb_memznnr; mi++) &#123; //针对其中一个memarea_t结构给msadsc_t结构打上标签 sretf = merlove_setallmarflgs_onmemarea(&amp;marea[mi], mstatp, msanr); if ((~0UL) == sretf) &#123; return FALSE; &#125; &#125; //遍历每个memarea_t结构 for (uint_t maidx = 0; maidx &lt; (uint_t)mbsp->mb_memznnr; maidx++) &#123; //针对其中一个memarea_t结构对msadsc_t结构进行合并 if (merlove_mem_onmemarea(&amp;marea[maidx], mstatp, msanr) == FALSE) &#123; return FALSE; &#125; maxp += marea[maidx].ma_maxpages; &#125; return TRUE; &#125; //初始化页面合并 void init_merlove_mem() &#123; if (merlove_mem_core(&amp;kmachbsp) == FALSE) &#123; system_error(\"merlove_mem_core fail\\n\"); &#125; return; &#125; 从 init_merlove_mem 函数开始，但是它并不实际干活，作为入口函数，它调用的 merlove_mem_core 函数才是真正干活的。这个 merlove_mem_core 函数有两个遍历内存区，第一次遍历是为了完成上述第一步：确定内存页属于哪个区。当确定内存页属于哪个区之后，就来到了第二次遍历 memarea_t 结构，合并其中的 msadsc_t 结构，并把它们挂载到其中的 memdivmer_t 结构下的 dm_mdmlielst 数组中。 第一，它要保证其中所有的 msadsc_t 结构挂载到 dm_mdmlielst 数组中合适的 bafhlst_t 结构中。 第二，它要保证多个 msadsc_t 结构有最大的连续性。 举个例子，比如一个内存区中有 12 个页面，其中 10 个页面是连续的地址为 0～0x9000，还有两个页面其中一个地址为 0xb000，另一个地址为 0xe000。 这样的情况下，需要多个页面保持最大的连续性，还有在 m_mdmlielst 数组中找到合适的 bafhlst_t 结构。 那么：0～0x7000 这 8 个页面就要挂载到 m_mdmlielst 数组中第 3 个 bafhlst_t 结构中；0x8000～0x9000 这 2 个页面要挂载到 m_mdmlielst 数组中第 1 个 bafhlst_t 结构中，而 0xb000 和 0xe000 这 2 个页面都要挂载到 m_mdmlielst 数组中第 0 个 bafhlst_t 结构中。 从上述代码可以看出，遍历每个内存区，然后针对其中每一个内存区进行 msadsc_t 结构的合并操作，完成这个操作的是 merlove_mem_onmemarea，我们这就去写好这个函数，代码如下所示。 bool_t continumsadsc_add_bafhlst(memarea_t *mareap, bafhlst_t *bafhp, msadsc_t *fstat, msadsc_t *fend, uint_t fmnr) &#123; fstat->md_indxflgs.mf_olkty = MF_OLKTY_ODER; //开始的msadsc_t结构指向最后的msadsc_t结构 fstat->md_odlink = fend; fend->md_indxflgs.mf_olkty = MF_OLKTY_BAFH; //最后的msadsc_t结构指向它属于的bafhlst_t结构 fend->md_odlink = bafhp; //把多个地址连续的msadsc_t结构的的开始的那个msadsc_t结构挂载到bafhlst_t结构的af_frelst中 list_add(&amp;fstat->md_list, &amp;bafhp->af_frelst); //更新bafhlst_t的统计数据 bafhp->af_fobjnr++; bafhp->af_mobjnr++; //更新内存区的统计数据 mareap->ma_maxpages += fmnr; mareap->ma_freepages += fmnr; mareap->ma_allmsadscnr += fmnr; return TRUE; &#125; bool_t continumsadsc_mareabafh_core(memarea_t *mareap, msadsc_t **rfstat, msadsc_t **rfend, uint_t *rfmnr) &#123; uint_t retval = *rfmnr, tmpmnr = 0; msadsc_t *mstat = *rfstat, *mend = *rfend; //根据地址连续的msadsc_t结构的数量查找合适bafhlst_t结构 bafhlst_t *bafhp = find_continumsa_inbafhlst(mareap, retval); //判断bafhlst_t结构状态和类型对不对 if ((BAFH_STUS_DIVP == bafhp->af_stus || BAFH_STUS_DIVM == bafhp->af_stus) &amp;&amp; MA_TYPE_PROC != mareap->ma_type) &#123; //看地址连续的msadsc_t结构的数量是不是正好是bafhp->af_oderpnr tmpmnr = retval - bafhp->af_oderpnr; //根据地址连续的msadsc_t结构挂载到bafhlst_t结构中 if (continumsadsc_add_bafhlst(mareap, bafhp, mstat, &amp;mstat[bafhp->af_oderpnr - 1], bafhp->af_oderpnr) == FALSE) &#123; return FALSE; &#125; //如果地址连续的msadsc_t结构的数量正好是bafhp->af_oderpnr则完成，否则返回再次进入此函数 if (tmpmnr == 0) &#123; *rfmnr = tmpmnr; *rfend = NULL; return TRUE; &#125; //挂载bafhp->af_oderpnr地址连续的msadsc_t结构到bafhlst_t中 *rfstat = &amp;mstat[bafhp->af_oderpnr]; //还剩多少个地址连续的msadsc_t结构 *rfmnr = tmpmnr; return TRUE; &#125; return FALSE; &#125; bool_t merlove_continumsadsc_mareabafh(memarea_t *mareap, msadsc_t *mstat, msadsc_t *mend, uint_t mnr) &#123; uint_t mnridx = mnr; msadsc_t *fstat = mstat, *fend = mend; //如果mnridx > 0并且NULL != fend就循环调用continumsadsc_mareabafh_core函数，而mnridx和fend由这个函数控制 for (; (mnridx > 0 &amp;&amp; NULL != fend);) &#123; //为一段地址连续的msadsc_t结构寻找合适m_mdmlielst数组中的bafhlst_t结构 continumsadsc_mareabafh_core(mareap, &amp;fstat, &amp;fend, &amp;mnridx) &#125; return TRUE; &#125; bool_t merlove_scan_continumsadsc(memarea_t *mareap, msadsc_t *fmstat, uint_t *fntmsanr, uint_t fmsanr, msadsc_t **retmsastatp, msadsc_t **retmsaendp, uint_t *retfmnr) &#123; u32_t muindx = 0; msadflgs_t *mdfp = NULL; msadsc_t *msastat = fmstat; uint_t retfindmnr = 0; bool_t rets = FALSE; uint_t tmidx = *fntmsanr; //从外层函数的fntmnr变量开始遍历所有msadsc_t结构 for (; tmidx &lt; fmsanr; tmidx++) &#123; //一个msadsc_t结构是否属于这个内存区，是否空闲 if (msastat[tmidx].md_indxflgs.mf_marty == mdfp->mf_marty &amp;&amp; 0 == msastat[tmidx].md_indxflgs.mf_uindx &amp;&amp; MF_MOCTY_FREE == msastat[tmidx].md_indxflgs.mf_mocty &amp;&amp; PAF_NO_ALLOC == msastat[tmidx].md_phyadrs.paf_alloc) &#123; //返回从这个msadsc_t结构开始到下一个非空闲、地址非连续的msadsc_t结构对应的msadsc_t结构索引号到retfindmnr变量中 rets = scan_len_msadsc(&amp;msastat[tmidx], mdfp, fmsanr, &amp;retfindmnr); //下一轮开始的msadsc_t结构索引 *fntmsanr = tmidx + retfindmnr + 1; //当前地址连续msadsc_t结构的开始地址 *retmsastatp = &amp;msastat[tmidx]; //当前地址连续msadsc_t结构的结束地址 *retmsaendp = &amp;msastat[tmidx + retfindmnr]; //当前有多少个地址连续msadsc_t结构 *retfmnr = retfindmnr + 1; return TRUE; &#125; &#125; return FALSE; &#125; bool_t merlove_mem_onmemarea(memarea_t *mareap, msadsc_t *mstat, uint_t msanr) &#123; msadsc_t *retstatmsap = NULL, *retendmsap = NULL, *fntmsap = mstat; uint_t retfindmnr = 0; uint_t fntmnr = 0; bool_t retscan = FALSE; for (; fntmnr &lt; msanr;) &#123; //获取最多且地址连续的msadsc_t结构体的开始、结束地址、一共多少个msadsc_t结构体，下一次循环的fntmnr retscan = merlove_scan_continumsadsc(mareap, fntmsap, &amp;fntmnr, msanr, &amp;retstatmsap, &amp;retendmsap, &amp;retfindmnr); if (NULL != retstatmsap &amp;&amp; NULL != retendmsap) &#123; //把一组连续的msadsc_t结构体挂载到合适的m_mdmlielst数组中的bafhlst_t结构中 merlove_continumsadsc_mareabafh(mareap, retstatmsap, retendmsap, retfindmnr) &#125; &#125; return TRUE; &#125; 上述代码中，整体上分为两步。 第一步，通过 merlove_scan_continumsadsc 函数，返回最多且地址连续的 msadsc_t 结构体的开始、结束地址、一共多少个 msadsc_t 结构体，下一轮开始的 msadsc_t 结构体的索引号。 第二步，根据第一步获取的信息调用 merlove_continumsadsc_mareabafh 函数，把第一步返回那一组连续的 msadsc_t 结构体，挂载到合适的 m_mdmlielst 数组中的 bafhlst_t 结构中。详细的逻辑已经在注释中说明。 初始化汇总根据前面内存管理数据结构的关系，很显然，它们的调用次序很重要，谁先谁后都有严格的规定，这关乎内存管理初始化的成败。所以，现在我们就在先前的 init_memmgr 函数中去调用它们，代码如下所示。 void init_memmgr() &#123; //初始化内存页结构 init_msadsc(); //初始化内存区结构 init_memarea(); //处理内存占用 init_search_krloccupymm(&amp;kmachbsp); //合并内存页到内存区中 init_merlove_mem(); init_memmgrob(); return; &#125; phymmarge_t 结构体的地址和数量、msadsc_t 结构体的地址和数据、memarea_t 结构体的地址和数量都保存在了 kmachbsp 变量中，这个变量其实不是用来管理内存的，而且它里面放的是物理地址 内核使用的是虚拟地址，每次都要转换极不方便，所以我们要设计一个专用的数据结构，用于内存管理 //cosmos/include/halinc/halglobal.c HAL_DEFGLOB_VARIABLE(memmgrob_t,memmgrob); typedef struct s_MEMMGROB &#123; list_h_t mo_list; spinlock_t mo_lock; //保护自身自旋锁 uint_t mo_stus; //状态 uint_t mo_flgs; //标志 u64_t mo_memsz; //内存大小 u64_t mo_maxpages; //内存最大页面数 u64_t mo_freepages; //内存最大空闲页面数 u64_t mo_alocpages; //内存最大分配页面数 u64_t mo_resvpages; //内存保留页面数 u64_t mo_horizline; //内存分配水位线 phymmarge_t* mo_pmagestat; //内存空间布局结构指针 u64_t mo_pmagenr; msadsc_t* mo_msadscstat; //内存页面结构指针 u64_t mo_msanr; memarea_t* mo_mareastat; //内存区结构指针 u64_t mo_mareanr; &#125;memmgrob_t; //cosmos/hal/x86/memmgrinit.c void memmgrob_t_init(memmgrob_t *initp) &#123; list_init(&amp;initp->mo_list); knl_spinlock_init(&amp;initp->mo_lock); initp->mo_stus = 0; initp->mo_flgs = 0; initp->mo_memsz = 0; initp->mo_maxpages = 0; initp->mo_freepages = 0; initp->mo_alocpages = 0; initp->mo_resvpages = 0; initp->mo_horizline = 0; initp->mo_pmagestat = NULL; initp->mo_pmagenr = 0; initp->mo_msadscstat = NULL; initp->mo_msanr = 0; initp->mo_mareastat = NULL; initp->mo_mareanr = 0; return; &#125; void init_memmgrob() &#123; machbstart_t *mbsp = &amp;kmachbsp; memmgrob_t *mobp = &amp;memmgrob; memmgrob_t_init(mobp); mobp->mo_pmagestat = (phymmarge_t *)phyadr_to_viradr((adr_t)mbsp->mb_e820expadr); mobp->mo_pmagenr = mbsp->mb_e820exnr; mobp->mo_msadscstat = (msadsc_t *)phyadr_to_viradr((adr_t)mbsp->mb_memmappadr); mobp->mo_msanr = mbsp->mb_memmapnr; mobp->mo_mareastat = (memarea_t *)phyadr_to_viradr((adr_t)mbsp->mb_memznpadr); mobp->mo_mareanr = mbsp->mb_memznnr; mobp->mo_memsz = mbsp->mb_memmapnr &lt;&lt; PSHRSIZE; mobp->mo_maxpages = mbsp->mb_memmapnr; uint_t aidx = 0; for (uint_t i = 0; i &lt; mobp->mo_msanr; i++) &#123; if (1 == mobp->mo_msadscstat[i].md_indxflgs.mf_uindx &amp;&amp; MF_MOCTY_KRNL == mobp->mo_msadscstat[i].md_indxflgs.mf_mocty &amp;&amp; PAF_ALLOC == mobp->mo_msadscstat[i].md_phyadrs.paf_alloc) &#123; aidx++; &#125; &#125; mobp->mo_alocpages = aidx; mobp->mo_freepages = mobp->mo_maxpages - mobp->mo_alocpages; return; &#125; 整理一、流程：init_hal-&gt;init_halmm-&gt;init_memmgr//每个页对应一个msadsc_t 结构体，循环填充msadsc_t 结构体数组-&gt;init_msadsc//初始化三类memarea_t，硬件区、内核区、用户区-&gt;init_memarea//对已使用的页打上标记，包括：BIOS中断表、内核栈、内核、内核映像-&gt;init_search_krloccupymm(&amp;kmachbsp);//将页面按地址范围，分配给内存区//然后按顺序依次查找最长连续的页面，根据连续页面的长度，//将这些页面的msadsc_t挂载到memdivmer_t 结构下的bafhlst_t数组dm_mdmlielst中-&gt;init_merlove_mem();//物理地址转为虚拟地址，便于以后使用-&gt;init_memmgrob(); 二、对于最后的问题，用了虚拟机进行测试，但无论内存大小，总有56K内存没能找到【有知道的小伙伴，麻烦帮忙解答一下】：1、4G内存情况如下：理论内存：0x1 0000 0000 = 4,194,304K可用内存：0xfff8fc00 = 4,193,855K预留区域：0x52400 = 329K硬件使用：0x10000 = 64K没能找到：0xE000 = 56K msadsc_t结构体大小为40，使用内存总计为：4,193,855K/4K*40=41,938,520=接近40M 2、2G内存情况如下理论内存：0x8000 0000 =2,097,152K可用内存：0x7ff8fc00 = 2,096,703K预留区域：0x52400 = 329K硬件使用：0x10000 = 64K没能找到：0xE000 = 56K msadsc_t结构体大小为40，使用内存总计为：2,096,703K/4K*40=20,967,030=接近20M 3、1G内存情况如下理论内存：0x4000 0000= 1,048,576K可用内存：0x3ff8fc00 = 1,048,127K预留区域：0x52400 = 329K硬件使用：0x10000 = 64K没能找到：0xE000 = 56K msadsc_t结构体大小为40，使用内存总计为：1,048,127K/4K*40=10,481,270=接近10M 三、如果想节约msadsc_t内存的话，感觉有几种方案：1、最简单的方法，就是大内存时采用更大的分页，但应用在申请内存时，同样会有更多内存浪费2、也可以用更复杂的页面管理机制，比如相同属性的连续页面不要用多个单独msadsc_t表示，而用一个msadsc_t表示并标明其范围，并通过skiplist等数据结构加速查询。但无论是申请内存还是归还内存时，性能会有所下降，感觉得不偿失。3、页面分组情况较少的时候，可以通过每个组建立一个链表记录哪些页面属于某个链表，而msadsc_t中只记录地址等少量信息，不适合复杂系统。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.innnovation.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"OS","slug":"OS","permalink":"https://blog.innnovation.cn/tags/OS/"}]},{"title":"操作系统-实现内存页分配与释放","slug":"操作系统-实现内存页的分配与释放","date":"2021-07-11T16:00:00.000Z","updated":"2021-07-12T07:08:51.141Z","comments":true,"path":"2021/07/12/cao-zuo-xi-tong-shi-xian-nei-cun-ye-de-fen-pei-yu-shi-fang/","link":"","permalink":"https://blog.innnovation.cn/2021/07/12/cao-zuo-xi-tong-shi-xian-nei-cun-ye-de-fen-pei-yu-shi-fang/","excerpt":"","text":"内存页的分配如果让你实现一次只分配一个页面，我相信这个问题很好解决，因为你只需要写一段循环代码，在其中遍历出一个空闲的 msadsc_t 结构，就可以返回了，这个算法就可以结束了 但现实却不容许我们这么简单地处理问题，我们内存管理器要为内核、驱动，还有应用提供服务，它们对请求内存页面的多少、内存页面是不是连续，内存页面所处的物理地址都有要求。 内存分配的接口函数 //内存分配页面框架函数 msadsc_t *mm_divpages_fmwk(memmgrob_t *mmobjp, uint_t pages, uint_t *retrelpnr, uint_t mrtype, uint_t flgs) &#123; //返回mrtype对应的内存区结构的指针 memarea_t *marea = onmrtype_retn_marea(mmobjp, mrtype); if (NULL == marea) &#123; *retrelpnr = 0; return NULL; &#125; uint_t retpnr = 0; //内存分配的核心函数 msadsc_t *retmsa = mm_divpages_core(marea, pages, &amp;retpnr, flgs); if (NULL == retmsa) &#123; *retrelpnr = 0; return NULL; &#125; *retrelpnr = retpnr; return retmsa; &#125; //内存分配页面接口 //mmobjp->内存管理数据结构指针 //pages->请求分配的内存页面数 //retrealpnr->存放实际分配内存页面数的指针 //mrtype->请求的分配内存页面的内存区类型 //flgs->请求分配的内存页面的标志位 msadsc_t *mm_division_pages(memmgrob_t *mmobjp, uint_t pages, uint_t *retrealpnr, uint_t mrtype, uint_t flgs) &#123; if (NULL == mmobjp || NULL == retrealpnr || 0 == mrtype) &#123; return NULL; &#125; uint_t retpnr = 0; msadsc_t *retmsa = mm_divpages_fmwk(mmobjp, pages, &amp;retpnr, mrtype, flgs); if (NULL == retmsa) &#123; *retrealpnr = 0; return NULL; &#125; *retrealpnr = retpnr; return retmsa; &#125; 我们内存管理代码的结构是：接口函数调用框架函数，框架函数调用核心函数。可以发现，这个接口函数返回的是一个 msadsc_t 结构的指针，如果是多个页面返回的就是起始页面对应的 msadsc_t 结构的指针。 为什么不直接返回内存的物理地址呢？因为我们物理内存管理器是最底层的内存管理器，而上层代码中可能需要页面的相关信息，所以直接返回页面对应 msadsc_t 结构的指针。 还有一个参数是用于返回实际分配的页面数的。比如，内核功能代码请求分配三个页面，我们的内存管理器不能分配三个页面，只能分配两个或四个页面，这时内存管理器就会分配四个页面返回，retrealpnr 指向的变量中就存放数字 4，表示实际分配页面的数量。 有了内存分配接口、框架函数，下面我们来实现内存分配的核心函数，代码如下 bool_t onmpgs_retn_bafhlst(memarea_t *malckp, uint_t pages, bafhlst_t **retrelbafh, bafhlst_t **retdivbafh) &#123; //获取bafhlst_t结构数组的开始地址 bafhlst_t *bafhstat = malckp->ma_mdmdata.dm_mdmlielst; //根据分配页面数计算出分配页面在dm_mdmlielst数组中下标 sint_t dividx = retn_divoder(pages); //从第dividx个数组元素开始搜索 for (sint_t idx = dividx; idx &lt; MDIVMER_ARR_LMAX; idx++) &#123; //如果第idx个数组元素对应的一次可分配连续的页面数大于等于请求的页面数，且其中的可分配对象大于0则返回 if (bafhstat[idx].af_oderpnr >= pages &amp;&amp; 0 &lt; bafhstat[idx].af_fobjnr) &#123; //返回请求分配的bafhlst_t结构指针 *retrelbafh = &amp;bafhstat[dividx]; //返回实际分配的bafhlst_t结构指针 *retdivbafh = &amp;bafhstat[idx]; return TRUE; &#125; &#125; *retrelbafh = NULL; *retdivbafh = NULL; return FALSE; &#125; msadsc_t *mm_reldivpages_onmarea(memarea_t *malckp, uint_t pages, uint_t *retrelpnr) &#123; bafhlst_t *retrelbhl = NULL, *retdivbhl = NULL; //根据页面数在内存区的m_mdmlielst数组中找出其中请求分配页面的bafhlst_t结构（retrelbhl）和实际要在其中分配页面的bafhlst_t结构(retdivbhl) bool_t rets = onmpgs_retn_bafhlst(malckp, pages, &amp;retrelbhl, &amp;retdivbhl); if (FALSE == rets) &#123; *retrelpnr = 0; return NULL; &#125; uint_t retpnr = 0; //实际在bafhlst_t结构中分配页面 msadsc_t *retmsa = mm_reldpgsdivmsa_bafhl(malckp, pages, &amp;retpnr, retrelbhl, retdivbhl); if (NULL == retmsa) &#123; *retrelpnr = 0; return NULL; &#125; *retrelpnr = retpnr; return retmsa; &#125; msadsc_t *mm_divpages_core(memarea_t *mareap, uint_t pages, uint_t *retrealpnr, uint_t flgs) &#123; uint_t retpnr = 0; msadsc_t *retmsa = NULL; cpuflg_t cpuflg; //内存区加锁 knl_spinlock_cli(&amp;mareap->ma_lock, &amp;cpuflg); if (DMF_RELDIV == flgs) &#123; //分配内存 retmsa = mm_reldivpages_onmarea(mareap, pages, &amp;retpnr); goto ret_step; &#125; retmsa = NULL; retpnr = 0; ret_step: //内存区锁 knl_spinunlock_sti(&amp;mareap->ma_lock, &amp;cpuflg); *retrealpnr = retpnr; return retmsa; &#125; 上述代码中 onmpgs_retn_bafhlst 函数返回的两个 bafhlst_t 结构指针，若是相等的，则在 mm_reldpgsdivmsa_bafhl 函数中很容易处理，只要取出 bafhlst_t 结构中对应的 msadsc_t 结构返回就好了。 问题是很多时候它们不相等，这就要分隔连续的 msadsc_t 结构了，下面我们通过 mm_reldpgsdivmsa_bafhl 这个函数来处理这个问题，代码如下所示 bool_t mrdmb_add_msa_bafh(bafhlst_t *bafhp, msadsc_t *msastat, msadsc_t *msaend) &#123; //把一段连续的msadsc_t结构加入到它所对应的bafhlst_t结构中 msastat->md_indxflgs.mf_olkty = MF_OLKTY_ODER; msastat->md_odlink = msaend; msaend->md_indxflgs.mf_olkty = MF_OLKTY_BAFH; msaend->md_odlink = bafhp; list_add(&amp;msastat->md_list, &amp;bafhp->af_frelst); bafhp->af_mobjnr++; bafhp->af_fobjnr++; return TRUE; &#125; msadsc_t *mm_divpages_opmsadsc(msadsc_t *msastat, uint_t mnr) &#123; //单个msadsc_t结构的情况 if (mend == msastat) &#123;//增加msadsc_t结构中分配计数，分配标志位设置为1 msastat->md_indxflgs.mf_uindx++; msastat->md_phyadrs.paf_alloc = PAF_ALLOC; msastat->md_indxflgs.mf_olkty = MF_OLKTY_ODER; msastat->md_odlink = mend; return msastat; &#125; msastat->md_indxflgs.mf_uindx++; msastat->md_phyadrs.paf_alloc = PAF_ALLOC; //多个msadsc_t结构的情况下，末端msadsc_t结构也设置已分配状态 mend->md_indxflgs.mf_uindx++; mend->md_phyadrs.paf_alloc = PAF_ALLOC; msastat->md_indxflgs.mf_olkty = MF_OLKTY_ODER; msastat->md_odlink = mend; return msastat; &#125; bool_t mm_retnmsaob_onbafhlst(bafhlst_t *bafhp, msadsc_t **retmstat, msadsc_t **retmend) &#123; //取出一个msadsc_t结构 msadsc_t *tmp = list_entry(bafhp->af_frelst.next, msadsc_t, md_list); //从链表中删除 list_del(&amp;tmp->md_list); //减少bafhlst_t结构中的msadsc_t计数 bafhp->af_mobjnr--; bafhp->af_fobjnr--; //增加分配计数 bafhp->af_freindx++; //返回msadsc_t结构 *retmstat = tmp; //返回当前msadsc_t结构连续的那个结尾的msadsc_t结构 *retmend = (msadsc_t *)tmp->md_odlink; if (MF_OLKTY_BAFH == tmp->md_indxflgs.mf_olkty) &#123;//如果只单个msadsc_t结构，那就是它本身 *retmend = tmp; &#125; return TRUE; &#125; msadsc_t *mm_reldpgsdivmsa_bafhl(memarea_t *malckp, uint_t pages, uint_t *retrelpnr, bafhlst_t *relbfl, bafhlst_t *divbfl) &#123; msadsc_t *retmsa = NULL; bool_t rets = FALSE; msadsc_t *retmstat = NULL, *retmend = NULL; //处理相等的情况 if (relbfl == divbfl) &#123; //从bafhlst_t结构中获取msadsc_t结构的开始与结束地址 rets = mm_retnmsaob_onbafhlst(relbfl, &amp;retmstat, &amp;retmend); //设置msadsc_t结构的相关信息表示已经删除 retmsa = mm_divpages_opmsadsc(retmstat, relbfl->af_oderpnr); //返回实际的分配页数 *retrelpnr = relbfl->af_oderpnr; return retmsa; &#125; //处理不等的情况 //从bafhlst_t结构中获取msadsc_t结构的开始与结束地址 rets = mm_retnmsaob_onbafhlst(divbfl, &amp;retmstat, &amp;retmend); uint_t divnr = divbfl->af_oderpnr; //从高bafhlst_t数组元素中向下遍历 for (bafhlst_t *tmpbfl = divbfl - 1; tmpbfl >= relbfl; tmpbfl--) &#123; //开始分割连续的msadsc_t结构，把剩下的一段连续的msadsc_t结构加入到对应该bafhlst_t结构中 if (mrdmb_add_msa_bafh(tmpbfl, &amp;retmstat[tmpbfl->af_oderpnr], (msadsc_t *)retmstat->md_odlink) == FALSE) &#123; system_error(\"mrdmb_add_msa_bafh fail\\n\"); &#125; retmstat->md_odlink = &amp;retmstat[tmpbfl->af_oderpnr - 1]; divnr -= tmpbfl->af_oderpnr; &#125; retmsa = mm_divpages_opmsadsc(retmstat, divnr); if (NULL == retmsa) &#123; *retrelpnr = 0; return NULL; &#125; *retrelpnr = relbfl->af_oderpnr; return retmsa; &#125; 这个算法将执行如下步骤： 根据一个页面的请求，会返回 m_mdmlielst 数组中的第 0 个 bafhlst_t 结构。 如果第 0 个 bafhlst_t 结构中有 msadsc_t 结构就直接返回，若没有 msadsc_t 结构，就会继续查找 m_mdmlielst 数组中的第 1 个 bafhlst_t 结构。 如果第 1 个 bafhlst_t 结构中也没有 msadsc_t 结构，就会继续查找 m_mdmlielst 数组中的第 2 个 bafhlst_t 结构。 如果第 2 个 bafhlst_t 结构中有 msadsc_t 结构，记住第 2 个 bafhlst_t 结构中对应是 4 个连续的 msadsc_t 结构。这时让这 4 个连续的 msadsc_t 结构从第 2 个 bafhlst_t 结构中脱离。 把这 4 个连续的 msadsc_t 结构，对半分割成 2 个双 msadsc_t 结构，把其中一个双 msadsc_t 结构挂载到第 1 个 bafhlst_t 结构中。 把剩下一个双 msadsc_t 结构，继续对半分割成两个单 msadsc_t 结构，把其中一个单 msadsc_t 结构挂载到第 0 个 bafhlst_t 结构中，剩下一个单 msadsc_t 结构返回给请求者，完成内存分配。 内存页的释放 //释放内存页面核心 bool_t mm_merpages_core(memarea_t *marea, msadsc_t *freemsa, uint_t freepgs) &#123; bool_t rets = FALSE; cpuflg_t cpuflg; //内存区加锁 knl_spinlock_cli(&amp;marea->ma_lock, &amp;cpuflg); //针对一个内存区进行操作 rets = mm_merpages_onmarea(marea, freemsa, freepgs); //内存区解锁 knl_spinunlock_sti(&amp;marea->ma_lock, &amp;cpuflg); return rets; &#125; //释放内存页面框架函数 bool_t mm_merpages_fmwk(memmgrob_t *mmobjp, msadsc_t *freemsa, uint_t freepgs) &#123; //获取要释放msadsc_t结构所在的内存区 memarea_t *marea = onfrmsa_retn_marea(mmobjp, freemsa, freepgs); if (NULL == marea) &#123; return FALSE; &#125; //释放内存页面的核心函数 bool_t rets = mm_merpages_core(marea, freemsa, freepgs); if (FALSE == rets) &#123; return FALSE; &#125; return rets; &#125; //释放内存页面接口 //mmobjp->内存管理数据结构指针 //freemsa->释放内存页面对应的首个msadsc_t结构指针 //freepgs->请求释放的内存页面数 bool_t mm_merge_pages(memmgrob_t *mmobjp, msadsc_t *freemsa, uint_t freepgs) &#123; if (NULL == mmobjp || NULL == freemsa || 1 > freepgs) &#123; return FALSE; &#125; //调用释放内存页面的框架函数 bool_t rets = mm_merpages_fmwk(mmobjp, freemsa, freepgs); if (FALSE == rets) &#123; return FALSE; &#125; return rets; &#125; 我们的内存释放页面的代码的结构依然是：接口函数调用框架函数，框架函数调用核心函数，函数的返回值都是 bool 类型，即 TRUE 或者 FALSE，来表示内存页面释放操作成功与否。 我们从框架函数中可以发现，内存区是由 msadsc_t 结构中获取的，因为之前该结构中保留了所在内存区的类型，所以可以查到并返回内存区。 在释放内存页面的核心 mm_merpages_core 函数中，会调用 mm_merpages_onmarea 函数，下面我们来实现这个函数，代码如下。 sint_t mm_merpages_opmsadsc(bafhlst_t *bafh, msadsc_t *freemsa, uint_t freepgs) &#123; msadsc_t *fmend = (msadsc_t *)freemsa->md_odlink; //处理只有一个单页的情况 if (freemsa == fmend) &#123; //页面的分配计数减1 freemsa->md_indxflgs.mf_uindx--; if (0 &lt; freemsa->md_indxflgs.mf_uindx) &#123;//如果依然大于0说明它是共享页面 直接返回1指示不需要进行下一步操作 return 1; &#125; //设置页未分配的标志 freemsa->md_phyadrs.paf_alloc = PAF_NO_ALLOC; freemsa->md_indxflgs.mf_olkty = MF_OLKTY_BAFH; freemsa->md_odlink = bafh;//指向所属的bafhlst_t结构 //返回2指示需要进行下一步操作 return 2; &#125; //多个页面的超始页面和结束页面都要减一 freemsa->md_indxflgs.mf_uindx--; fmend->md_indxflgs.mf_uindx--; //如果依然大于0说明它是共享页面 直接返回1指示不需要进行下一步操作 if (0 &lt; freemsa->md_indxflgs.mf_uindx) &#123; return 1; &#125; //设置起始、结束页页未分配的标志 freemsa->md_phyadrs.paf_alloc = PAF_NO_ALLOC; fmend->md_phyadrs.paf_alloc = PAF_NO_ALLOC; freemsa->md_indxflgs.mf_olkty = MF_OLKTY_ODER; //起始页面指向结束页面 freemsa->md_odlink = fmend; fmend->md_indxflgs.mf_olkty = MF_OLKTY_BAFH; //结束页面指向所属的bafhlst_t结构 fmend->md_odlink = bafh; //返回2指示需要进行下一步操作 return 2; &#125; bool_t onfpgs_retn_bafhlst(memarea_t *malckp, uint_t freepgs, bafhlst_t **retrelbf, bafhlst_t **retmerbf) &#123; //获取bafhlst_t结构数组的开始地址 bafhlst_t *bafhstat = malckp->ma_mdmdata.dm_mdmlielst; //根据分配页面数计算出分配页面在dm_mdmlielst数组中下标 sint_t dividx = retn_divoder(freepgs); //返回请求释放的bafhlst_t结构指针 *retrelbf = &amp;bafhstat[dividx]; //返回最大释放的bafhlst_t结构指针 *retmerbf = &amp;bafhstat[MDIVMER_ARR_LMAX - 1]; return TRUE; &#125; bool_t mm_merpages_onmarea(memarea_t *malckp, msadsc_t *freemsa, uint_t freepgs) &#123; bafhlst_t *prcbf = NULL; sint_t pocs = 0; bafhlst_t *retrelbf = NULL, *retmerbf = NULL; bool_t rets = FALSE; //根据freepgs返回请求释放的和最大释放的bafhlst_t结构指针 rets = onfpgs_retn_bafhlst(malckp, freepgs, &amp;retrelbf, &amp;retmerbf); //设置msadsc_t结构的信息，完成释放，返回1表示不需要下一步合并操作，返回2表示要进行合并操作 sint_t mopms = mm_merpages_opmsadsc(retrelbf, freemsa, freepgs); if (2 == mopms) &#123; //把msadsc_t结构进行合并然后加入对应bafhlst_t结构 return mm_merpages_onbafhlst(freemsa, freepgs, retrelbf, retmerbf); &#125; if (1 == mopms) &#123; return TRUE; &#125; return FALSE; &#125; 在经过 mm_merpages_opmsadsc 函数操作之后，我们并没有将 msadsc_t 结构加入到对应的 bafhlst_t 结构中，这其实是在下一个函数完成的，那就是 mm_merpages_onbafhlst 这个函数。下面我们来实现它，代码如下所示。 bool_t mpobf_add_msadsc(bafhlst_t *bafhp, msadsc_t *freemstat, msadsc_t *freemend) &#123; freemstat->md_indxflgs.mf_olkty = MF_OLKTY_ODER; //设置起始页面指向结束页 freemstat->md_odlink = freemend; freemend->md_indxflgs.mf_olkty = MF_OLKTY_BAFH; //结束页面指向所属的bafhlst_t结构 freemend->md_odlink = bafhp; //把起始页面挂载到所属的bafhlst_t结构中 list_add(&amp;freemstat->md_list, &amp;bafhp->af_frelst); //增加bafhlst_t结构的空闲页面对象和总的页面对象的计数 bafhp->af_fobjnr++; bafhp->af_mobjnr++; return TRUE; &#125; bool_t mm_merpages_onbafhlst(msadsc_t *freemsa, uint_t freepgs, bafhlst_t *relbf, bafhlst_t *merbf) &#123; sint_t rets = 0; msadsc_t *mnxs = freemsa, *mnxe = &amp;freemsa[freepgs - 1]; bafhlst_t *tmpbf = relbf; //从实际要开始遍历，直到最高的那个bafhlst_t结构 for (; tmpbf &lt; merbf; tmpbf++) &#123; //查看最大地址连续、且空闲msadsc_t结构，如释放的是第0个msadsc_t结构我们就去查找第1个msadsc_t结构是否空闲，且与第0个msadsc_t结构的地址是不是连续的 rets = mm_find_cmsa2blk(tmpbf, &amp;mnxs, &amp;mnxe); if (1 == rets) &#123; break; &#125; &#125; //把合并的msadsc_t结构（从mnxs到mnxe）加入到对应的bafhlst_t结构中 if (mpobf_add_msadsc(tmpbf, mnxs, mnxe) == FALSE) &#123; return FALSE; &#125; return TRUE; &#125; 最核心的还是要对空闲页面进行合并，合并成更大的连续的内存页面 比如，现在我们要释放一个页面，这个算法将执行如下步骤。 释放一个页面，会返回 m_mdmlielst 数组中的第 0 个 bafhlst_t 结构。设置这个页面对应的 msadsc_t 结构的相关信息，表示已经执行了释放操作。 开始查看第 0 个 bafhlst_t 结构中有没有空闲的 msadsc_t，并且它和要释放的 msadsc_t 对应的物理地址是连续的。没有则把这个释放的 msadsc_t 挂载第 0 个 bafhlst_t 结构中，算法结束，否则进入下一步。 把第 0 个 bafhlst_t 结构中的 msadsc_t 结构拿出来与释放的 msadsc_t 结构，合并成 2 个连续且更大的 msadsc_t。继续查看第 1 个 bafhlst_t 结构中有没有空闲的 msadsc_t，而且这个空闲 msadsc_t 要和上一步合并的 2 个 msadsc_t 对应的物理地址是连续的。 没有则把这个合并的 2 个 msadsc_t 挂载第 1 个 bafhlst_t 结构中，算法结束，否则进入下一步。 把第 1 个 bafhlst_t 结构中的 2 个连续的 msadsc_t 结构，还有合并的 2 个地址连续的 msadsc_t 结构拿出来，合并成 4 个连续且更大的 msadsc_t 结构。 继续查看第 2 个 bafhlst_t 结构，有没有空闲的 msadsc_t 结构，并且它要和上一步合并的 4 个 msadsc_t 结构对应的物理地址是连续的。 没有则把这个合并的 4 个 msadsc_t 挂载第 2 个 bafhlst_t 结构中，算法结束。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.innnovation.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"OS","slug":"OS","permalink":"https://blog.innnovation.cn/tags/OS/"}]},{"title":"webFlux及相关知识点理解和学习","slug":"WebFlux学习及原理性理解","date":"2021-07-09T16:00:00.000Z","updated":"2021-07-16T05:56:15.161Z","comments":true,"path":"2021/07/10/webflux-xue-xi-ji-yuan-li-xing-li-jie/","link":"","permalink":"https://blog.innnovation.cn/2021/07/10/webflux-xue-xi-ji-yuan-li-xing-li-jie/","excerpt":"","text":"本人英语渣渣，但是学习较新的知识还是选择外面的世界多看看，某度的引擎真的是毒瘤，本文概念和词汇可能包含机翻部分，欢迎大神们批评指针 Quote:https://docs.spring.io/spring-framework/docs/current/reference/html/web-reactive.html#webflux-new-framework 我们先看看Spring为什么要创造WebFlux: Part of the answer is the need for a non-blocking web stack to handle concurrency with a small number of threads and scale with fewer hardware resources. Servlet 3.1 did provide an API for non-blocking I/O. However, using it leads away from the rest of the Servlet API, where contracts are synchronous (Filter, Servlet) or blocking (getParameter, getPart). This was the motivation for a new common API to serve as a foundation across any non-blocking runtime. That is important because of servers (such as Netty) that are well-established in the async, non-blocking space. The other part of the answer is functional programming. Much as the addition of annotations in Java 5 created opportunities (such as annotated REST controllers or unit tests), the addition of lambda expressions in Java 8 created opportunities for functional APIs in Java. This is a boon for non-blocking applications and continuation-style APIs (as popularized by CompletableFuture and ReactiveX) that allow declarative composition of asynchronous logic. At the programming-model level, Java 8 enabled Spring WebFlux to offer functional web endpoints alongside annotated controllers. 总结 需要一个非阻塞的Web技术栈来处理具有少量线程的并发并使用较少的硬件资源进行扩展，Servlet3.1之后提供了NIO非阻塞式API以及Netty都提供了支持 Java的函数式编程（functional programming）支持，lambda表达式提供了函数式API，允许异步逻辑的声明式组合的非阻塞应用程序 那么所以学习WebFlux之前必须了解两个东西lambda、Stream lambda和流API都是Java8才有的（21年3.16JDK16发布），这里就不对lambda和Stream的使用做介绍了，我们主要研究一下实现原理 lambda实现原理初看lambda表达式，感觉就是一个匿名函数（类比匿名内部类），所以我推测当时出这个功能就是Java程序员发现Java只能传递对象，没办法传递函数，大致的使用场景可能是这样的 static void executeFunc(函数引用,String word) &#123; // todo 用传入的方法打印word变量 &#125; 这段代码要是换成C来，只要往这个方法传入一个函数指针即可（函数入口地址） void printWord(char* str);//声明函数 ... void (*fun_ptr)(char*);//定义函数指针 fun_ptr=printWord;//把printWord函数地址赋值给fun_ptr (*fun_ptr)(\"Hello World!\");//直接调用 another_func(fun_ptr,\"Hello World\");//间接调用，回调函数就是用这种方式实现的 那么Java怎么办解决这个问题呢，前面提到了匿名内部类，lambda最初开发的时候可能真的是从这里启发来的，用一个接口(匿名类的父类接口)包裹住这个方法，传接口对象进去就可以了 为了研究这个问题 上代码，用反编译工具查看底层字节码研究原理 package main; interface Wrapper &#123; void myPrint(String w); &#125; class Solution &#123; static void executeFunc(Wrapper w, String word) &#123; w.myPrint(word); &#125; public static void main(String[] args) &#123; // 匿名内部类写法 executeFunc(new Wrapper() &#123; @Override public void myPrint(String w) &#123; // 个性化拓展，例如在打印之前记录时间什么的 System.out.println(w); &#125; &#125;, \"Hello Lambda!\"); // lambda写法 executeFunc(w -> &#123; // 个性化拓展，例如在打印之前记录时间什么的 System.out.println(w); &#125;, \"Hello Lambda!\"); &#125; &#125; 我们看lambda这部分的字节码 // class version 55.0 (55) // access flags 0x20 class main/Solution &#123; // compiled from: Solution.java // access flags 0x19 public final static INNERCLASS java/lang/invoke/MethodHandles$Lookup java/lang/invoke/MethodHandles Lookup // access flags 0x0 &lt;init>()V L0 LINENUMBER 8 L0 ALOAD 0 INVOKESPECIAL java/lang/Object.&lt;init> ()V RETURN L1 LOCALVARIABLE this Lmain/Solution; L0 L1 0 MAXSTACK = 1 MAXLOCALS = 1 // access flags 0x8 static executeFunc(Lmain/Wrapper;Ljava/lang/String;)V L0 LINENUMBER 11 L0 ALOAD 0 ALOAD 1 INVOKEINTERFACE main/Wrapper.myPrint (Ljava/lang/String;)V (itf) L1 LINENUMBER 12 L1 RETURN L2 LOCALVARIABLE w Lmain/Wrapper; L0 L2 0 LOCALVARIABLE word Ljava/lang/String; L0 L2 1 MAXSTACK = 2 MAXLOCALS = 2 // access flags 0x9 public static main([Ljava/lang/String;)V L0 LINENUMBER 26 L0 INVOKEDYNAMIC myPrint()Lmain/Wrapper; [ // handle kind 0x6 : INVOKESTATIC java/lang/invoke/LambdaMetafactory.metafactory(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodHandle;Ljava/lang/invoke/MethodType;)Ljava/lang/invoke/CallSite; // arguments: (Ljava/lang/String;)V, // handle kind 0x6 : INVOKESTATIC main/Solution.lambda$main$0(Ljava/lang/String;)V, (Ljava/lang/String;)V ] LDC \"Hello Lambda!\" INVOKESTATIC main/Solution.executeFunc (Lmain/Wrapper;Ljava/lang/String;)V L1 LINENUMBER 31 L1 RETURN L2 LOCALVARIABLE args [Ljava/lang/String; L0 L2 0 MAXSTACK = 2 MAXLOCALS = 1 // access flags 0x100A private static synthetic lambda$main$0(Ljava/lang/String;)V L0 LINENUMBER 28 L0 GETSTATIC java/lang/System.out : Ljava/io/PrintStream; ALOAD 0 INVOKEVIRTUAL java/io/PrintStream.println (Ljava/lang/String;)V L1 LINENUMBER 29 L1 RETURN L2 LOCALVARIABLE w Ljava/lang/String; L0 L2 0 MAXSTACK = 2 MAXLOCALS = 1 &#125; 注意一行代码 private static synthetic lambda$main$0(Ljava/lang/String;)V//line 61 synthetic这个词的意思是“人造的; (人工)合成的; 综合(型)的”，作为关键字，它表示该方法由编译器自动生成。简言之，把它当做void，这是一个无返回值的静态方法，叫lambda$main$0，接受一个String类型的参数。里面又有一系列操作 编译器把我们写的lambda表达式转换成了一个静态的私有函数，通过调用这个函数来解决传递一段代码的问题 那么还剩下一个问题，JVM是通过什么知道如何执行该私有函数呢，这就得翻阅Oracle官方文档的The invokedynamic Instruction章节 INVOKEDYNAMIC指令，顾名思义，就是动态激活。那这个动态是怎么理解的？这里的”动态”意思是，在运行时才确定，这个调用lambda函数的下一步指向哪里。在初始状态（程序编译成字节码后但未运行时），这个标记是空的，不会执行任何动作。当程序开始执行时，具体一点就是发生字节码的43行，初始化就开始了。在运行时确定，无外乎反射，仔细看43行的一大串包，也确实是通过反射实现的 Stream实现原理先上一张copy的速查表格 Stream操作分类 中间操作(Intermediate operations) 无状态(Stateless) unordered() filter() map() mapToInt() mapToLong() mapToDouble() flatMap() flatMapToInt() flatMapToLong() flatMapToDouble() peek() 有状态(Stateful) distinct() sorted() sorted() limit() skip() 结束操作(Terminal operations) 非短路操作 forEach() forEachOrdered() toArray() reduce() collect() max() min() count() 短路操作(short-circuiting) anyMatch() allMatch() noneMatch() findFirst() findAny() 原理解释引用自:https://github.com/CarpenterLee/JavaLambdaInternals/blob/master/6-Stream%20Pipelines.md &gt;&gt; 操作如何记录 注意这里使用的是“操作(operation)”一词，指的是“Stream中间操作”的操作，很多Stream操作会需要一个回调函数（Lambda表达式），因此一个完整的操作是&lt;*数据来源，操作，回调函数*&gt;构成的三元组。Stream中使用Stage的概念来描述一个完整的操作，并用某种实例化后的PipelineHelper来代表Stage，将具有先后顺序的各个Stage连到一起，就构成了整个流水线。跟Stream相关类和接口的继承关系图示。 还有IntPipeline, LongPipeline, DoublePipeline没在图中画出，这三个类专门为三种基本类型（不是包装类型）而定制的，跟ReferencePipeline是并列关系。图中Head用于表示第一个Stage，即调用调用诸如Collection.stream()*方法产生的Stage，很显然这个Stage里不包含任何操作；*StatelessOp*和*StatefulOp分别表示无状态和有状态的Stage，对应于无状态和有状态的中间操作。 Stream流水线组织结构示意图如下： 图中通过Collection.stream()方法得到Head也就是stage0，紧接着调用一系列的中间操作，不断产生新的Stream。这些Stream对象以双向链表的形式组织在一起，构成整个流水线，由于每个Stage都记录了前一个Stage和本次的操作以及回调函数，依靠这种结构就能建立起对数据源的所有操作。这就是Stream记录操作的方式。 &gt;&gt; 操作如何叠加以上只是解决了操作记录的问题，要想让流水线起到应有的作用我们需要一种将所有操作叠加到一起的方案。你可能会觉得这很简单，只需要从流水线的head开始依次执行每一步的操作（包括回调函数）就行了。这听起来似乎是可行的，但是你忽略了前面的Stage并不知道后面Stage到底执行了哪种操作，以及回调函数是哪种形式。换句话说，只有当前Stage本身才知道该如何执行自己包含的动作。这就需要有某种协议来协调相邻Stage之间的调用关系。 这种协议由Sink接口完成，Sink接口包含的方法如下表所示： 方法名 作用 void begin(long size) 开始遍历元素之前调用该方法，通知Sink做好准备。 void end() 所有元素遍历完成之后调用，通知Sink没有更多的元素了。 boolean cancellationRequested() 是否可以结束操作，可以让短路操作尽早结束。 void accept(T t) 遍历元素时调用，接受一个待处理元素，并对元素进行处理。Stage把自己包含的操作和回调方法封装到该方法里，前一个Stage只需要调用当前Stage.accept(T t)方法就行了。 有了上面的协议，相邻Stage之间调用就很方便了，每个Stage都会将自己的操作封装到一个Sink里，前一个Stage只需调用后一个Stage的accept()方法即可，并不需要知道其内部是如何处理的。当然对于有状态的操作，Sink的begin()和end()方法也是必须实现的。比如Stream.sorted()是一个有状态的中间操作，其对应的Sink.begin()方法可能创建一个盛放结果的容器，而accept()方法负责将元素添加到该容器，最后end()负责对容器进行排序。对于短路操作，Sink.cancellationRequested()也是必须实现的，比如Stream.findFirst()是短路操作，只要找到一个元素，cancellationRequested()就应该返回true，以便调用者尽快结束查找。Sink的四个接口方法常常相互协作，共同完成计算任务。实际上Stream API内部实现的的本质，就是如何重写Sink的这四个接口方法。 有了Sink对操作的包装，Stage之间的调用问题就解决了，执行时只需要从流水线的head开始对数据源依次调用每个Stage对应的Sink.{begin(), accept(), cancellationRequested(), end()}方法就可以了。一种可能的Sink.accept()方法流程是这样的： void accept(U u)&#123; 1. 使用当前Sink包装的回调函数处理u 2. 将处理结果传递给流水线下游的Sink &#125; Sink接口的其他几个方法也是按照这种[处理-&gt;转发]的模型实现。下面我们结合具体例子看看Stream的中间操作是如何将自身的操作包装成Sink以及Sink是如何将处理结果转发给下一个Sink的。先看Stream.map()方法： // Stream.map()，调用该方法将产生一个新的Stream public final &lt;R> Stream&lt;R> map(Function&lt;? super P_OUT, ? extends R> mapper) &#123; ... return new StatelessOp&lt;P_OUT, R>(this, StreamShape.REFERENCE, StreamOpFlag.NOT_SORTED | StreamOpFlag.NOT_DISTINCT) &#123; @Override /*opWripSink()方法返回由回调函数包装而成Sink*/ Sink&lt;P_OUT> opWrapSink(int flags, Sink&lt;R> downstream) &#123; return new Sink.ChainedReference&lt;P_OUT, R>(downstream) &#123; @Override public void accept(P_OUT u) &#123; R r = mapper.apply(u);// 1. 使用当前Sink包装的回调函数mapper处理u downstream.accept(r);// 2. 将处理结果传递给流水线下游的Sink &#125; &#125;; &#125; &#125;; &#125; 上述代码看似复杂，其实逻辑很简单，就是将回调函数mapper包装到一个Sink当中。由于Stream.map()是一个无状态的中间操作，所以map()方法返回了一个StatelessOp内部类对象（一个新的Stream），调用这个新Stream的opWripSink()方法将得到一个包装了当前回调函数的Sink。 再来看一个复杂一点的例子。Stream.sorted()方法将对Stream中的元素进行排序，显然这是一个有状态的中间操作，因为读取所有元素之前是没法得到最终顺序的。抛开模板代码直接进入问题本质，sorted()方法是如何将操作封装成Sink的呢？sorted()一种可能封装的Sink代码如下： // Stream.sort()方法用到的Sink实现 class RefSortingSink&lt;T> extends AbstractRefSortingSink&lt;T> &#123; private ArrayList&lt;T> list;// 存放用于排序的元素 RefSortingSink(Sink&lt;? super T> downstream, Comparator&lt;? super T> comparator) &#123; super(downstream, comparator); &#125; @Override public void begin(long size) &#123; ... // 创建一个存放排序元素的列表 list = (size >= 0) ? new ArrayList&lt;T>((int) size) : new ArrayList&lt;T>(); &#125; @Override public void end() &#123; list.sort(comparator);// 只有元素全部接收之后才能开始排序 downstream.begin(list.size()); if (!cancellationWasRequested) &#123;// 下游Sink不包含短路操作 list.forEach(downstream::accept);// 2. 将处理结果传递给流水线下游的Sink &#125; else &#123;// 下游Sink包含短路操作 for (T t : list) &#123;// 每次都调用cancellationRequested()询问是否可以结束处理。 if (downstream.cancellationRequested()) break; downstream.accept(t);// 2. 将处理结果传递给流水线下游的Sink &#125; &#125; downstream.end(); list = null; &#125; @Override public void accept(T t) &#123; list.add(t);// 1. 使用当前Sink包装动作处理t，只是简单的将元素添加到中间列表当中 &#125; &#125; 上述代码完美的展现了Sink的四个接口方法是如何协同工作的： 首先begin()方法告诉Sink参与排序的元素个数，方便确定中间结果容器的的大小； 之后通过accept()方法将元素添加到中间结果当中，最终执行时调用者会不断调用该方法，直到遍历所有元素； 最后end()方法告诉Sink所有元素遍历完毕，启动排序步骤，排序完成后将结果传递给下游的Sink； 如果下游的Sink是短路操作，将结果传递给下游时不断询问下游cancellationRequested()是否可以结束处理。 &gt;&gt; 叠加之后的操作如何执行 Sink完美封装了Stream每一步操作，并给出了[处理-&gt;转发]的模式来叠加操作。这一连串的齿轮已经咬合，就差最后一步拨动齿轮启动执行。是什么启动这一连串的操作呢？也许你已经想到了启动的原始动力就是结束操作(Terminal Operation)，一旦调用某个结束操作，就会触发整个流水线的执行。 结束操作之后不能再有别的操作，所以结束操作不会创建新的流水线阶段(Stage)，直观的说就是流水线的链表不会在往后延伸了。结束操作会创建一个包装了自己操作的Sink，这也是流水线中最后一个Sink，这个Sink只需要处理数据而不需要将结果传递给下游的Sink（因为没有下游）。对于Sink的[处理-&gt;转发]模型，结束操作的Sink就是调用链的出口。 我们再来考察一下上游的Sink是如何找到下游Sink的。一种可选的方案是在PipelineHelper中设置一个Sink字段，在流水线中找到下游Stage并访问Sink字段即可。但Stream类库的设计者没有这么做，而是设置了一个Sink AbstractPipeline.opWrapSink(int flags, Sink downstream)方法来得到Sink，该方法的作用是返回一个新的包含了当前Stage代表的操作以及能够将结果传递给downstream的Sink对象。为什么要产生一个新对象而不是返回一个Sink字段？这是因为使用opWrapSink()可以将当前操作与下游Sink（上文中的downstream参数）结合成新Sink。试想只要从流水线的最后一个Stage开始，不断调用上一个Stage的opWrapSink()方法直到最开始（不包括stage0，因为stage0代表数据源，不包含操作），就可以得到一个代表了流水线上所有操作的Sink，用代码表示就是这样： // AbstractPipeline.wrapSink() // 从下游向上游不断包装Sink。如果最初传入的sink代表结束操作， // 函数返回时就可以得到一个代表了流水线上所有操作的Sink。 final &lt;P_IN> Sink&lt;P_IN> wrapSink(Sink&lt;E_OUT> sink) &#123; ... for (AbstractPipeline p=AbstractPipeline.this; p.depth > 0; p=p.previousStage) &#123; sink = p.opWrapSink(p.previousStage.combinedFlags, sink); &#125; return (Sink&lt;P_IN>) sink; &#125; 现在流水线上从开始到结束的所有的操作都被包装到了一个Sink里，执行这个Sink就相当于执行整个流水线，执行Sink的代码如下： // AbstractPipeline.copyInto(), 对spliterator代表的数据执行wrappedSink代表的操作。 final &lt;P_IN> void copyInto(Sink&lt;P_IN> wrappedSink, Spliterator&lt;P_IN> spliterator) &#123; ... if (!StreamOpFlag.SHORT_CIRCUIT.isKnown(getStreamAndOpFlags())) &#123; wrappedSink.begin(spliterator.getExactSizeIfKnown());// 通知开始遍历 spliterator.forEachRemaining(wrappedSink);// 迭代 wrappedSink.end();// 通知遍历结束 &#125; ... &#125; 上述代码首先调用wrappedSink.begin()方法告诉Sink数据即将到来，然后调用spliterator.forEachRemaining()方法对数据进行迭代（Spliterator是容器的一种迭代器，[参阅](https://github.com/CarpenterLee/JavaLambdaInternals/blob/master/3-Lambda and Collections.md#spliterator)），最后调用wrappedSink.end()方法通知Sink数据处理结束。逻辑如此清晰。 &gt;&gt; 执行后的结果在哪里最后一个问题是流水线上所有操作都执行后，用户所需要的结果（如果有）在哪里？首先要说明的是不是所有的Stream结束操作都需要返回结果，有些操作只是为了使用其副作用(Side-effects)，比如使用Stream.forEach()方法将结果打印出来就是常见的使用副作用的场景（事实上，除了打印之外其他场景都应避免使用副作用），对于真正需要返回结果的结束操作结果存在哪里呢？ 特别说明：副作用不应该被滥用，也许你会觉得在Stream.forEach()里进行元素收集是个不错的选择，就像下面代码中那样，但遗憾的是这样使用的正确性和效率都无法保证，因为Stream可能会并行执行。大多数使用副作用的地方都可以使用[归约操作](https://github.com/CarpenterLee/JavaLambdaInternals/blob/master/5-Streams API(II).md)更安全和有效的完成。 // 错误的收集方式 ArrayList&lt;String> results = new ArrayList&lt;>(); stream.filter(s -> pattern.matcher(s).matches()) .forEach(s -> results.add(s)); // Unnecessary use of side-effects! // 正确的收集方式 List&lt;String>results = stream.filter(s -> pattern.matcher(s).matches()) .collect(Collectors.toList()); // No side-effects! 回到流水线执行结果的问题上来，需要返回结果的流水线结果存在哪里呢？这要分不同的情况讨论，下表给出了各种有返回结果的Stream结束操作。 返回类型 对应的结束操作 boolean anyMatch() allMatch() noneMatch() Optional findFirst() findAny() 归约结果 reduce() collect() 数组 toArray() 对于表中返回boolean或者Optional的操作（Optional是存放 一个 值的容器）的操作，由于值返回一个值，只需要在对应的Sink中记录这个值，等到执行结束时返回就可以了。 对于归约操作，最终结果放在用户调用时指定的容器中（容器类型通过[收集器](https://github.com/CarpenterLee/JavaLambdaInternals/blob/master/5-Streams API(II).md#收集器)指定）。collect(), reduce(), max(), min()都是归约操作，虽然max()和min()也是返回一个Optional，但事实上底层是通过调用[reduce()](https://github.com/CarpenterLee/JavaLambdaInternals/blob/master/5-Streams API(II).md#多面手reduce)方法实现的。 对于返回是数组的情况，毫无疑问的结果会放在数组当中。这么说当然是对的，但在最终返回数组之前，结果其实是存储在一种叫做Node的数据结构中的。Node是一种多叉树结构，元素存储在树的叶子当中，并且一个叶子节点可以存放多个元素。这样做是为了并行执行方便。关于Node的具体结构，我们会在下一节探究Stream如何并行执行时给出详细说明。 我们直接举个栗子 List&lt;String> data = new ArrayList&lt;>(); data.add(\"张三\"); data.add(\"李四\"); data.add(\"王三\"); data.add(\"马六\"); data.stream() .filter(x -> x.length() == 2) .map(x -> x.replace(\"三\",\"五\")) .sorted() .filter(x -> x.contains(\"五\")) .forEach(System.out::println); 操作是如何记录下来的? Head记录Stream起始操作 StatelessOp记录中间操作 StatefulOp记录有状态的中间操作这三个操作实例化会指向其父类AbstractPipeline,也就是在AbstractPipeline中建立了双向链表 对于Head AbstractPipeline(Spliterator&lt;?> source, int sourceFlags, boolean parallel) &#123; this.previousStage = null; //首操作上一步为null this.sourceSpliterator = source; //数据 this.sourceStage = this; //Head操作 this.sourceOrOpFlags = sourceFlags &amp; StreamOpFlag.STREAM_MASK; this.combinedFlags = (~(sourceOrOpFlags &lt;&lt; 1)) &amp; StreamOpFlag.INITIAL_OPS_VALUE; this.depth = 0; this.parallel = parallel; &#125; 对于其他Stage: AbstractPipeline(AbstractPipeline&lt;?, E_IN, ?> previousStage, int opFlags) &#123; if (previousStage.linkedOrConsumed) throw new IllegalStateException(MSG_STREAM_LINKED); previousStage.linkedOrConsumed = true; //双向链表的建立 previousStage.nextStage = this; this.previousStage = previousStage; this.sourceStage = previousStage.sourceStage; this.depth = previousStage.depth + 1; this.sourceOrOpFlags = opFlags &amp; StreamOpFlag.OP_MASK; this.combinedFlags = StreamOpFlag.combineOpFlags(opFlags, previousStage.combinedFlags); if (opIsStateful()) sourceStage.sourceAnyStateful = true; &#125; 调用过程如此用双向链表串联起来,每一步都得知其上一步与下一步的操作. data.stream() .filter(x -> x.length() == 2) .map(x -> x.replace(“三”,”五”)) .sorted() .filter(x -> x.contains(“五”)) .forEach(System.out::println); 操作是如何叠加的? Sink&lt;T&gt;接口: void begin(long size),循环开始前调用,通知每个Stage做好准备 void end(),循环结束时调用,依次调用每个Stage的end方法,处理结果 boolean cancellationRequested(),判断是否可以提前结束循环 void accept(T value),每一步的处理 其子类之一ChainedReference: static abstract class ChainedReference&lt;T, E_OUT> implements Sink&lt;T> &#123; protected final Sink&lt;? super E_OUT> downstream; public ChainedReference(Sink&lt;? super E_OUT> downstream) &#123; this.downstream = Objects.requireNonNull(downstream); &#125; @Override public void begin(long size) &#123; downstream.begin(size); &#125; @Override public void end() &#123; downstream.end(); &#125; @Override public boolean cancellationRequested() &#123; return downstream.cancellationRequested(); &#125; &#125; 例Filter: @Override public final Stream&lt;P_OUT> filter(Predicate&lt;? super P_OUT> predicate) &#123; Objects.requireNonNull(predicate); return new StatelessOp&lt;P_OUT, P_OUT>(this, StreamShape.REFERENCE, StreamOpFlag.NOT_SIZED) &#123; @Override Sink&lt;P_OUT> opWrapSink(int flags, Sink&lt;P_OUT> sink) &#123; return new Sink.ChainedReference&lt;P_OUT, P_OUT>(sink) &#123; @Override public void begin(long size) &#123; downstream.begin(-1); &#125; @Override public void accept(P_OUT u) &#123; //条件成立则传递给下一个操作,也因为如此所以有状态的操作必须放到 //end方法里面 if (predicate.test(u)) downstream.accept(u); &#125; &#125;; &#125; &#125;; &#125; 再例如sorted(): @Override public void begin(long size) &#123; if (size >= Nodes.MAX_ARRAY_SIZE) throw new IllegalArgumentException(Nodes.BAD_SIZE); list = (size >= 0) ? new ArrayList&lt;T>((int) size) : new ArrayList&lt;T>(); &#125; @Override public void end() &#123; list.sort(comparator); downstream.begin(list.size()); if (!cancellationWasRequested) &#123; list.forEach(downstream::accept); &#125; else &#123; for (T t : list) &#123; if (downstream.cancellationRequested()) break; downstream.accept(t); &#125; &#125; downstream.end(); list = null; &#125; @Override public void accept(T t) &#123; list.add(t); &#125; 叠加后如何执行? 执行操作是由终端操作来触发的,例如foreach操作 @Override public void forEach(Consumer&lt;? super P_OUT> action) &#123; //evaluate就是开关,一旦调用就立即执行整个Stream evaluate(ForEachOps.makeRef(action, false)); &#125; 执行前会对操作从末尾到起始反向包裹起来,得到调用链 Sink opWrapSink(int flags, Sink&lt;P_OUT> sink) ; //这个Sink是终端操作所对应的Sink final &lt;P_IN> Sink&lt;P_IN> wrapSink(Sink&lt;E_OUT> sink) &#123; Objects.requireNonNull(sink); for ( AbstractPipeline p=AbstractPipeline.this; p.depth > 0; p=p.previousStage) &#123; sink = p.opWrapSink(p.previousStage.combinedFlags, sink); &#125; return (Sink&lt;P_IN>) sink; &#125; @Override final &lt;P_IN> void copyInto(Sink&lt;P_IN> wrappedSink, Spliterator&lt;P_IN> spliterator) &#123; Objects.requireNonNull(wrappedSink); if (!StreamOpFlag.SHORT_CIRCUIT.isKnown(getStreamAndOpFlags())) &#123; //依次执行调用链 wrappedSink.begin(spliterator.getExactSizeIfKnown()); spliterator.forEachRemaining(wrappedSink); wrappedSink.end(); &#125; else &#123; copyIntoWithCancel(wrappedSink, spliterator); &#125; &#125; 有状态的中间操作何时执行? 例如sorted()操作,其依赖上一次操作的结果集,按照调用链来说结果集必须在accept()调用完才会产生.那也就说明sorted操作需要在end中,然后再重新开启调用链. sorted的end方法: @Override public void end() &#123; list.sort(comparator); downstream.begin(list.size()); if (!cancellationWasRequested) &#123; list.forEach(downstream::accept); &#125; else &#123; for (T t : list) &#123; if (downstream.cancellationRequested()) break; downstream.accept(t); &#125; &#125; downstream.end(); list = null; &#125; 那么就相当于sorted给原有操作断路了一次,然后又重新接上,再次遍历. 如何收集到结果? foreach是不需要收集到结果的,但是对于collect这样的操作是需要拿到最终end产生的结果.end产生的结果在最后一个Sink中,这样的操作最终都会提供一个取出数据的get方法. @Override public &lt;P_IN> R evaluateSequential(PipelineHelper&lt;T> helper, Spliterator&lt;P_IN> spliterator) &#123; return helper.wrapAndCopyInto(makeSink(), spliterator).get(); &#125; WebFlux关于WebFlux，阅读官网文档会发现有这些单词 We touched on “non-blocking” and “functional” but what does reactive mean? The term, “reactive,” refers to programming models that are built around reacting to change — network components reacting to I/O events, UI controllers reacting to mouse events, and others. In that sense, non-blocking is reactive, because, instead of being blocked, we are now in the mode of reacting to notifications as operations complete or data becomes available. There is also another important mechanism that we on the Spring team associate with “reactive” and that is non-blocking back pressure. In synchronous, imperative code, blocking calls serve as a natural form of back pressure that forces the caller to wait. In non-blocking code, it becomes important to control the rate of events so that a fast producer does not overwhelm its destination. 其中有几个关键词：响应式、异步非阻塞、背压。这就是WebFlux的概念核心 先上图 WebFlux模型主要依赖响应式编程库Reactor，Reactor 有两种模型，Flux 和 Mono，提供了非阻塞、支持回压机制的异步流处理能力。WebFlux API接收普通Publisher作为输入，在内部使其适配Reactor类型，使用它并返回Flux或Mono作为输出。 我们平时开发过程中只要在Controller中控制每个接口的Publisher入参和Mono返回值(web项目用mono比较多吧)就可以顺利使用WebFlux框架完成业务，并不需要关注黑盒中的东西。 上图中我们主要到从Http客户端到Spring Controller到Service到数据库之间所有的请求全部是Flux，即全部各个节点之间都是异步非阻塞，如果一个系统完全遵守Flux的模型开发，将会大大提高系统的吞吐量，比如Mybatis、JDBC等技术将不推荐使用，因为他们本质都是同步操作，这样还是会使得请求在最后的同步IO管线上堆积。对于底层的数据源来说，MongoDB, Redis, 和 Cassandra 可以直接以reactive的方式支持Spring Data。似乎目前公司的持久层还是用的JPA和mybatis，这样的话也许会使数据库服务器的并发连接成为系统的短板。 我整理了一张反应式组件的图 其中JDK9的响应式流其实本质上完全跟JDK8的同步流不是一个东西，Project Reactor其实就是根据JDK9响应流接口1：1做的设计 Reactor官方javadoc Package Description reactor.adapter Adapt Publisher to Java 9+ Flow.Publisher. reactor.core Core components of the framework supporting extensions to the Reactive Stream programming model. reactor.core.publisher Provide for Flux, Mono composition API and Processor implementations reactor.core.scheduler Scheduler contract and static registry and factory methods in Schedulers. reactor.util Miscellaneous utility classes, such as loggers, tuples or queue suppliers and implementations. reactor.util.annotation reactor.util.concurrent Queue suppliers and utilities Used for operational serialization (serializing threads) or buffering (asynchronous boundary). reactor.util.context Miscellaneous utility classes, such as loggers, tuples or queue suppliers and implementations. reactor.util.function Tuples provide a type-safe way to specify multiple parameters. reactor.util.retry JDK Flow API Modifier and Type Class Description static interface Flow.Processor&lt;T,R&gt; A component that acts as both a Subscriber and Publisher. static interface Flow.Publisher&lt;T&gt; A producer of items (and related control messages) received by Subscribers. static interface Flow.Subscriber&lt;T&gt; A receiver of messages. static interface Flow.Subscription Message control linking a Flow.Publisher and Flow.Subscriber. 观察对比，其中adapter做的就是对jdk9流做的适配，无论是哪个，他们都围绕一个非常关键的设计模式：观察者模式 观察者模式 对注册到Subject(对应Publisher) 的Observer(对应Subcriber)，当发生需要通知的方法时，通知所有集合中的observer对象 Java实现 import java.util.List; import java.util.ArrayList; import java.util.Scanner; class EventSource &#123; public interface Observer &#123; void update(String event); &#125; private final List&lt;Observer> observers = new ArrayList&lt;>(); private void notifyObservers(String event) &#123; observers.forEach(observer -> observer.update(event)); &#125; public void addObserver(Observer observer) &#123; observers.add(observer); &#125; public void scanSystemIn() &#123; Scanner scanner = new Scanner(System.in); while (scanner.hasNextLine()) &#123; String line = scanner.nextLine(); notifyObservers(line); &#125; &#125; &#125; public class ObserverDemo &#123; public static void main(String[] args) &#123; System.out.println(\"Enter Text: \"); EventSource eventSource = new EventSource(); eventSource.addObserver(event -> &#123; System.out.println(\"Received response: \" + event); &#125;); eventSource.scanSystemIn(); &#125; &#125; Project ReactorFlux &amp; MonoFlux&lt;T&gt;是一个标准的Reactive Streams规范中的Publisher&lt;T&gt;，它代表一个包含了[0…N]个元素的异步序列流。在Reactive Streams规范中，针对流中每个元素，订阅者将会监听这三个事件：onNext、onComplete、onError。 Mono&lt;T&gt;是一个特殊的Flux&lt;T&gt;，它代表一个仅包含1个元素的异步序列流。因为只有一个元素，所以订阅者只需要监听onComplete、onError 创建并订阅Flux或Mono创建Flux或Mono的最简单方法，是使用那些工厂方法，如just、fromIterable、empty、range。当需要订阅它们时，可以调用如下几个重载的方法。 subscribe();//仅订阅并触发流，不做其它处理 subscribe(Consumer&lt;? super T> consumer);//处理流中的每个元素 subscribe(Consumer&lt;? super T> consumer, Consumer&lt;? super Throwable> errorConsumer);//处理流中的元素、处理相应的异常 subscribe(Consumer&lt;? super T> consumer, Consumer&lt;? super Throwable> errorConsumer, Runnable completeConsumer);//处理流中的元素、处理相应的异常；当流结束时，可以执行一些内容 subscribe(Consumer&lt;? super T> consumer, Consumer&lt;? super Throwable> errorConsumer, Runnable completeConsumer, Consumer&lt;? super Subscription> subscriptionConsumer);//最后一个参数Subscription，代表处理一个元素的生命周期，也是Reactive Streams规范中定义的 编程的方式创建Fluxgenerate、create、push、handle方法支持以编程的方式创建Flux，使创建方式更加灵活。 generate方法创建的流是同步的，流内元素是有序的，依次被订阅者消费。 create方法以异步、多线程的方式创建流。 push方法以异步、单线程的方式创建流。 handle方法是一个示例方法，它类似于generate，将一个已经存在的流，转换成同步的流。 以下是generate方法的一个简单示例： Flux&lt;String> flux = Flux.generate( () -> 0, (state, sink) -> &#123; sink.next(\"3 x \" + state + \" = \" + 3 * state); if (state == 5) sink.complete(); return state + 1; &#125;); /** 流中的元素依次是： 3 x 0 = 0 3 x 1 = 3 3 x 2 = 6 3 x 3 = 9 3 x 4 = 12 3 x 5 = 15 **/ 使用示例使用静态工厂方法fromIterable创建一个Flux对象，而flatMap、filter等非静态方法即所谓操作符，多种操作符组合使用，可以对数据流的元素进行复杂处理。 List&lt;String> words = Arrays.asList(\"th\", \"qu\"); Flux&lt;String> manyLetters = Flux .fromIterable(words) .flatMap(word -> &#123; System.out.println(\"Step1=\" + word); return Flux.fromArray(word.split(\"\"));&#125;) .filter(s -> &#123; System.out.println(\"Step2=\" + s); return true; &#125;).filter(s -> &#123; System.out.println(\"Step3=\" + s); return true; &#125;); manyLetters.subscribe(s -> System.out.println(\"Result=\" + s + \"\\n\")); /** 输出结果： Step1=th Step2=t Step3=t Result=t Step2=h Step3=h Result=h Step1=qu Step2=q Step3=q Result=q Step2=u Step3=u Result=u **/ 观察manyLetters变量的结构，Flux之所以支持对流中数据的链式调用，是因为每一步返回的Flux对象都被上一个Flux对象包含。 manyLetters 组合的操作符(Operator)对流中数据进行处理，实际上是对Publisher发布消息前的功能增强，使元素可以在发布之前被加工处理好。 如果仅看上面这种使用方式，看起来与Java 8的Stream差不多，并没有体现异步的特性，数据流在一开始就是确定的。假如不存在异步处理，使用Reactor就没有什么意义了。 与Java 8的Stream不同，Reactor支持以异步的方式创建Flux，看如下代码片段，MongoDB与Reactor结合使用： private MongoCollection&lt;Document> collection; public Flux&lt;Restaurant> findAll() &#123; return Flux.from(collection.find()) .map(RestaurantTransfer::toDomainObject) .filter(Optional::isPresent) .map(Optional::get); &#125; public static void main(String[] args) &#123; ReactiveRestaurantRepository repository = new ReactiveRestaurantRepository(); Flux&lt;Restaurant> flux = repository.findAll(); flux.subscribe(System.out::println); &#125; public static &lt;T&gt; Flux&lt;T&gt; from(Publisher&lt;? extends T&gt; source)方法接收一个org.reactivestreams.Publisher参数，该对象是由MongoDB的Reactive客户端API创建的，MongoDB通过该对象，将DB中的数据，链接到Reactor的流中。 当调用subscribe方法后，一个发布-订阅的机制形成，只有当对象被从DB中取出并放入内存后，JVM才会占用线程资源，将消息发送给订阅者；从阻塞等待转变为了被动接收，因此节省了资源。 由此可见，只有流中的数据全部是反应式的，Reactor才能发挥最大作用，一旦有节点被阻塞，就达不到节省资源的目的了。 正是由于MongoDB和Reactor Core都实现了Reactive Streams规范，它们才能相互沟通交互，Reactive Streams规范在反应式编程的推广过程中，起着至关重要的作用。 核心模型publisherpackage org.reactivestreams; public interface Publisher&lt;T> &#123; public void subscribe(Subscriber&lt;? super T> s); &#125; ~ 相当于观察者模式的观察者Observer。Mono、flux就是实现的Publisher。 这里有规范一些publisher定义。github|reactive streams common Subscriberpackage org.reactivestreams; public interface Subscriber&lt;T> &#123; public void onSubscribe(Subscription s); public void onNext(T t); public void onError(Throwable t); public void onComplete(); &#125; 相当于观察者模式的Observable。 Subscription(控速、背压)package org.reactivestreams; public interface Subscription &#123; public void request(long n); public void cancel(); &#125; Subscription是在一般观察者模式中没有的东西。 java文档有如下解释： It is used to both signal desire for data and cancel demand (and allow resource cleanup). 通过分析源码，和reactive streams的java doc可知Subscription的作用，Subscription能通过request方法控制执行流速，也能通过cancel来取消执行。这就是前面提到背压的原理实现 调用关系上面这些基本模型调用关系如下： 运行流程我以一个简洁的示例执行代码为例作分析(字符串拼接打印)： Mono.just(\"hetl\") //@1 .map(s -> s + \"@akulaku.com\") //@2 .filter(s -> s.contains(\"h\")) //@3 .subscribe(System.out::println); //@4 声明阶段在操作声明阶段，看步骤@1、@2、@3的源代码，这些方法就是每次将publisher和operator组合成一个新的publisher。 public static &lt;T> Mono&lt;T> just(T data) &#123; return onAssembly(new MonoJust&lt;>(data)); &#125; public final &lt;R> Mono&lt;R> map(Function&lt;? super T, ? extends R> mapper) &#123; if (this instanceof Fuseable) &#123; return onAssembly(new MonoMapFuseable&lt;>(this, mapper)); &#125; return onAssembly(new MonoMap&lt;>(this, mapper)); &#125; subscribe阶段 By the act of subscribing, you tie the Publisher to a Subscriber, which triggers the flow of data in the whole chain. This is achieved internally by a single request signal from the Subscriber that is propagated upstream, all the way back to the source Publisher. — project reactor reference 3.3.4 subscribe最终会调用最后一个publisher的subsribe方法，然后，逐步包装subscriber和具体操作(operation)， @Override @SuppressWarnings(\"unchecked\") public void subscribe(CoreSubscriber&lt;? super R> actual) &#123; if (actual instanceof ConditionalSubscriber) &#123; ConditionalSubscriber&lt;? super R> cs = (ConditionalSubscriber&lt;? super R>) actual; source.subscribe(new FluxMapFuseable.MapFuseableConditionalSubscriber&lt;>(cs, mapper)); return; &#125; source.subscribe(new FluxMapFuseable.MapFuseableSubscriber&lt;>(actual, mapper)); &#125; 最后，到MonoJust （datasource publisher） @Override public void subscribe(CoreSubscriber&lt;? super T> actual) &#123; actual.onSubscribe(Operators.scalarSubscription(actual, value)); &#125; 将actual包装到Subscription中，开始转入onSubscribe。 onSubscribe阶段onSubscribe阶段，执行Subscriber的onSubscribe方法，Subscriber这里是，LambdaMonoSubscriber。 然后，发出request(n)。 @Override public final void onSubscribe(Subscription s) &#123; if (Operators.validate(subscription, s)) &#123; this.subscription = s; if (subscriptionConsumer != null) &#123; try &#123; subscriptionConsumer.accept(s); &#125; catch (Throwable t) &#123; Exceptions.throwIfFatal(t); s.cancel(); onError(t); &#125; &#125; else &#123; s.request(Long.MAX_VALUE); &#125; &#125; &#125; request阶段紧接onSubscribe的代码继续说明。其中较重要的就是Subscription.request()方法。 然后一路request就会到最外层的，并执行Subscriber.onNext，也就到执行阶段。 执行阶段//代码摘自FluxMapFuseable.java @Override public void onNext(T t) &#123; if (sourceMode == ASYNC) &#123; actual.onNext(null); &#125; else &#123; if (done) &#123; Operators.onNextDropped(t, actual.currentContext()); return; &#125; R v; try &#123; v = Objects.requireNonNull(mapper.apply(t), //@1 \"The mapper returned a null value.\"); &#125; catch (Throwable e) &#123; onError(Operators.onOperatorError(s, e, t, actual.currentContext())); return; &#125; actual.onNext(v); //@2 &#125; &#125; 看@1、@2，各Subscriber会一直执行onNext并触发执行所声明的操作，最后触发下一步操作。 总结基于上面 核心模型-&gt;调用关系一节的分析，融合以上流程，然后，可得出如下整体流程图： WebFlux请求分发reactor的map，flatMap，concatMap map map是同步非阻塞的1对1的转换数据处理。map方法签名接受Function&lt;T, U&gt; 返回Flux&lt;U&gt;。 [ //同步执行乘法操作 Flux.just(1,2,3,4,5) .log() .map(i->&#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return i*10; &#125;) .subscribe(c->log.info(\"getInt:&#123;&#125;\",c)); flatMap flatMap是异步非阻塞的1对N的转换数据处理。flayMap方法签名接受Function&lt;T, Publisher&lt;V&gt;&gt; 返回 Flux&lt;V&gt;。 //异步执行乘法 Flux.just(1,2,3,4,5) .log() .flatMap(i-> Flux.just(i*10).delayElements(Duration.ofSeconds(1))) .subscribe(c ->log.info(\"getInt:&#123;&#125;\",c)); 总结：flatMap的转换Function要求返回一个Publisher，这个Publisher代表一个作用于元素的异步的转换操作；而map仅仅是同步的元素转换操作。 concatMap concatMap 操作符的作用是把流中的每个元素转换成一个流, 再把所有流进行合并. 与 flatMap不同的是,concatMap会根据原始流中的元素顺序依次把转换之后的流进行合并。 WebFlux 的DispatcherHandlerSpring MVC 的前端控制器是 DispatcherServlet，而WebFlux 的前端控制器是 DispatcherHandler，它实现了 WebHandler接口。DispatcherHandler完成 handler 的查找、调用和结果处理等步骤，关联的Bean如下： Bean 类型 解释 HandlerMapping 将请求映射到对应的 handler。主要的 HandlerMapping 实现有处理 @RequestMapping 注解的 RequestMappingHandlerMapping ，处理函数路由的RouterFunctionMapping，以及处理简单 URL 映射的 SimpleUrlHandlerMapping。 HandlerAdapter 帮助 DispatcherHandler 调用请求对应的 handler，而不用关心该 handler 具体的调用方式。例如，调用一个通过注解的方式定义的 controller 就需要寻找对应的注解，而 HandlerAdapter 的主要目的就是为了帮助 DispatcherHandler 屏蔽类似的细节. HandlerResultHandler 处理 handler 调用后的结果，并生成最后的响应。参考 Result Handling。 public interface WebHandler &#123; /** * Handle the web server exchange. * @param exchange the current server exchange * @return &#123;@code Mono&lt;Void>&#125; to indicate when request handling is complete */ Mono&lt;Void> handle(ServerWebExchange exchange); &#125; public class DispatcherHandler implements WebHandler, ApplicationContextAware &#123; ... @Override public Mono&lt;Void> handle(ServerWebExchange exchange) &#123; //流程1 if (this.handlerMappings == null) &#123; return createNotFoundError(); &#125; return Flux.fromIterable(this.handlerMappings) //流程2 .concatMap(mapping -> mapping.getHandler(exchange)) .next() //流程3 .switchIfEmpty(createNotFoundError()) //流程4 .flatMap(handler -> invokeHandler(exchange, handler)) //流程5 .flatMap(result -> handleResult(exchange, result)); &#125; &#125; ServerWebExchange对象每一次 HTTP 请求的信息（包括请求参数，路径，Cookie等） 从DispatcherHandler的handle实现可以看出WebFlux的请求分发流程： 判断整个接口映射 mappings集合是否为空，空则创建一个 Not Found 的请求错误响应； 根据具体的请求地址获取对应的 handlerMapping（处理方法）; handlerMapping为空的话找不到对应的处理方法，创建一个 Not Found 的请求错误响应； 通过 invokeHandler 方法找到对应的 HandlerAdapter 来完成调用 由 HandlerResultHandler 对结果进行处理，并生成响应 适用场景使用 Spring WebFlux，下游使用的安全认证层、数据访问层框架都必须使用 Reactive API 保证上下游都是匹配的，非阻塞的。然而Spring Data Reactive Repositories 目前只支持 MongoDB、Redis 和Couchbase 等几种不支持事务管理的 NOSQL，技术选型时需要权衡利弊和风险。 Spring MVC能满足场景的，就不需要更改为 Spring WebFlux，毕竟Reactive写法对比原本同步执行的程序写法很不同，而且很多基于Servlet线程模型的库将无法使用，如Spring Transaction……。 需要底层容器的支持（Netty和Servlet3.1+）。 适合应用在 IO 密集型的服务中（IO 密集型包括：磁盘IO密集型, 网络IO密集型），微服务网关就属于网络 IO 密集型，使用异步非阻塞式编程模型，能够显著地提升网关对下游服务转发的吞吐量。","categories":[{"name":"Java","slug":"Java","permalink":"https://blog.innnovation.cn/categories/Java/"},{"name":"Spring","slug":"Java/Spring","permalink":"https://blog.innnovation.cn/categories/Java/Spring/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.innnovation.cn/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"https://blog.innnovation.cn/tags/Spring/"},{"name":"WebFlux","slug":"WebFlux","permalink":"https://blog.innnovation.cn/tags/WebFlux/"}]},{"title":"git复习进阶及命令速查","slug":"git使用复习进阶速查手册","date":"2021-07-06T16:00:00.000Z","updated":"2021-07-13T07:41:13.096Z","comments":true,"path":"2021/07/07/git-shi-yong-fu-xi-jin-jie-su-cha-shou-ce/","link":"","permalink":"https://blog.innnovation.cn/2021/07/07/git-shi-yong-fu-xi-jin-jie-su-cha-shou-ce/","excerpt":"","text":"原理主要操作流程图 基本原理版本控制系统(VCS)主要分为中央式和分布式 中央式：将代码保存在中央服务器，加入编码的成员均与服务器通讯，在本地完成代码，上传/同步他人代码，代码由服务器统一管理 分布式：所有参与开发的人员都维护自己的本地仓库，对自己本地仓库提交切换分支等操作均在本地完成，远程仓库由仓库管理员管理分配权限，本地仓库和远程仓库有时候会各自维护自己的分支，而远程仓库更多只是起到团队同步代码的作用 git本地存在：本地仓库，暂存区和工作目录，git init时所在的目录就是工作目录，暂存区和本地仓库都存在.git隐藏文件中。 git文件有两种状态：是否被跟踪，未被跟踪的文件就和资源文件管理器中的普通文件一样，可以随意修改删除，但是被git add 添加到git追踪的文件本地仓库就会对他们负责，通过git status查看他们的状态来进行相应的操作。 一个项目中的gitignore文件指定了git要忽略的文件类型而不追踪他们。 前面所提到的暂存区是当我们在工作目录中进行文件操作时，要先添加到暂存区，然后再将暂存区中的文件快照提交到本地仓库，这样就实现了各个快照之间可以随意的进行回滚操作。 git对象从git设计者角度考虑，git存在一套内容寻址的文件系统，本质上是K-V键值对，通过key找value，从源码层面看，git将指针保存在git对象中，根据指针来找到需要的文件内容。git对象分为：commit tree blob对象。 commit结构体包含 tree_ptr,parent_ptr,author,time … tree结构记录文件列表list 每个文件的内容为blob commit对象记录每次提交到本地快照，每次提交一个就会让上一个commit对象指向新的commit（本质就是一个链表），当我们进行恢复commit操作时，就通过id来找到我们需要恢复的commit节点即可。而HEAD对象其实就是指向最近一个提交的commit，也就是最后一个commit 引用（分支）在使用git log 命令时我们会看到一些HEAD-&gt;master,origin/master,origin/HEAD等字样，他们本质上就是指向commit的引用，如果理解了前面的commit对象，就知道引用其实就是该对象名的一个别名（可以简单理解为C++的引用） 如果你想在某处创建 branch ，只需要输入一行 git branch 名称 切换分支 就是将HEAD指针指向别的分支 切换后同样可以commit新节点 如果你再切换到 master 去 commit，就会真正地出现分叉了 各个commit之间会形成分支树(Tree),任意branch之间都是平等的 任意branch包含的节点一定是从（branch最开始的commit节点）头节点开始的——1234-&gt;12347;1256-&gt;12567 每个commit有一个唯一SHA-1校验码用来区分，用户可以用过他们的引用（名字）来对他们进行操作 HEAD：当前commit引用branch：一个特别的引用，可以被HEAD指向，形象的理解为commit发生时会带着HEAD移动 master/main:默认branch1.新建时 2.clone时 而删除分支时，仅仅是删除引用 push的本质本质就是比较push过来的branch引用，从最后一个相同节点后面接上新的commit节点 push会把当前分支上的commit节点上传到远程库，git push会推送本地的默认分支，不是默认分支时需要git push origin branch_name来指定 远程库的HEAD引用不会因为push而改变，而是随着默认分支移动 merge的本质本质是将两个链表的最后一个节点合并成一个新节点，并让HEAD指向该节点 特殊情况1：冲突merge合并commit时，如果修改了同一个文件，最终导致无法生成一个文件快照，这就是冲突（conflict） 解决方法：解决冲突（修改掉冲突的代码）-&gt;手动commit/取消merge 解决冲突时根据需要删除&lt;&lt;&lt;&lt;&lt; ========== &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;这些辅助性字符即可 解决完后 git add shopping\\ list.txt # commit 前也需要先 add 一下 git commit 当然 也可以放弃解决冲突 git merge --abort 特殊情况2：HEAD领先于目标commitgit默认不进行任何操作 特殊情况3：HEAD落后git默认进行fast-forward，将head移动到最近的commit 同样，当发生在本地库和远程库时，下图也正解释了为什么从远程库同步代码时只需要git pull命令即可 git pull=git fetch + git merge(fast-forward) Feature Branching 工作流在最基本的团队工作模型中，所有人都工作在 master 上，写完了的 commit 可以通过 push 来发送到中央仓库，并且可以使用 pull 来获取到别人的最新 commit。这种工作模型解决了团队合作最基本的问题：多人并行开发和版本管理。 但这种工作模型也有它的限制：使用这种工作模型时，每个人的代码在被大家看到的时候，就是它进入正式的生产库的时候。所有人的工作都会被直接 push 到 master，这导致每个人的代码在正式启用前无法被别人看到，这样就让代码在正式启用前的讨论和 review（审阅）非常不方便。现在的商业团队，开发项目多是采用「边开发边发布、边开发边更新、边开发边修复」的持续开发策略，所以代码分享的不便会极大地影响团队的开发效率。 简介工作流的核心内容可以总结为两点： 任何新的功能（feature）或 bug 修复全都新建一个 branch 来写； branch 写完后，合并到 master，然后删掉这个 branch。 运行原理和流程假设团队分配给我一个任务，开发一个新功能books git checkout -b books git push origin books 于是我本地开发完成后push到远程库后产生books分支 然后代码审计人员想要查看我写的怎么样 git pull git chekcout books 如果代码通过审核，于是审计人员同意merge到master git checkout master git pull # merge 之前 pull 一下，让 master 更新到和远程仓库同步 git merge books 最后把合并后的结果 push 到了中央仓库，并删掉了 books 这个 branch 如果代码审核不通过，审计人员通知到我，我只需要在books上面继续修改，然后push，继续回到上面的流程，直到审核通过。 Pull RequestPull Request就是我们常说的PR，这是由Git仓库服务提供方提供的功能，方便团队讨论一个branch，并可以一键合并到master Rebase——在新位置重新提交本质上讲rebase就是将一个子树的根节点移动到另一个节点上，就是改变了他们的（子树）根节点。 git rebase 目标基础点 例： git checkout branch1 git rebase master 可以看出，通过 rebase，5 和 6 两条 commits 把基础点从 2 换成了 4 。通过这样的方式，就让本来分叉了的提交历史重新回到了一条线。这种「重新设置基础点」的操作，就是 rebase 的含义 另外，在 rebase 之后，记得切回 master 再 merge 一下，把 master 移到最新的 commit git checkout master git merge branch1 注意：为了避免和远端仓库发生冲突，一般不要从 master 向其他 branch 执行 rebase 操作。而如果是 master 以外的 branch 之间的 rebase（比如 branch1 和 branch2 之间），就不必这么多费一步，直接 rebase 就好 rebase -i:交互式rebasegit rebase -i HEAD^^ //回到倒数第二个提交 说明：在 Git 中，有两个「偏移符号」： ^ 和 ~。 ^ 的用法：在 commit 的后面加一个或多个 ^ 号，可以把 commit 往回偏移，偏移的数量是 ^ 的数量。例如：master^ 表示 master 指向的 commit 之前的那个 commit； HEAD^^ 表示 HEAD 所指向的 commit 往前数两个 commit。 ~ 的用法：在 commit 的后面加上 ~ 号和一个数，可以把 commit 往回偏移，偏移的数量是 ~ 号后面的数。例如：HEAD~5 表示 HEAD 指向的 commit往前数 5 个 commit。 编辑界面：选择 commit 和对应的操作这两行指示了两个信息： 需要处理哪些 commit； 怎么处理它们。 把 pick 修改成 edit 后，就可以退出编辑界面了： 上图的提示信息说明，rebase 过程已经停在了第二个 commit 的位置，那么现在你就可以去修改你想修改的内容了。 修改完成之后，和上节里的方法一样，用 commit --amend 来把修正应用到当前最新的 commit： git add 笑声 git commit --amend 继续 rebase 过程在修复完成之后，就可以用 rebase --continue 来继续 rebase 过程，把后面的 commit 直接应用上去。 git rebase --continue 然后，这次交互式 rebase 的过程就完美结束了，你的那个倒数第二个写错的 commit 就也被修正了 如果commit已经提交了发现错误，怎么优雅的改掉git commit --amend Git 会把你带到提交信息编辑界面。可以看到，提交信息默认是当前提交的提交信息。你可以修改或者保留它，然后保存退出。然后，你的最新 commit 就被更新了 撤销Commitgit reset --hard 目标commit 通过前面原理可知，节点是不会被删除的，如果还需要，可以记住SHA-1码来找到对应commit git reset --hard HEAD^ 撤销不是最新的commitgit rebase -i HEAD^^ 然后就会跳到 commit 序列的编辑界面，需要修改这个序列来进行操作。不过不同的是，之前修正 commit 的方法是把要修改的 commit 左边的 pick 改成 edit，而如果你要撤销某个 commit ，直接删掉这一行就好。 pick 的直接意思是「选取」，在这个界面的意思就是应用这个 commit。而如果把这一行删掉，那就相当于在 rebase 的过程中跳过了这个 commit，从而也就把这个 commit 撤销掉了。 用 rebase –onto 撤销提交除了用交互式 rebase ，你还可以用 rebase --onto 来更简便地撤销提交。 rebase 加上 --onto 选项之后，可以指定 rebase 的「起点」。一般的 rebase，告诉 Git 的是「我要把当前 commit 以及它之前的 commits 重新提交到目标 commit 上去，这其中，rebase 的「起点」是自动判定的：选取当前 commit 和目标 commit 在历史上的交叉点作为起点。 例如下面这种情况： 如果在这里执行： git rebase 第3个commit 那么 Git 会自动选取 3 和 5 的历史交叉点 2 作为 rebase 的起点，依次将 4 和 5 重新提交到 3 的路径上去。 而 --onto 参数，就可以额外给 rebase 指定它的起点。例如同样以上图为例，如果我只想把 5 提交到 3 上，不想附带上 4，那么我可以执行： git rebase --onto 第3个commit 第4个commit branch1 --onto 参数后面有三个附加参数：目标 commit、起点 commit（注意：rebase 的时候会把起点排除在外）、终点 commit。所以上面这行指令就会从 4 往下数，拿到 branch1 所指向的 5，然后把 5 重新提交到 3 上去。 同样的，你也可以用 rebase --onto 来撤销提交： git rebase --onto HEAD^^ HEAD^ branch1 上面这行代码的意思是：以倒数第二个 commit 为起点（起点不包含在 rebase 序列里哟），branch1 为终点，rebase 到倒数第三个 commit 上。 也就是这样： 代码已经 push 上去了才发现写错？有的时候，代码 push 到了中央仓库，才发现有个 commit 写错了。这种问题的处理分两种情况： 1. 出错的内容在你自己的 branch 由于在本地对已有的 commit 做了修改，这时再 push 就会失败，因为中央仓库包含本地没有的 commit。但这个和前面讲过的情况不同，这次的冲突不是因为同事 push 了新的提交，而是因为你刻意修改了一些内容，这个冲突是你预料到的，你本来就希望用本地的内容覆盖掉中央仓库的内容 git push origin branch1 -f 这样，在本地修改了错误的 commits，然后强制 push 上去，问题解决。 2. 出错的内容已经合并到 master用法很简单，你希望撤销哪个 commit，就把它填在后面： git revert HEAD^ 上面这行代码就会增加一条新的 commit，它的内容和倒数第二个 commit 是相反的，从而和倒数第二个 commit 相互抵消，达到撤销的效果。 在 revert 完成之后，把新的 commit 再 push 上去，这个 commit 的内容就被撤销了。它和前面所介绍的撤销方式相比，最主要的区别是，这次改动只是被「反转」了，并没有在历史中消失掉，你的历史中会存在两条 commit ：一个原始 commit ，一个对它的反转 commit。 分支策略明白了git仓库和分支的概念和操作，下面介绍几种常见开发的分支策略，可以使得版本库的演进保持简洁，主干清晰，各个分支各司其职、井井有条。 主分支Master 首先，代码库应该有一个、且仅有一个主分支。所有提供给用户使用的正式版本，都在这个主分支上发布。 Git主分支的名字，默认叫做Master。它是自动建立的，版本库初始化以后，默认就是在主分支在进行开发。 开发分支Develop 主分支只用来分布重大版本，日常开发应该在另一条分支上完成。我们把开发用的分支，叫做Develop。 这个分支可以用来生成代码的最新隔夜版本（nightly）。如果想正式对外发布，就在Master分支上，对Develop分支进行”合并”（merge）。 临时性分支 前面讲到版本库的两条主要分支：Master和Develop。前者用于正式发布，后者用于日常开发。其实，常设分支只需要这两条就够了，不需要其他了。 但是，除了常设分支以外，还有一些临时性分支，用于应对一些特定目的的版本开发。临时性分支主要有三种： * 功能（feature）分支 * 预发布（release）分支 * 修补bug（fixbug）分支 这三种分支都属于临时性需要，使用完以后，应该删除，使得代码库的常设分支始终只有Master和Develop。 1.功能分支 它是为了开发某种特定功能，从Develop分支上面分出来的。开发完成后，要再并入Develop。 功能分支的名字，可以采用feature-*的形式命名。 2.预发布分支 它是指发布正式版本之前（即合并到Master分支之前），我们可能需要有一个预发布的版本进行测试。 预发布分支是从Develop分支上面分出来的，预发布结束以后，必须合并进Develop和Master分支。它的命名，可以采用release-*的形式。 3.修补bug分支 软件正式发布以后，难免会出现bug。这时就需要创建一个分支，进行bug修补。 修补bug分支是从Master分支上面分出来的。修补结束以后，再合并进Master和Develop分支。它的命名，可以采用fixbug-*的形式。 IDEA操作git操作其实本不在乎用命令行还是界面，关键是理解分支和仓库的本质。新版idea的git操作非常非常方便，在IDE界面的右下角，和项目的右键GIT菜单都可以快捷打开所有常用命令，且均提供可视化操作界面，可谓老少皆宜，idea 永远滴神！ 可以用commit子窗口快速提交 右下角可以快速checkout切换分支 冲突处理界面也异常友好，除了显示两个冲突文件，还会显示前有一次正确提交的文件在中间以供参考 命令速查","categories":[{"name":"杂项","slug":"杂项","permalink":"https://blog.innnovation.cn/categories/%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"杂项","slug":"杂项","permalink":"https://blog.innnovation.cn/tags/%E6%9D%82%E9%A1%B9/"},{"name":"GIT","slug":"GIT","permalink":"https://blog.innnovation.cn/tags/GIT/"}]},{"title":"操作系统-第一个C函数-实现板级初始化","slug":"操作系统-第一个C函数-实现板级初始化","date":"2021-07-04T16:00:00.000Z","updated":"2021-07-08T02:41:44.243Z","comments":true,"path":"2021/07/05/cao-zuo-xi-tong-di-yi-ge-c-han-shu-shi-xian-ban-ji-chu-shi-hua/","link":"","permalink":"https://blog.innnovation.cn/2021/07/05/cao-zuo-xi-tong-di-yi-ge-c-han-shu-shi-xian-ban-ji-chu-shi-hua/","excerpt":"","text":"第一个C函数void hal_start() &#123; //第一步：初始化hal层 //第二步：初始化内核层 for(;;); return; &#125; 死循环避免函数返回 hal层初始化void init_hal() &#123; //初始化平台 //初始化内存 //初始化中断 return; &#125; 初始化平台、初始化内存、初始化中断的功能函数 初始化平台 把二级引导器建立的机器信息结构复制到 hal 层中的一个全局变量中，方便内核中的其它代码使用里面的信息，之后二级引导器建立的数据所占用的内存都会被释放。 要初始化图形显示驱动，内核在运行过程要在屏幕上输出信息 void machbstart_t_init(machbstart_t *initp) &#123; //清零 memset(initp, 0, sizeof(machbstart_t)); return; &#125; void init_machbstart() &#123; machbstart_t *kmbsp = &amp;kmachbsp; machbstart_t *smbsp = MBSPADR;//物理地址1MB处 machbstart_t_init(kmbsp); //复制，要把地址转换成虚拟地址 memcopy((void *)phyadr_to_viradr((adr_t)smbsp), (void *)kmbsp, sizeof(machbstart_t)); return; &#125; //平台初始化函数 void init_halplaltform() &#123; //复制机器信息结构 init_machbstart(); //初始化图形显示驱动 init_bdvideo(); return; &#125; kmachbsp结构体类型是 machbstart_t，这个结构和二级引导器所使用的一样 //全局变量定义变量放在data段 #define HAL_DEFGLOB_VARIABLE(vartype,varname) \\ EXTERN __attribute__((section(\".data\"))) vartype varname HAL_DEFGLOB_VARIABLE(machbstart_t,kmachbsp); init_bdvideo函数 void init_bdvideo() &#123; dftgraph_t *kghp = &amp;kdftgh; //初始化图形数据结构，里面放有图形模式，分辨率，图形驱动函数指针 init_dftgraph(); //初始bga图形显卡的函数指针 init_bga(); //初始vbe图形显卡的函数指针 init_vbe(); //清空屏幕 为黑色 fill_graph(kghp, BGRA(0, 0, 0)); //显示背景图片 set_charsdxwflush(0, 0); hal_background(); return; &#125; dftgraph_t结构体 typedef struct s_DFTGRAPH &#123; u64_t gh_mode; //图形模式 u64_t gh_x; //水平像素点 u64_t gh_y; //垂直像素点 u64_t gh_framphyadr; //显存物理地址 u64_t gh_fvrmphyadr; //显存虚拟地址 u64_t gh_fvrmsz; //显存大小 u64_t gh_onepixbits; //一个像素字占用的数据位数 u64_t gh_onepixbyte; u64_t gh_vbemodenr; //vbe模式号 u64_t gh_bank; //显存的bank数 u64_t gh_curdipbnk; //当前bank u64_t gh_nextbnk; //下一个bank u64_t gh_banksz; //bank大小 u64_t gh_fontadr; //字库地址 u64_t gh_fontsz; //字库大小 u64_t gh_fnthight; //字体高度 u64_t gh_nxtcharsx; //下一字符显示的x坐标 u64_t gh_nxtcharsy; //下一字符显示的y坐标 u64_t gh_linesz; //字符行高 pixl_t gh_deffontpx; //默认字体大小 u64_t gh_chardxw; u64_t gh_flush; u64_t gh_framnr; u64_t gh_fshdata; //刷新相关的 dftghops_t gh_opfun; //图形驱动操作函数指针结构体 &#125;dftgraph_t; typedef struct s_DFTGHOPS &#123; //读写显存数据 size_t (*dgo_read)(void* ghpdev,void* outp,size_t rdsz); size_t (*dgo_write)(void* ghpdev,void* inp,size_t wesz); sint_t (*dgo_ioctrl)(void* ghpdev,void* outp,uint_t iocode); //刷新 void (*dgo_flush)(void* ghpdev); sint_t (*dgo_set_bank)(void* ghpdev, sint_t bnr); //读写像素 pixl_t (*dgo_readpix)(void* ghpdev,uint_t x,uint_t y); void (*dgo_writepix)(void* ghpdev,pixl_t pix,uint_t x,uint_t y); //直接读写像素 pixl_t (*dgo_dxreadpix)(void* ghpdev,uint_t x,uint_t y); void (*dgo_dxwritepix)(void* ghpdev,pixl_t pix,uint_t x,uint_t y); //设置x，y坐标和偏移 sint_t (*dgo_set_xy)(void* ghpdev,uint_t x,uint_t y); sint_t (*dgo_set_vwh)(void* ghpdev,uint_t vwt,uint_t vhi); sint_t (*dgo_set_xyoffset)(void* ghpdev,uint_t xoff,uint_t yoff); //获取x，y坐标和偏移 sint_t (*dgo_get_xy)(void* ghpdev,uint_t* rx,uint_t* ry); sint_t (*dgo_get_vwh)(void* ghpdev,uint_t* rvwt,uint_t* rvhi); sint_t (*dgo_get_xyoffset)(void* ghpdev,uint_t* rxoff,uint_t* ryoff); &#125;dftghops_t; //刷新显存 void flush_videoram(dftgraph_t *kghp) &#123; kghp->gh_opfun.dgo_flush(kghp); return; &#125; 最后调用函数 //在halinit.c文件中 void init_hal() &#123; init_halplaltform(); return; &#125; //在hal_start.c文件中 void hal_start() &#123; init_hal();//初始化hal层，其中会调用初始化平台函数，在那里会调用初始化图形驱动 for(;;); return; &#125; 初始化内存相比较二级引导器的内存布局信息，内存管理器需要保存更多的信息，最好是顺序的内存布局信息，这样可以增加额外的功能属性，同时降低代码的复杂度。 定义结构s_PHYMMARGE #define PMR_T_OSAPUSERRAM 1 #define PMR_T_RESERVRAM 2 #define PMR_T_HWUSERRAM 8 #define PMR_T_ARACONRAM 0xf #define PMR_T_BUGRAM 0xff #define PMR_F_X86_32 (1&lt;&lt;0) #define PMR_F_X86_64 (1&lt;&lt;1) #define PMR_F_ARM_32 (1&lt;&lt;2) #define PMR_F_ARM_64 (1&lt;&lt;3) #define PMR_F_HAL_MASK 0xff typedef struct s_PHYMMARGE &#123; spinlock_t pmr_lock;//保护这个结构是自旋锁 u32_t pmr_type; //内存地址空间类型 u32_t pmr_stype; u32_t pmr_dtype; //内存地址空间的子类型，见上面的宏 u32_t pmr_flgs; //结构的标志与状态 u32_t pmr_stus; u64_t pmr_saddr; //内存空间的开始地址 u64_t pmr_lsize; //内存空间的大小 u64_t pmr_end; //内存空间的结束地址 u64_t pmr_rrvmsaddr;//内存保留空间的开始地址 u64_t pmr_rrvmend; //内存保留空间的结束地址 void* pmr_prip; //结构的私有数据指针，以后扩展所用 void* pmr_extp; //结构的扩展数据指针，以后扩展所用 &#125;phymmarge_t; 有些情况下内核要另起炉灶，不想把所有的内存空间都交给内存管理器去管理，所以要保留一部分内存空间，这就是上面结构中那两个 pmr_rrvmsaddr、pmr_rrvmend 字段的作用。 有了数据结构，我们还要写代码来操作它： u64_t initpmrge_core(e820map_t *e8sp, u64_t e8nr, phymmarge_t *pmargesp) &#123; u64_t retnr = 0; for (u64_t i = 0; i &lt; e8nr; i++) &#123; //根据一个e820map_t结构建立一个phymmarge_t结构 if (init_one_pmrge(&amp;e8sp[i], &amp;pmargesp[i]) == FALSE) &#123; return retnr; &#125; retnr++; &#125; return retnr; &#125; void init_phymmarge() &#123; machbstart_t *mbsp = &amp;kmachbsp; phymmarge_t *pmarge_adr = NULL; u64_t pmrgesz = 0; //根据machbstart_t机器信息结构计算获得phymmarge_t结构的开始地址和大小 ret_phymmarge_adrandsz(mbsp, &amp;pmarge_adr, &amp;pmrgesz); u64_t tmppmrphyadr = mbsp->mb_nextwtpadr; e820map_t *e8p = (e820map_t *)((adr_t)(mbsp->mb_e820padr)); //建立phymmarge_t结构 u64_t ipmgnr = initpmrge_core(e8p, mbsp->mb_e820nr, pmarge_adr); //把phymmarge_t结构的地址大小个数保存machbstart_t机器信息结构中 mbsp->mb_e820expadr = tmppmrphyadr; mbsp->mb_e820exnr = ipmgnr; mbsp->mb_e820exsz = ipmgnr * sizeof(phymmarge_t); mbsp->mb_nextwtpadr = PAGE_ALIGN(mbsp->mb_e820expadr + mbsp->mb_e820exsz); //phymmarge_t结构中地址空间从低到高进行排序，我已经帮你写好了 phymmarge_sort(pmarge_adr, ipmgnr); return; &#125; 根据 e820map_t 结构数组，建立了一个 phymmarge_t 结构数组，init_one_pmrge 函数正是把 e820map_t 结构中的信息复制到 phymmarge_t 结构中来。 调用 void init_halmm() &#123; init_phymmarge(); //init_memmgr(); return; &#125; 初始化中断在 x86 CPU 上，最多支持 256 个中断，还记得前面所说的中断表和中断门描述符吗，这意味着我们要准备 256 个中断门描述符和 256 个中断处理程序的入口。 typedef struct s_GATE &#123; u16_t offset_low; /* 偏移 */ u16_t selector; /* 段选择子 */ u8_t dcount; /* 该字段只在调用门描述符中有效。如果在利用调用门调用子程序时引起特权级的转换和堆栈的改变，需要将外层堆栈中的参数复制到内层堆栈。该双字计数字段就是用于说明这种情况发生时，要复制的双字参数的数量。*/ u8_t attr; /* P(1) DPL(2) DT(1) TYPE(4) */ u16_t offset_high; /* 偏移的高位段 */ u32_t offset_high_h; u32_t offset_resv; &#125;__attribute__((packed)) gate_t; //定义中断表 HAL_DEFGLOB_VARIABLE(gate_t,x64_idt)[IDTMAX]; 中断表其实是个 gate_t 结构的数组，由 CPU 的 IDTR 寄存器指向，IDTMAX 为 256 定义函数给中断表设置数据 //vector 向量也是中断号 //desc_type 中断门类型，中断门，陷阱门 //handler 中断处理程序的入口地址 //privilege 中断门的权限级别 void set_idt_desc(u8_t vector, u8_t desc_type, inthandler_t handler, u8_t privilege) &#123; gate_t *p_gate = &amp;x64_idt[vector]; u64_t base = (u64_t)handler; p_gate->offset_low = base &amp; 0xFFFF; p_gate->selector = SELECTOR_KERNEL_CS; p_gate->dcount = 0; p_gate->attr = (u8_t)(desc_type | (privilege &lt;&lt; 5)); p_gate->offset_high = (u16_t)((base >> 16) &amp; 0xFFFF); p_gate->offset_high_h = (u32_t)((base >> 32) &amp; 0xffffffff); p_gate->offset_resv = 0; return; &#125; 上面的代码，正是按照要求，把这些数据填入中断门描述符中的。有了中断门之后，还差中断处理程序，中断处理程序只负责这三件事： 保护 CPU 寄存器，即中断发生时的程序运行的上下文。 调用中断处理程序，这个程序可以是修复异常的，可以是设备驱动程序中对设备响应的程序。 恢复 CPU 寄存器，即恢复中断时程序运行的上下文，使程序继续运行。 先来写好完成以上三个功能的汇编宏代码，避免写 256 遍同样的代码，代码如下 &#x2F;&#x2F;保存中断后的寄存器 %macro SAVEALL 0 push rax push rbx push rcx push rdx push rbp push rsi push rdi push r8 push r9 push r10 push r11 push r12 push r13 push r14 push r15 xor r14,r14 mov r14w,ds push r14 mov r14w,es push r14 mov r14w,fs push r14 mov r14w,gs push r14 %endmacro &#x2F;&#x2F;恢复中断后寄存器 %macro RESTOREALL 0 pop r14 mov gs,r14w pop r14 mov fs,r14w pop r14 mov es,r14w pop r14 mov ds,r14w pop r15 pop r14 pop r13 pop r12 pop r11 pop r10 pop r9 pop r8 pop rdi pop rsi pop rbp pop rdx pop rcx pop rbx pop rax iretq %endmacro &#x2F;&#x2F;保存异常下的寄存器 %macro SAVEALLFAULT 0 push rax push rbx push rcx push rdx push rbp push rsi push rdi push r8 push r9 push r10 push r11 push r12 push r13 push r14 push r15 xor r14,r14 mov r14w,ds push r14 mov r14w,es push r14 mov r14w,fs push r14 mov r14w,gs push r14 %endmacro &#x2F;&#x2F;恢复异常下寄存器 %macro RESTOREALLFAULT 0 pop r14 mov gs,r14w pop r14 mov fs,r14w pop r14 mov es,r14w pop r14 mov ds,r14w pop r15 pop r14 pop r13 pop r12 pop r11 pop r10 pop r9 pop r8 pop rdi pop rsi pop rbp pop rdx pop rcx pop rbx pop rax add rsp,8 iretq %endmacro &#x2F;&#x2F;没有错误码CPU异常 %macro SRFTFAULT 1 push _NOERRO_CODE SAVEALLFAULT mov r14w,0x10 mov ds,r14w mov es,r14w mov fs,r14w mov gs,r14w mov rdi,%1 ;rdi, rsi mov rsi,rsp call hal_fault_allocator RESTOREALLFAULT %endmacro &#x2F;&#x2F;CPU异常 %macro SRFTFAULT_ECODE 1 SAVEALLFAULT mov r14w,0x10 mov ds,r14w mov es,r14w mov fs,r14w mov gs,r14w mov rdi,%1 mov rsi,rsp call hal_fault_allocator RESTOREALLFAULT %endmacro &#x2F;&#x2F;硬件中断 %macro HARWINT 1 SAVEALL mov r14w,0x10 mov ds,r14w mov es,r14w mov fs,r14w mov gs,r14w mov rdi, %1 mov rsi,rsp call hal_intpt_allocator RESTOREALL %endmacro 有的 CPU 异常，CPU 自动把异常码压入到栈中，而有的 CPU 异常没有异常码，为了统一，我们对没有异常码的手动压入一个常数，维持栈的平衡。 有了中断异常处理的宏，我们还要它们变成中断异常的处理程序入口点函数。汇编函数其实就是一个标号加一段汇编代码，C 编译器把 C 语言函数编译成汇编代码后，也是标号加汇编代码，函数名就是标号 //除法错误异常 比如除0 exc_divide_error: SRFTFAULT 0 //单步执行异常 exc_single_step_exception: SRFTFAULT 1 exc_nmi: SRFTFAULT 2 //调试断点异常 exc_breakpoint_exception: SRFTFAULT 3 //溢出异常 exc_overflow: SRFTFAULT 4 //段不存在异常 exc_segment_not_present: SRFTFAULT_ECODE 11 //栈异常 exc_stack_exception: SRFTFAULT_ECODE 12 //通用异常 exc_general_protection: SRFTFAULT_ECODE 13 //缺页异常 exc_page_fault: SRFTFAULT_ECODE 14 hxi_exc_general_intpfault: SRFTFAULT 256 //硬件1～7号中断 hxi_hwint00: HARWINT (INT_VECTOR_IRQ0+0) hxi_hwint01: HARWINT (INT_VECTOR_IRQ0+1) hxi_hwint02: HARWINT (INT_VECTOR_IRQ0+2) hxi_hwint03: HARWINT (INT_VECTOR_IRQ0+3) hxi_hwint04: HARWINT (INT_VECTOR_IRQ0+4) hxi_hwint05: HARWINT (INT_VECTOR_IRQ0+5) hxi_hwint06: HARWINT (INT_VECTOR_IRQ0+6) hxi_hwint07: HARWINT (INT_VECTOR_IRQ0+7) 有了中断程序入口地址，就可以设置中断门描述符 void init_idt_descriptor() &#123; //一开始把所有中断的处理程序设置为保留的通用处理程序 for (u16_t intindx = 0; intindx &lt;= 255; intindx++) &#123; set_idt_desc((u8_t)intindx, DA_386IGate, hxi_exc_general_intpfault, PRIVILEGE_KRNL); &#125; set_idt_desc(INT_VECTOR_DIVIDE, DA_386IGate, exc_divide_error, PRIVILEGE_KRNL); set_idt_desc(INT_VECTOR_DEBUG, DA_386IGate, exc_single_step_exception, PRIVILEGE_KRNL); set_idt_desc(INT_VECTOR_NMI, DA_386IGate, exc_nmi, PRIVILEGE_KRNL); set_idt_desc(INT_VECTOR_BREAKPOINT, DA_386IGate, exc_breakpoint_exception, PRIVILEGE_USER); set_idt_desc(INT_VECTOR_OVERFLOW, DA_386IGate, exc_overflow, PRIVILEGE_USER); //篇幅所限，未全部展示 set_idt_desc(INT_VECTOR_PAGE_FAULT, DA_386IGate, exc_page_fault, PRIVILEGE_KRNL); set_idt_desc(INT_VECTOR_IRQ0 + 0, DA_386IGate, hxi_hwint00, PRIVILEGE_KRNL); set_idt_desc(INT_VECTOR_IRQ0 + 1, DA_386IGate, hxi_hwint01, PRIVILEGE_KRNL); set_idt_desc(INT_VECTOR_IRQ0 + 2, DA_386IGate, hxi_hwint02, PRIVILEGE_KRNL); set_idt_desc(INT_VECTOR_IRQ0 + 3, DA_386IGate, hxi_hwint03, PRIVILEGE_KRNL); //篇幅所限，未全部展示 return; &#125; 调用 void init_halintupt() &#123; init_idt_descriptor(); init_intfltdsc(); return; &#125; 但是，前面我们只是解决了中断的 CPU 相关部分，而 CPU 只是响应中断，但是并不能解决产生中断的问题。比如缺页中断来了，我们要解决内存地址映射关系，程序才可以继续运行。再比如硬盘中断来了，我们要读取硬盘的数据，要处理这问题，就要写好相应的处理函数。 中断异常描述符 typedef struct s_INTFLTDSC&#123; spinlock_t i_lock; u32_t i_flg; u32_t i_stus; uint_t i_prity; //中断优先级 uint_t i_irqnr; //中断号 uint_t i_deep; //中断嵌套深度 u64_t i_indx; //中断计数 list_h_t i_serlist; //也可以使用中断回调函数的方式 uint_t i_sernr; //中断回调函数个数 list_h_t i_serthrdlst; //中断线程链表头 uint_t i_serthrdnr; //中断线程个数 void* i_onethread; //只有一个中断线程时直接用指针 void* i_rbtreeroot; //如果中断线程太多则按优先级组成红黑树 list_h_t i_serfisrlst; uint_t i_serfisrnr; void* i_msgmpool; //可能的中断消息池 void* i_privp; void* i_extp; &#125;intfltdsc_t; 如果内核或者设备驱动程序要安装一个中断处理函数，就要先申请一个 intserdsc_t 结构体，然后把中断函数的地址写入其中，最后把这个结构挂载到对应的 intfltdsc_t 结构中的 i_serfisrlst 链表中。 为什么不能直接把中断处理函数放在 intfltdsc_t 结构中呢，还要多此一举搞个 intserdsc_t 结构体呢？ 因为我们的计算机中可能有很多设备，每个设备都可能产生中断，但是中断控制器的中断信号线是有限的。你可以这样理解：中断控制器最多只能产生几十号中断号，而设备不止几十个，所以会有多个设备共享一根中断信号线。这就导致一个中断发生后，无法确定是哪个设备产生的中断，所以我们干脆让设备驱动程序来决定，因为它是最了解设备的。这里我们让这个 intfltdsc_t 结构上的所有中断处理函数都依次执行，查看是不是自己的设备产生了中断，如果是就处理，不是则略过。 //定义intfltdsc_t结构数组大小为256 HAL_DEFGLOB_VARIABLE(intfltdsc_t,machintflt)[IDTMAX]; 实现中断异常分发函数 //中断处理函数 void hal_do_hwint(uint_t intnumb, void *krnlsframp) &#123; intfltdsc_t *ifdscp = NULL; cpuflg_t cpuflg; //根据中断号获取中断异常描述符地址 ifdscp = hal_retn_intfltdsc(intnumb); //对断异常描述符加锁并中断 hal_spinlock_saveflg_cli(&amp;ifdscp->i_lock, &amp;cpuflg); ifdscp->i_indx++; ifdscp->i_deep++; //运行中断处理的回调函数 hal_run_intflthandle(intnumb, krnlsframp); ifdscp->i_deep--; //解锁并恢复中断状态 hal_spinunlock_restflg_sti(&amp;ifdscp->i_lock, &amp;cpuflg); return; &#125; //异常分发器 void hal_fault_allocator(uint_t faultnumb, void *krnlsframp) &#123; //我们的异常处理回调函数也是放在中断异常描述符中的 hal_do_hwint(faultnumb, krnlsframp); return; &#125; //中断分发器 void hal_hwint_allocator(uint_t intnumb, void *krnlsframp) &#123; hal_do_hwint(intnumb, krnlsframp); return; &#125; hal_run_intflthandle负责调用中断处理函数 void hal_run_intflthandle(uint_t ifdnr, void *sframe) &#123; intserdsc_t *isdscp; list_h_t *lst; //根据中断号获取中断异常描述符地址 intfltdsc_t *ifdscp = hal_retn_intfltdsc(ifdnr); //遍历i_serlist链表 list_for_each(lst, &amp;ifdscp->i_serlist) &#123; //获取i_serlist链表上对象即intserdsc_t结构 isdscp = list_entry(lst, intserdsc_t, s_list); //调用中断处理回调函数 isdscp->s_handle(ifdnr, isdscp->s_device, sframe); &#125; return; &#125; 初始化中断控制器把 CPU 端的中断搞定了以后，还有设备端的中断，这个可以交给设备驱动程序，但是 CPU 和设备之间的中断控制器，还需要我们出面解决。 多个设备的中断信号线都会连接到中断控制器上，中断控制器可以决定启用或者屏蔽哪些设备的中断，还可以决定设备中断之间的优先线，所以它才叫中断控制器 x86 平台上的中断控制器有多种，最开始是 8259A，然后是 IOAPIC，最新的是 MSI-X。为了简单的说明原理，我们选择了 8259A 中断控制器。 8259A 在任何 x86 平台上都可以使用，x86 平台使用了两片 8259A 芯片，以级联的方式存在。它拥有 15 个中断源（即可以有 15 个中断信号接入）。 上面直接和 CPU 连接的是主 8259A，下面的是从 8259A，每一个 8259A 芯片都有两个 I/O 端口，我们可以通过它们对 8259A 进行编程。 主 8259A 的端口地址是 0x20，0x21；从 8259A 的端口地址是 0xA0，0xA1 下面我们来做代码初始化，我们程序员可以向 8259A 写两种命令字： ICW 和 OCW；ICW 这种命令字用来实现 8259a 芯片的初始化。而 OCW 这种命令用来向 8259A 发布命令，以对其进行控制。OCW 可以在 8259A 被初始化之后的任何时候被使用 8259.c void init_i8259() &#123; //初始化主从8259a out_u8_p(ZIOPT, ICW1); out_u8_p(SIOPT, ICW1); out_u8_p(ZIOPT1, ZICW2); out_u8_p(SIOPT1, SICW2); out_u8_p(ZIOPT1, ZICW3); out_u8_p(SIOPT1, SICW3); out_u8_p(ZIOPT1, ICW4); out_u8_p(SIOPT1, ICW4); //屏蔽全部中断源 out_u8_p(ZIOPT1, 0xff); out_u8_p(SIOPT1, 0xff); return; &#125; 进入内核层由于内核层是从 hal 层进入的，必须在 hal_start() 函数中被调用，所以在此完成这个函数——init_krl() void init_krl() &#123; //禁止函数返回 die(0); return; &#125; 调用 void hal_start() &#123; //初始化hal层 init_hal(); //初始化内核层 init_krl(); return; &#125; 小结 一、HAL层调用链hal_start() A、先去处理HAL层的初始化-&gt;init_hal() -&gt;-&gt;init_halplaltform()初始化平台-&gt;-&gt;-&gt;init_machbstart()主要是把二级引导器建立的机器信息结构，复制到了hal层一份给内核使用，同时也为释放二级引导器占用内存做好准备。其做法就是拷贝了一份mbsp到kmbsp，其中用到了虚拟地址转换hyadr_to_viradr-&gt;-&gt;-&gt;init_bdvideo()初始化图形机构初始化BGA显卡 或 VBE图形显卡信息【函数指针的使用】清空屏幕找到”background.bmp”，并显示背景图片-&gt;-&gt;-&gt;-&gt;hal_dspversion（）输出版本号等信息【vsprintfk】其中，用ret_charsinfo根据字体文件获取字符像素信息 -&gt;-&gt;move_img2maxpadr()将移动initldrsve.bin到最大地址 -&gt;-&gt;init_halmm()初始化内存-&gt;-&gt;-&gt;init_phymmarge申请phymmarge_t内存根据 e820map_t 结构数组，复制数据到phymmarge_t 结构数组按内存开始地址进行排序 -&gt;-&gt;init_halintupt();初始化中断-&gt;-&gt;-&gt;init_descriptor();初始化GDT描述符x64_gdt-&gt;-&gt;-&gt;init_idt_descriptor();初始化IDT描述符x64_idt，绑定了中断编号及中断处理函数-&gt;-&gt;-&gt;init_intfltdsc();初始化中断异常表machintflt，拷贝了中断相关信息-&gt;-&gt;-&gt;init_i8259();初始化8529芯片中断-&gt;-&gt;-&gt;i8259_enabled_line(0);好像是取消mask，开启中断请求 最后，跳转去处理内核初始化-&gt;init_krl() 二、中断调用链，以硬件中断为例A、kernel.inc中，通过宏定义，进行了中断定义。以硬件中断为例，可以在kernel.inc中看到：宏为HARWINT，硬件中断分发器函数为hal_hwint_allocator%macro HARWINT 1 保存现场…… mov rdi, %1 mov rsi,rsp call hal_hwint_allocator 恢复现场……%endmacro B、而在kernel.asm中，定义了各种硬件中断编号，比如hxi_hwint00，作为中断处理入口ALIGN 16hxi_hwint00: HARWINT (INT_VECTOR_IRQ0+0) C、有硬件中断时，会先到达中断处理入口，然后调用到硬件中断分发器函数hal_hwint_allocator第一个参数为中断编号，在rdi第二个参数为中断发生时的栈指针，在rsi然后调用异常处理函数hal_do_hwint D、hal_do_hwint加锁调用中断回调函数hal_run_intflthandle释放锁 E、hal_run_intflthandle先获取中断异常表machintflt然后调用i_serlist 链表上所有挂载intserdsc_t 结构中的中断处理的回调函数，是否处理由函数自己判断 F、中断处理完毕 G、异常处理类似，只是触发源头不太一样而已","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.innnovation.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"OS","slug":"OS","permalink":"https://blog.innnovation.cn/tags/OS/"}]},{"title":"操作系统-硬件到软件启动初始化-设置工作模式与环境","slug":"操作系统-设置工作模式与环境","date":"2021-06-24T16:00:00.000Z","updated":"2021-07-05T06:15:14.845Z","comments":true,"path":"2021/06/25/cao-zuo-xi-tong-she-zhi-gong-zuo-mo-shi-yu-huan-jing/","link":"","permalink":"https://blog.innnovation.cn/2021/06/25/cao-zuo-xi-tong-she-zhi-gong-zuo-mo-shi-yu-huan-jing/","excerpt":"","text":"部分代码来源于极客时间《操作系统45讲》，这里记录了我自己操作的过程和思路 建立计算机（磁盘）一个常规操作系统的启动如下图： 而这里的helloOS中的文件我们称之为内核镜像文件，我们可以引导GRUB加载一个或者多个文件 多文件加载时grub会解析如下图的文件然后加载： 映像文件头描述符和文件描述符如下： //映像文件头描述符 typedef struct s_mlosrddsc &#123; u64_t mdc_mgic; //映像文件标识 u64_t mdc_sfsum;//未使用 u64_t mdc_sfsoff;//未使用 u64_t mdc_sfeoff;//未使用 u64_t mdc_sfrlsz;//未使用 u64_t mdc_ldrbk_s;//映像文件中二级引导器的开始偏移 u64_t mdc_ldrbk_e;//映像文件中二级引导器的结束偏移 u64_t mdc_ldrbk_rsz;//映像文件中二级引导器的实际大小 u64_t mdc_ldrbk_sum;//映像文件中二级引导器的校验和 u64_t mdc_fhdbk_s;//映像文件中文件头描述的开始偏移 u64_t mdc_fhdbk_e;//映像文件中文件头描述的结束偏移 u64_t mdc_fhdbk_rsz;//映像文件中文件头描述的实际大小 u64_t mdc_fhdbk_sum;//映像文件中文件头描述的校验和 u64_t mdc_filbk_s;//映像文件中文件数据的开始偏移 u64_t mdc_filbk_e;//映像文件中文件数据的结束偏移 u64_t mdc_filbk_rsz;//映像文件中文件数据的实际大小 u64_t mdc_filbk_sum;//映像文件中文件数据的校验和 u64_t mdc_ldrcodenr;//映像文件中二级引导器的文件头描述符的索引号 u64_t mdc_fhdnr;//映像文件中文件头描述符有多少个 u64_t mdc_filnr;//映像文件中文件头有多少个 u64_t mdc_endgic;//映像文件结束标识 u64_t mdc_rv;//映像文件版本 &#125;mlosrddsc_t; #define FHDSC_NMAX 192 //文件名长度 //文件头描述符 typedef struct s_fhdsc &#123; u64_t fhd_type;//文件类型 u64_t fhd_subtype;//文件子类型 u64_t fhd_stuts;//文件状态 u64_t fhd_id;//文件id u64_t fhd_intsfsoff;//文件在映像文件位置开始偏移 u64_t fhd_intsfend;//文件在映像文件的结束偏移 u64_t fhd_frealsz;//文件实际大小 u64_t fhd_fsum;//文件校验和 char fhd_name[FHDSC_NMAX];//文件名 &#125;fhdsc_t; 我用的是windows10+VMware的方式进行开发： 我在VMware中先建立了虚拟机： 1GB 1CPU 100MB硬盘 其他64位 在Ubuntu64bit下生产硬盘 在虚拟机中开辟一块全0的100MB的虚拟磁盘 dd bs=512 if=/dev/zero of=hd.img count=204800 ;bs:表示块大小，这里是512字节 ;if：表示输入文件，/dev/zero就是Linux下专门返回0数据的设备文件，读取它就返回0 ;of：表示输出文件，即我们的硬盘文件。 ;count：表示输出多少块 然后将这个文件转换为可识别的虚拟磁盘： 将文件转为设备并设置回环设备 格式化为EXT4文件系统 挂在虚拟磁盘到目录下 安装GRUB sudo losetup -a #查看所有设备，我的机器上已经有十几个设备了，不能使用这些设备的名称 sudo losetup /dev/loop15 hd.img #0-14都被占用了 sudo mkfs.ext4 -q /dev/loop15 sudo mount -o loop ./hd.img ./hdisk/ ;挂载硬盘文件 sudo mkdir ./hdisk/boot/ ;建立boot目录 第一步挂载虚拟硬盘文件为loop0回环设备 sudo losetup /dev/loop15 hd.img sudo mount -o loop ./hd.img ./hdisk/ ;挂载硬盘文件 第二步安装GRUB sudo grub-install --boot-directory=./hdisk/boot/ --force --allow-floppy /dev/loop15 ；--boot-directory 指向先前我们在虚拟硬盘中建立的boot目录。 ；--force --allow-floppy ：指向我们的虚拟硬盘设备文件/dev/loop0 安装完毕后用sudo在**/hdisk/boot/grub/** 目录下建立一个grub.cfg 文本文件,grub会通过该文件找到系统镜像 grub.cfg menuentry 'HelloOS' &#123; #名字随意 insmod part_msdos insmod ext2 set root='hd0' #我的机器只能指定hd0，表示第一块磁盘引导启动 multiboot2 /boot/HelloOS.eki #加载boot目录下的HelloOS.eki文件 boot #引导启动 &#125; set timeout_style=menu if [ \"$&#123;timeout&#125;\" = 0 ]; then set timeout=10 #等待10秒钟自动启动 fi 硬盘设置好后如图，hdisk文件夹中就是hd.img磁盘镜像中的内容，修改hdisk即可修改磁盘 然后将hd.img复制到物理机，通过StarWindConverter软件转换img磁盘格式为vmdk格式（让VMware可以加载） 建造二级引导器课程开源代码https://gitee.com/lmos/cosmos 先设计一个数据结构存放机器信息 typedef struct s_MACHBSTART &#123; u64_t mb_krlinitstack;//内核栈地址 u64_t mb_krlitstacksz;//内核栈大小 u64_t mb_imgpadr;//操作系统映像 u64_t mb_imgsz;//操作系统映像大小 u64_t mb_bfontpadr;//操作系统字体地址 u64_t mb_bfontsz;//操作系统字体大小 u64_t mb_fvrmphyadr;//机器显存地址 u64_t mb_fvrmsz;//机器显存大小 u64_t mb_cpumode;//机器CPU工作模式 u64_t mb_memsz;//机器内存大小 u64_t mb_e820padr;//机器e820数组地址 u64_t mb_e820nr;//机器e820数组元素个数 u64_t mb_e820sz;//机器e820数组大小 //…… u64_t mb_pml4padr;//机器页表数据地址 u64_t mb_subpageslen;//机器页表个数 u64_t mb_kpmapphymemsz;//操作系统映射空间大小 //…… graph_t mb_ghparm;//图形信息 &#125;__attribute__((packed)) machbstart_t; 下图为课程插图 接着实现GRUB头 流程：初始化 CPU 的寄存器，加载 GDT，切换到 CPU 的保护模式 MBT_HDR_FLAGS EQU 0x00010003 MBT_HDR_MAGIC EQU 0x1BADB002 MBT2_MAGIC EQU 0xe85250d6 global _start extern inithead_entry [section .text] [bits 32] _start: jmp _entry align 4 mbt_hdr: dd MBT_HDR_MAGIC dd MBT_HDR_FLAGS dd -(MBT_HDR_MAGIC+MBT_HDR_FLAGS) dd mbt_hdr dd _start dd 0 dd 0 dd _entry ALIGN 8 mbhdr: DD 0xE85250D6 DD 0 DD mhdrend - mbhdr DD -(0xE85250D6 + 0 + (mhdrend - mbhdr)) DW 2, 0 DD 24 DD mbhdr DD _start DD 0 DD 0 DW 3, 0 DD 12 DD _entry DD 0 DW 0, 0 DD 8 mhdrend: 关中断，加载GDT _entry: cli ；关中断 in al, 0x70 or al, 0x80 out 0x70,al ；关掉不可屏蔽中断 lgdt [GDT_PTR] ；加载GDT地址到GDTR寄存器 jmp dword 0x8 :_32bits_mode ；长跳转刷新CS影子寄存器 ;……………… ;GDT全局段描述符表 GDT_START: knull_dsc: dq 0 kcode_dsc: dq 0x00cf9e000000ffff kdata_dsc: dq 0x00cf92000000ffff k16cd_dsc: dq 0x00009e000000ffff ；16位代码段描述符 k16da_dsc: dq 0x000092000000ffff ；16位数据段描述符 GDT_END: GDT_PTR: GDTLEN dw GDT_END-GDT_START-1 ;GDT界限 GDTBASE dd GDT_ST 初始化段寄存器和通用寄存器、栈寄存器，为C函数做准备 _32bits_mode： mov ax, 0x10 mov ds, ax mov ss, ax mov es, ax mov fs, ax mov gs, ax xor eax,eax xor ebx,ebx xor ecx,ecx xor edx,edx xor edi,edi xor esi,esi xor ebp,ebp xor esp,esp mov esp,0x7c00 ；设置栈顶为0x7c00 call inithead_entry ；调用inithead_entry函数在inithead.c中实现 jmp 0x200000 ；跳转到0x200000地址 于是实现inithead_entry函数 函数中调用write_realintsvefile write_ldrkrlfile主要作用就是把映像文件中的 initldrsve.bin 文件和 initldrkrl.bin 文件写入到特定的内存地址空间中加载地址在宏中定义 而加载时又依赖find_file 和 m2mcopy函数 find_file 函数负责扫描映像文件中的文件头描述符，对比其中的文件名，然后返回对应的文件头描述符的地址，这样就可以得到文件在映像文件中的位置和大小了。 m2mcopy就是复制镜像到内存空间操作。 #define MDC_ENDGIC 0xaaffaaffaaffaaff #define MDC_RVGIC 0xffaaffaaffaaffaa #define REALDRV_PHYADR 0x1000 #define IMGFILE_PHYADR 0x4000000 #define IMGKRNL_PHYADR 0x2000000 #define LDRFILEADR IMGFILE_PHYADR #define MLOSDSC_OFF (0x1000) #define MRDDSC_ADR (mlosrddsc_t*)(LDRFILEADR+0x1000) void inithead_entry() &#123; write_realintsvefile(); write_ldrkrlfile(); return; &#125; //写initldrsve.bin文件到特定的内存中 void write_realintsvefile() &#123; fhdsc_t *fhdscstart = find_file(\"initldrsve.bin\"); if (fhdscstart == NULL) &#123; error(\"not file initldrsve.bin\"); &#125; m2mcopy((void *)((u32_t)(fhdscstart->fhd_intsfsoff) + LDRFILEADR), (void *)REALDRV_PHYADR, (sint_t)fhdscstart->fhd_frealsz); return; &#125; //写initldrkrl.bin文件到特定的内存中 void write_ldrkrlfile() &#123; fhdsc_t *fhdscstart = find_file(\"initldrkrl.bin\"); if (fhdscstart == NULL) &#123; error(\"not file initldrkrl.bin\"); &#125; m2mcopy((void *)((u32_t)(fhdscstart->fhd_intsfsoff) + LDRFILEADR), (void *)ILDRKRL_PHYADR, (sint_t)fhdscstart->fhd_frealsz); return; &#125; //在映像文件中查找对应的文件 fhdsc_t *find_file(char_t *fname) &#123; mlosrddsc_t *mrddadrs = MRDDSC_ADR; if (mrddadrs->mdc_endgic != MDC_ENDGIC || mrddadrs->mdc_rv != MDC_RVGIC || mrddadrs->mdc_fhdnr &lt; 2 || mrddadrs->mdc_filnr &lt; 2) &#123; error(\"no mrddsc\"); &#125; s64_t rethn = -1; fhdsc_t *fhdscstart = (fhdsc_t *)((u32_t)(mrddadrs->mdc_fhdbk_s) + LDRFILEADR); for (u64_t i = 0; i &lt; mrddadrs->mdc_fhdnr; i++) &#123; if (strcmpl(fname, fhdscstart[i].fhd_name) == 0) &#123; rethn = (s64_t)i; goto ok_l; &#125; &#125; rethn = -1; ok_l: if (rethn &lt; 0) &#123; error(\"not find file\"); &#125; return &amp;fhdscstart[rethn]; &#125; 在前面的GRUB头部分最后一行代码jmp 0x200000 跳转地址正好是initldrkrl.bin内存中的地址，该模块即为二级引导器主模块 由于模块改变，还需要加载 GDTR 和 IDTR 寄存器，然后初始化 CPU 相关的寄存器 _entry: cli lgdt [GDT_PTR]；加载GDT地址到GDTR寄存器 lidt [IDT_PTR]；加载IDT地址到IDTR寄存器 jmp dword 0x8 :_32bits_mode；长跳转刷新CS影子寄存器 _32bits_mode: mov ax, 0x10 ; 数据段选择子(目的) mov ds, ax mov ss, ax mov es, ax mov fs, ax mov gs, ax xor eax,eax xor ebx,ebx xor ecx,ecx xor edx,edx xor edi,edi xor esi,esi xor ebp,ebp xor esp,esp mov esp,0x90000 ；使得栈底指向了0x90000 call ldrkrl_entry ；调用ldrkrl_entry函数 xor ebx,ebx jmp 0x2000000 ；跳转到0x2000000的内存地址 jmp $ GDT_START: knull_dsc: dq 0 kcode_dsc: dq 0x00cf9a000000ffff ;a-e kdata_dsc: dq 0x00cf92000000ffff k16cd_dsc: dq 0x00009a000000ffff ；16位代码段描述符 k16da_dsc: dq 0x000092000000ffff ；16位数据段描述符 GDT_END: GDT_PTR: GDTLEN dw GDT_END-GDT_START-1 ;GDT界限 GDTBASE dd GDT_START IDT_PTR: IDTLEN dw 0x3ff IDTBAS dd 0 ；这是BIOS中断表的地址和长度 如何调用bios中断C语言运行在32位保护模式下，而中断在16位实模式下工作 于是需要实现一个切换运行模式的功能，流程如下： 保存 C 语言环境下的 CPU 上下文 ，即保护模式下的所有通用寄存器、段寄存器、程序指针寄存器，栈寄存器，把它们都保存在内存中。 切换回实模式，调用 BIOS 中断，把 BIOS 中断返回的相关结果，保存在内存中。 切换回保护模式，重新加载第 1 步中保存的寄存器。这样 C 语言代码才能重新恢复执行。 realadr_call_entry: pushad ;保存通用寄存器 push ds push es push fs ;保存4个段寄存器 push gs call save_eip_jmp ；调用save_eip_jmp pop gs pop fs pop es ;恢复4个段寄存器 pop ds popad ;恢复通用寄存器 ret save_eip_jmp: pop esi ；弹出call save_eip_jmp时保存的eip到esi寄存器中， mov [PM32_EIP_OFF],esi ；把eip保存到特定的内存空间中 mov [PM32_ESP_OFF],esp ；把esp保存到特定的内存空间中 jmp dword far [cpmty_mode]；长跳转这里表示把cpmty_mode处的第一个4字节装入eip，把其后的2字节装入cs cpmty_mode: dd 0x1000 dw 0x18 jmp $ jmp dword far [cpmty_mode]后面的0x18正是GDT 中的 16 位代码段描述符，偏址0x1000是要运行的代码 [bits 16] _start: _16_mode: mov bp,0x20 ;0x20是指向GDT中的16位数据段描述符 mov ds, bp mov es, bp mov ss, bp mov ebp, cr0 and ebp, 0xfffffffe mov cr0, ebp ；CR0.P&#x3D;0 关闭保护模式 jmp 0:real_entry ；刷新CS影子寄存器，真正进入实模式 real_entry: mov bp, cs mov ds, bp mov es, bp mov ss, bp ；重新设置实模式下的段寄存器 都是CS中值，即为0 mov sp, 08000h ；设置栈 mov bp,func_table add bp,ax call [bp] ；调用函数表中的汇编函数，ax是C函数中传递进来的 cli call disable_nmi mov ebp, cr0 or ebp, 1 mov cr0, ebp ；CR0.P&#x3D;1 开启保护模式 jmp dword 0x8 :_32bits_mode [BITS 32] _32bits_mode: mov bp, 0x10 mov ds, bp mov ss, bp；重新设置保护模式下的段寄存器0x10是32位数据段描述符的索引 mov esi,[PM32_EIP_OFF]；加载先前保存的EIP mov esp,[PM32_ESP_OFF]；加载先前保存的ESP jmp esi ；eip&#x3D;esi 回到了realadr_call_entry函数中 func_table: ;函数表 dw _getmmap ；获取内存布局视图的函数 dw _read ；读取硬盘的函数 dw _getvbemode ；获取显卡VBE模式 dw _getvbeonemodeinfo ；获取显卡VBE模式的数据 dw _setvbemode ；设置显卡VBE模式 进入二级引导器主函数 void ldrkrl_entry() &#123; init_bstartparm();//收集机器环境信息的主函数 return; &#125; 小结1、grub启动后，选择对应的启动菜单项，grub会通过自带文件系统驱动，定位到对应的eki文件 2、grub会尝试加载eki文件【eki文件需要满足grub多协议引导头的格式要求】 这些是在imginithead.asm中实现的，所以要包括： A、grub文件头，包括魔数、grub1和grub2支持等 B、定位的_start符号等 3、grub校验成功后，会调用_start，然跳转到_entry A、_entry中:关闭中断 B、加载GDT C、然后进入_32bits_mode，清理寄存器，设置栈顶 D、调用inithead_entry【C】 4、inithead_entry.c A、从imginithead.asm进入后，首先进入函数调用inithead_entry B、初始化光标，清屏 C、从eki文件内部，找到initldrsve.bin文件，并分别拷贝到内存的指定物理地址 D、从eki文件内部，找到initldrkrl.bin文件，并分别拷贝到内存的指定物理地址 E、返回imginithead.asm 5、imginithead.asm中继续执行 jmp 0x200000 而这个位置，就是initldrkrl.bin在内存的位置ILDRKRL_PHYADR 所以后面要执行initldrkrl.bin的内容 6、这样就到了ldrkrl32.asm的_entry A、将GDT加载到GDTR寄存器【内存】 B、将IDT加载到IDTR寄存器【中断】 C、跳转到_32bits_mode 初始寄存器 初始化栈 调用ldrkrl_entry【C】 7、ldrkrlentry.c A、初始化光标，清屏 B、收集机器参数init_bstartparm【C】 8、bstartparm.c A、初始化machbstart_t B、各类初始化函数，填充machbstart_t的内容 C、返回 9、ldrkrlentry.c A、返回 10、ldrkrl32.asm A、跳转到0x2000000地址继续执行 探查和搜集信息在引导器主函数中，需要检查 CPU 是否支持 64 位的工作模式、收集内存布局信息，看看是不是合乎我们操作系统的最低运行要求，还要设置操作系统需要的 MMU 页表、设置显卡模式、释放中文字体文件 void ldrkrl_entry() &#123; init_bstartparm();//收集机器环境信息的主函数 return; &#125; //========================================================= //初始化machbstart_t结构体，清0,并设置一个标志 void machbstart_t_init(machbstart_t* initp) &#123; memset(initp,0,sizeof(machbstart_t)); initp->mb_migc=MBS_MIGC; return; &#125; void init_bstartparm() &#123; machbstart_t* mbsp = MBSPADR;//1MB的内存地址 machbstart_t_init(mbsp); return; &#125; 上述代码中的结构体为1MB包含机器基本信息的结构体 检查cpuchk_cpuid、chk_cpu_longmode 来干两件事，一个是检查 CPU 否支持 CPUID 指令，然后另一个用 CPUID 指令检查 CPU 支持 64 位长模式 //通过改写Eflags寄存器的第21位，观察其位的变化判断是否支持CPUID int chk_cpuid() &#123; int rets = 0; __asm__ __volatile__( \"pushfl \\n\\t\" \"popl %%eax \\n\\t\" \"movl %%eax,%%ebx \\n\\t\" \"xorl $0x0200000,%%eax \\n\\t\" \"pushl %%eax \\n\\t\" \"popfl \\n\\t\" \"pushfl \\n\\t\" \"popl %%eax \\n\\t\" \"xorl %%ebx,%%eax \\n\\t\" \"jz 1f \\n\\t\" \"movl $1,%0 \\n\\t\" \"jmp 2f \\n\\t\" \"1: movl $0,%0 \\n\\t\" \"2: \\n\\t\" : \"=c\"(rets) : :); return rets; &#125; //检查CPU是否支持长模式 int chk_cpu_longmode() &#123; int rets = 0; __asm__ __volatile__( \"movl $0x80000000,%%eax \\n\\t\" \"cpuid \\n\\t\" //把eax中放入0x80000000调用CPUID指令 \"cmpl $0x80000001,%%eax \\n\\t\"//看eax中返回结果 \"setnb %%al \\n\\t\" //不为0x80000001,则不支持0x80000001号功能 \"jb 1f \\n\\t\" \"movl $0x80000001,%%eax \\n\\t\" \"cpuid \\n\\t\"//把eax中放入0x800000001调用CPUID指令，检查edx中的返回数据 \"bt $29,%%edx \\n\\t\" //长模式 支持位 是否为1 \"setcb %%al \\n\\t\" \"1: \\n\\t\" \"movzx %%al,%%eax \\n\\t\" : \"=a\"(rets) : :); return rets; &#125; //检查CPU主函数 void init_chkcpu(machbstart_t *mbsp) &#123; if (!chk_cpuid()) &#123; kerror(\"Your CPU is not support CPUID sys is die!\"); CLI_HALT(); &#125; if (!chk_cpu_longmode()) &#123; kerror(\"Your CPU is not support 64bits mode sys is die!\"); CLI_HALT(); &#125; mbsp->mb_cpumode = 0x40;//如果成功则设置机器信息结构的cpu模式为64位 return; &#125; 获取内存布局init_mem函数 #define ETYBAK_ADR 0x2000 #define PM32_EIP_OFF (ETYBAK_ADR) #define PM32_ESP_OFF (ETYBAK_ADR+4) #define E80MAP_NR (ETYBAK_ADR+64)//保存e820map_t结构数组元素个数的地址 #define E80MAP_ADRADR (ETYBAK_ADR+68) //保存e820map_t结构数组的开始地址 void init_mem(machbstart_t *mbsp) &#123; e820map_t *retemp; u32_t retemnr = 0; mmap(&amp;retemp, &amp;retemnr); if (retemnr == 0) &#123; kerror(\"no e820map\\n\"); &#125; //根据e820map_t结构数据检查内存大小 if (chk_memsize(retemp, retemnr, 0x100000, 0x8000000) == NULL) &#123; kerror(\"Your computer is low on memory, the memory cannot be less than 128MB!\"); &#125; mbsp->mb_e820padr = (u64_t)((u32_t)(retemp));//把e820map_t结构数组的首地址传给mbsp->mb_e820padr mbsp->mb_e820nr = (u64_t)retemnr;//把e820map_t结构数组元素个数传给mbsp->mb_e820nr mbsp->mb_e820sz = retemnr * (sizeof(e820map_t));//把e820map_t结构数组大小传给mbsp->mb_e820sz mbsp->mb_memsz = get_memsize(retemp, retemnr);//根据e820map_t结构数据计算内存大小。 return; &#125; void mmap(e820map_t **retemp, u32_t *retemnr) &#123; realadr_call_entry(RLINTNR(0), 0, 0); *retemnr = *((u32_t *)(E80MAP_NR)); *retemp = (e820map_t *)(*((u32_t *)(E80MAP_ADRADR))); return; &#125; 内存信息结构体 #define RAM_USABLE 1 //可用内存 #define RAM_RESERV 2 //保留内存不可使用 #define RAM_ACPIREC 3 //ACPI表相关的 #define RAM_ACPINVS 4 //ACPI NVS空间 #define RAM_AREACON 5 //包含坏内存 typedef struct s_e820&#123; u64_t saddr; /* 内存开始地址 */ u64_t lsize; /* 内存大小 */ u32_t type; /* 内存类型 */ &#125;e820map_t; mmap函数中调用了bios中断原因是，通过调中断获取e820map结构数组 其调用了实模式下的_getmmap函数来获取，代码如下 _getmmap: push ds push es push ss mov esi,0 mov dword[E80MAP_NR],esi mov dword[E80MAP_ADRADR],E80MAP_ADR ;e820map结构体开始地址 xor ebx,ebx mov edi,E80MAP_ADR loop: mov eax,0e820h ;获取e820map结构参数 mov ecx,20 ;e820map结构大小 mov edx,0534d4150h ;获取e820map结构参数必须是这个数据 int 15h ;BIOS的15h中断 jc .1 add edi,20 cmp edi,E80MAP_ADR+0x1000 jg .1 inc esi cmp ebx,0 jne loop ;循环获取e820map结构 jmp .2 .1: mov esi,0 ;出错处理，e820map结构数组元素个数为0 .2: mov dword[E80MAP_NR],esi ;e820map结构数组元素个数 pop ss pop es pop ds ret 初始化内核栈 #define IKSTACK_PHYADR (0x90000-0x10) #define IKSTACK_SIZE 0x1000 //初始化内核栈 void init_krlinitstack(machbstart_t *mbsp) &#123; if (1 > move_krlimg(mbsp, (u64_t)(0x8f000), 0x1001)) &#123; kerror(\"iks_moveimg err\"); &#125; mbsp->mb_krlinitstack = IKSTACK_PHYADR;//栈顶地址 mbsp->mb_krlitstacksz = IKSTACK_SIZE; //栈大小是4KB return; &#125; 内核空间为：0x8f000～（0x8f000+0x1001） 检查他们于其他空间是否有冲突即可 放置内核文件与字库文件 //放置内核文件 void init_krlfile(machbstart_t *mbsp) &#123; //在映像中查找相应的文件，并复制到对应的地址，并返回文件的大小，这里是查找kernel.bin文件 u64_t sz = r_file_to_padr(mbsp, IMGKRNL_PHYADR, \"kernel.bin\"); if (0 == sz) &#123; kerror(\"r_file_to_padr err\"); &#125; //放置完成后更新机器信息结构中的数据 mbsp->mb_krlimgpadr = IMGKRNL_PHYADR; mbsp->mb_krlsz = sz; //mbsp->mb_nextwtpadr始终要保持指向下一段空闲内存的首地址 mbsp->mb_nextwtpadr = P4K_ALIGN(mbsp->mb_krlimgpadr + mbsp->mb_krlsz); mbsp->mb_kalldendpadr = mbsp->mb_krlimgpadr + mbsp->mb_krlsz; return; &#125; //放置字库文件 void init_defutfont(machbstart_t *mbsp) &#123; u64_t sz = 0; //获取下一段空闲内存空间的首地址 u32_t dfadr = (u32_t)mbsp->mb_nextwtpadr; //在映像中查找相应的文件，并复制到对应的地址，并返回文件的大小，这里是查找font.fnt文件 sz = r_file_to_padr(mbsp, dfadr, \"font.fnt\"); if (0 == sz) &#123; kerror(\"r_file_to_padr err\"); &#125; //放置完成后更新机器信息结构中的数据 mbsp->mb_bfontpadr = (u64_t)(dfadr); mbsp->mb_bfontsz = sz; //更新机器信息结构中下一段空闲内存的首地址 mbsp->mb_nextwtpadr = P4K_ALIGN((u32_t)(dfadr) + sz); mbsp->mb_kalldendpadr = mbsp->mb_bfontpadr + mbsp->mb_bfontsz; return; &#125; 调用 r_file_to_padr 函数在映像中查找 kernel.bin 和 font.fnt 文件，并复制到对应的空闲内存空间中 建立 MMU 页表数据内核虚拟地址空间从 0xffff800000000000 开始，所以我们这个虚拟地址映射到从物理地址 0 开始，大小都是 0x400000000 即 16GB，也就是说我们要虚拟地址空间：0xffff800000000000～0xffff800400000000 映射到物理地址空间 0～0x400000000。 采用长模式下的 2MB 分页方式 #define KINITPAGE_PHYADR 0x1000000 void init_bstartpages(machbstart_t *mbsp) &#123; //顶级页目录 u64_t *p = (u64_t *)(KINITPAGE_PHYADR);//16MB地址处 //页目录指针 u64_t *pdpte = (u64_t *)(KINITPAGE_PHYADR + 0x1000); //页目录 u64_t *pde = (u64_t *)(KINITPAGE_PHYADR + 0x2000); //物理地址从0开始 u64_t adr = 0; if (1 > move_krlimg(mbsp, (u64_t)(KINITPAGE_PHYADR), (0x1000 * 16 + 0x2000))) &#123; kerror(\"move_krlimg err\"); &#125; //将顶级页目录、页目录指针的空间清0 for (uint_t mi = 0; mi &lt; PGENTY_SIZE; mi++) &#123; p[mi] = 0; pdpte[mi] = 0; &#125; //映射 for (uint_t pdei = 0; pdei &lt; 16; pdei++) &#123; pdpte[pdei] = (u64_t)((u32_t)pde | KPDPTE_RW | KPDPTE_P); for (uint_t pdeii = 0; pdeii &lt; PGENTY_SIZE; pdeii++) &#123;//大页KPDE_PS 2MB，可读写KPDE_RW，存在KPDE_P pde[pdeii] = 0 | adr | KPDE_PS | KPDE_RW | KPDE_P; adr += 0x200000; &#125; pde = (u64_t *)((u32_t)pde + 0x1000); &#125; //让顶级页目录中第0项和第((KRNL_VIRTUAL_ADDRESS_START) >> KPML4_SHIFT) &amp; 0x1ff项，指向同一个页目录指针页 p[((KRNL_VIRTUAL_ADDRESS_START) >> KPML4_SHIFT) &amp; 0x1ff] = (u64_t)((u32_t)pdpte | KPML4_RW | KPML4_P); p[0] = (u64_t)((u32_t)pdpte | KPML4_RW | KPML4_P); //把页表首地址保存在机器信息结构中 mbsp->mb_pml4padr = (u64_t)(KINITPAGE_PHYADR); mbsp->mb_subpageslen = (u64_t)(0x1000 * 16 + 0x2000); mbsp->mb_kpmapphymemsz = (u64_t)(0x400000000); return; &#125; 映射的核心逻辑由两重循环控制，外层循环控制页目录指针顶，只有 16 项，其中每一项都指向一个页目录，每个页目录中有 512 个物理页地址 物理地址每次增加 2MB，这是由 26～30 行的内层循环控制，每执行一次外层循环就要执行 512 次内层循环。顶级页目录中第 0 项和第 ((KRNL_VIRTUAL_ADDRESS_START) &gt;&gt; KPML4_SHIFT) &amp; 0x1ff 项，指向同一个页目录指针页，这样的话就能让虚拟地址：0xffff800000000000～0xffff800400000000 和虚拟地址：0～0x400000000，访问到同一个物理地址空间 0～0x400000000，这样做是有目的，内核在启动初期，虚拟地址和物理地址要保持相同。 设置图形模式 void init_graph(machbstart_t* mbsp) &#123; //初始化图形数据结构 graph_t_init(&amp;mbsp->mb_ghparm); //获取VBE模式，通过BIOS中断 get_vbemode(mbsp); //获取一个具体VBE模式的信息，通过BIOS中断 get_vbemodeinfo(mbsp); //设置VBE模式，通过BIOS中断 set_vbemodeinfo(); return; &#125; VBE 是显卡的一个图形规范标准，它定义了显卡的几种图形模式，每个模式包括屏幕分辨率，像素格式与大小，显存大小。调用 BIOS 10h 中断可以返回这些数据结构。 我们选择使用了 VBE 的 118h 模式，该模式下屏幕分辨率为 1024x768，显存大小是 16.8MB。显存开始地址一般为 0xe0000000 屏幕分辨率为 1024x768，即把屏幕分成 768 行，每行 1024 个像素点，但每个像素点占用显存的 32 位数据（4 字节，红、绿、蓝、透明各占 8 位）。我们只要往对应的显存地址写入相应的像素数据，屏幕对应的位置就能显示了。 像素点结构体如下 typedef struct s_PIXCL &#123; u8_t cl_b; //蓝 u8_t cl_g; //绿 u8_t cl_r; //红 u8_t cl_a; //透明 &#125;__attribute__((packed)) pixcl_t; #define BGRA(r,g,b) ((0|(r&lt;&lt;16)|(g&lt;&lt;8)|b)) //通常情况下用pixl_t 和 BGRA宏 typedef u32_t pixl_t; 像素点和显存位置对应如下 u32_t* dispmem = (u32_t*)mbsp->mb_ghparm.gh_framphyadr; dispmem[x + (y * 1024)] = pix; //x，y是像素的位置 连结操作 void init_bstartparm() &#123; machbstart_t *mbsp = MBSPADR; machbstart_t_init(mbsp); //检查CPU init_chkcpu(mbsp); //获取内存布局 init_mem(mbsp); //初始化内核栈 init_krlinitstack(mbsp); //放置内核文件 init_krlfile(mbsp); //放置字库文件 init_defutfont(mbsp); init_meme820(mbsp); //建立MMU页表 init_bstartpages(mbsp); //设置图形模式 init_graph(mbsp); return; &#125; 显示logo void logo(machbstart_t* mbsp) &#123; u32_t retadr=0,sz=0; //在映像文件中获取logo.bmp文件 get_file_rpadrandsz(\"logo.bmp\",mbsp,&amp;retadr,&amp;sz); if(0==retadr) &#123; kerror(\"logo getfilerpadrsz err\"); &#125; //显示logo文件中的图像数据 bmp_print((void*)retadr,mbsp); return; &#125; void init_graph(machbstart_t* mbsp) &#123; //……前面代码省略 //显示 logo(mbsp); return; &#125; logo为24位位图文件 在图格式的文件中，除了文件头的数据就是图形像素点的数据，只不过 24 位的位图每个像素占用 3 字节，并且位置是倒排的，即第一个像素的数据是在文件的最后，依次类推。我们只要依次将位图文件的数据，按照倒排次序写入显存中，这样就可以显示了 把二级引导器的文件和 logo 文件打包成映像文件，然后放在虚拟硬盘中。复制文件到虚拟硬盘中得先 mount，然后复制，最后转换成 VDI 格式的虚拟硬盘，再挂载到虚拟机上启动就行了。这也是为什么要手动建立硬盘的原因，打包命令如下 lmoskrlimg -m k -lhf initldrimh.bin -o Cosmos.eki -f initldrsve.bin initldrkrl.bin font.fnt logo.bmp 进入OS调用 Cosmos 第一个 C 函数之前，我们依然要写一小段汇编代码，切换 CPU 到长模式，初始化 CPU 寄存器和 C 语言要用的栈。因为目前代码执行流在二级引导器中，进入到 Cosmos 中这样在二级引导器中初始过的东西都不能用了。因为 CPU 进入了长模式，寄存器的位宽都变了，所以需要重新初始化 [section .start.text] [BITS 32] _start: cli mov ax,0x10 mov ds,ax mov es,ax mov ss,ax mov fs,ax mov gs,ax lgdt [eGdtPtr] ;开启 PAE mov eax, cr4 bts eax, 5 ; CR4.PAE &#x3D; 1 mov cr4, eax mov eax, PML4T_BADR ;加载MMU顶级页目录 mov cr3, eax ;开启 64bits long-mode mov ecx, IA32_EFER rdmsr bts eax, 8 ; IA32_EFER.LME &#x3D;1 wrmsr ;开启 PE 和 paging mov eax, cr0 bts eax, 0 ; CR0.PE &#x3D;1 bts eax, 31 ;开启 CACHE btr eax,29 ; CR0.NW&#x3D;0 btr eax,30 ; CR0.CD&#x3D;0 CACHE mov cr0, eax ; IA32_EFER.LMA &#x3D; 1 jmp 08:entry64 [BITS 64] entry64: mov ax,0x10 mov ds,ax mov es,ax mov ss,ax mov fs,ax mov gs,ax xor rax,rax xor rbx,rbx xor rbp,rbp xor rcx,rcx xor rdx,rdx xor rdi,rdi xor rsi,rsi xor r8,r8 xor r9,r9 xor r10,r10 xor r11,r11 xor r12,r12 xor r13,r13 xor r14,r14 xor r15,r15 mov rbx,MBSP_ADR mov rax,KRLVIRADR mov rcx,[rbx+KINITSTACK_OFF] add rax,rcx xor rcx,rcx xor rbx,rbx mov rsp,rax push 0 push 0x8 mov rax,hal_start ;调用内核主函数 push rax dw 0xcb48 jmp $ [section .start.data] [BITS 32] x64_GDT: enull_x64_dsc: dq 0 ekrnl_c64_dsc: dq 0x0020980000000000 ; 64-bit 内核代码段 ekrnl_d64_dsc: dq 0x0000920000000000 ; 64-bit 内核数据段 euser_c64_dsc: dq 0x0020f80000000000 ; 64-bit 用户代码段 euser_d64_dsc: dq 0x0000f20000000000 ; 64-bit 用户数据段 eGdtLen equ $ - enull_x64_dsc ; GDT长度 eGdtPtr: dw eGdtLen - 1 ; GDT界限 dq ex64_GDT 1～11 行表示加载 70～75 行的 GDT，13～17 行是设置 MMU 并加载在二级引导器中准备好的 MMU 页表，19～30 行是开启长模式并打开 Cache，34～54 行则是初始化长模式下的寄存器，55～61 行是读取二级引导器准备的机器信息结构中的栈地址，并用这个数据设置 RSP 寄存器。 最关键的是 63～66 行，它开始把 8 和 hal_start 函数的地址压入栈中。dw 0xcb48 是直接写一条指令的机器码——0xcb48，这是一条返回指令。这个返回指令有点特殊，它会把栈中的数据分别弹出到 RIP，CS 寄存器，这正是为了调用我们 os 的第一个 C 函数 hal_start。 小结211、返回到bstartparm.c 调用了chkcpmm.c的init_bstartpages 12、然后调用到了fs.c的move_krlimg函数申请了内存，建立了MMU页表： 顶级页目录，开始于0x1000000 页目录指针目录，开始于0x1001000，，共16项 ，其中每一项都指向一个页目录 页目录，开始于0x1002000， 每页指向512 个物理页，每页2MB【 0x200000】 让物理地址p[0]和虚拟地址p[((KRNL_VIRTUAL_ADDRESS_START) &gt;&gt; KPML4_SHIFT) &amp; 0x1ff]，指向同一个页目录指针页，确保内核在启动初期，虚拟地址和物理地址要保持相同 没搞清楚为什么虚拟地址是这个，也暂时没搞清楚为何要指向(u64_t)((u32_t)pdpte | KPML4_RW | KPML4_P) 最后，把页表首地址保存在机器信息结构中 13、返回到bstartparm.c 调用了graph.c的init_graph A、初始化了数据结构 B、调用init_bgadevice 首先获取GBA设备ID 检查设备最大分辨率 设置显示参数，并将参数保存到mbsp结构中 C、如果不是图形模式，要通过BIOS中断进行切换，设置显示参数，并将参数保存到mbsp结构中： 获取VBE模式，通过BIOS中断 获取一个具体VBE模式的信息，通过BIOS中断 设置VBE模式，通过BIOS中断 这三个方法同样用到了realadr_call_entry，调用路径与上面_getmmap类似，不再展开 D、初始化了一块儿内存 感觉会与物理地址与虚拟地址之间转换由一定关系 E、进行logo显示 调用get_file_rpadrandsz定位到位图文件 调用bmp_print，读入像素点，BGRA转换 最后调用write_pixcolor，写入到mbsp-&gt;mb_ghparm正确的位置，图像就显示出来了 14、然后一路返回 到bstartparm.c的init_bstartparm 到ldrkrlentry.c的ldrkrl_entry 到ldrkrl32.asm的call ldrkrl_entry 再往下是jmp 0x2000000 这个地址就是IMGKRNL_PHYADR，就是刚才放Cosmos.eki的位置","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.innnovation.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"OS","slug":"OS","permalink":"https://blog.innnovation.cn/tags/OS/"}]},{"title":"火绒安全一面病毒样本分析","slug":"火绒安全一面病毒样本分析","date":"2021-03-15T04:01:00.000Z","updated":"2021-06-25T14:41:13.003Z","comments":true,"path":"2021/03/15/huo-rong-an-quan-yi-mian-bing-du-yang-ben-fen-xi/","link":"","permalink":"https://blog.innnovation.cn/2021/03/15/huo-rong-an-quan-yi-mian-bing-du-yang-ben-fen-xi/","excerpt":"","text":"Summary ： 病毒高危行为： 请求一系列加密的未知IP的443端口，连接可能与其他恶意软件通讯;尝试HTTP但是全都失败了 使用了大量的微软提供的加密函数，已知是AES对称加密 过程中怀疑使用了代码混淆 存在多处绕检测，反调试行为：比如频繁分配内存调用native方法，存在用另一个用户开启线程；多处使用了RDTSC指令来比对执行时间检测虚拟化或反调试；存在大量延迟尝试绕检测；存在LdrLoadDll动态调用绕检测；可能读取PEB信息检测调试器；修改token权限 MD5 974d669e861896a0ebd61c7f2d6e8729 SHA-1 3166a8b05fab2c455586e717210bdf1dad621fc1 SHA-256 b00e7f74539cf39940c9044b6ac1d131a23c896c7905d71a087a01245232ada3 Vhash 0150366d556”z Authentihash 85badbaa56eef4169eb3c0127d9dace88a0b65b5965ad5a146a3477ab38914d4 SSDEEP 3072:9Wql7iWCRq3JV0npTvzY7hEsZNhh8J3Wn:9DNiWn52k7hEsBh TLSH T1B8D3490AE7D782B1FE9601B0167EB73F997152216B159EC3C7A01C20AD512E3A33E76D File type Win32 EXE Magic PE32 executable for MS Windows (GUI) Intel 80386 32-bit 调试过程logs查完文件基本信息之后发现不是常见的vc/C++程序,直接拉到OD里面跑起来,试了一下平时用的一些脱壳方法完全不凑效,发现该程序的PE比较奇怪导入和导出表地址均为0, 于是开始单步调试大法 程序一定需要运行时动态加载未加载完的dll,所以我打开内存映射窗口,看着主窗口,一遍单步一遍观察内存情况同时适当的跳过一些未知的加密循环,光标所在行的上一个call eax为Sleep大约3秒然后光标处加载剩余dll(function0040C2F0),随后马上进入一个Function 00416870, Function 00416870大概应该可以确定是程序的主体逻辑了.(本来尝试过dump 但是dump之后还是无法运行 大概是动态解密的关系吧) 进来16870之后是一个包含大量Sleep，获取CPU时钟相关API的函数，除此之外还有一个子调用，其中主要功能大概为遍历某个文件夹下的所有文件 随后经过一系列Sleephe GetTickCount，再后面看到几个消息处理的API一直到RtlAddVectoredExceptionHandler。然后出现一个子调用call Function 405B30 进入之后看到上面有几个子调用然后出现一个ConvertStringSecurityDescriptorToSecurityDescriptorW CreateMutexW 查阅MSDN之后，前者通常用于转换安全描述符，结合后面的CreateMutexW打开或者创建互斥量可以得出，这段代码大概率是在打开与当前病毒进程文件同名的信号互斥量，判断信号互斥量是否存在，防止病毒行为的二次执行。 那么我大胆推测病毒的主要功能就在这个代码的上方，结合ida的F5 我认为Function0041C6E0（v3 = mainfunction((int)v17, 0);） 包含了大量逻辑 v3 = mainfunction((int)v17, 0); v4 = 1; if ( !v3 ) { sub_4121B0(v16, 50); if ( ConvertStringSecurityDescriptorToSecurityDescriptorW(v16, 1, &amp;v10, 0) ) { v11 = v15; } else { v10 = 0; v11 = 0; } v5 = sub_41A120(); v15[0] = 12; v15[2] = 0; LODWORD(v13) = -1640531527 * v5; HIDWORD(v13) = -1640531527 * sub_41A120(); v15[1] = v10; v14 = sub_403020(v17, 2 * v2, v13, HIDWORD(v13)); if ( dword_41F138(&amp;v13, &amp;v12) &lt; 0 ) { sub_4121B0(v17, 119); sub_401CB0(v17, 128, v17, v13, v14); v12 = v17; } v6 = sub_4121B0(v16, 27); sub_41DD90((char *)v16 + 2 * v6, v12, 100); v7 = createMute(v11, 1, v16); *this = v7; if ( v10 ) { localFree(v10); v7 = *this; } if ( !v7 || (v8 = getLastError(), v4 = 1, v8 != 183) ) v4 = 0; } return v4; 进入这个函数 发现非常非常的长，再逐行分析的话非常不容易，于是我查找所有的模块间调用同时查找所有模块中的名称，查看一下是否有一些敏感的api调用。 以下为我整理的可疑函数： 加密： 00406D10 CryptAcquireContextW,CryptImportKey,CryptSetKeyParam,CryptSetKeyParam,CryptDecrypt,CryptReleaseContext,CryptDestroyKey 0040E760 CryptAcquireContextW,CryptCreateHash,CryptHashData,CryptGetHashParam,CryptGetHashParam,CryptDestroyHash,CryptReleaseContext 0041A890 CryptStringToBinaryW,CryptStringToBinaryW 0040E300 CryptBinaryToStringW,CryptBinaryToStringW 网络：（函数在OD 中并没有执行 可能是因为检测到调试器吧？unsure ） 00415C80 WSAStartup WSACleanup HeapCreate GetProcessHeap RtlAllocateHeap FreeAddrInfoW getaddrinfo FreeAddrInfoW RtlFreeHeap 004067A0 GetCurrentProcess,OpenProcessToken,LookupPrivilegeValueW,AdjustTokenPrivileges,FindCloseChangeNotification 00407870 GetCurrentProcess,OpenProcessToken,LookupPrivilegeValueW,AdjustTokenPrivileges,RevertToSelf,DuplicateTokenEx,CloseHandle,AdjustTokenPrivileges,CloseHandle 0040DB40 GetStartupInfoW,GetCurrentProcess,OpenProcessToken,LookupPrivilegeValueW,AdjustTokenPrivileges,OpenProcess,OpenProcessToken,GetTokenInformation,AllocateAndInitializeSid,EqualSid,OpenProcessToken,RevertToSelf,DuplicateTokenEx,GetTokenInformation,GetTokenInformation,LookupAccountSidW,CreateProcessAsUserW,GetLastError,CloseHandle,CloseHandle,CloseHandle,AdjustTokenPrivileges,CloseHandle 0040BFB0 GetCurrentProcess,OpenProcessToken,LookupPrivilegeValueW,AdjustTokenPrivileges,CloseHandle, 可能的 反调试，绕检测： 004021A0 RtlReAllocateHeap,NtDelayExecution,NtDelayExecution, 0041C6E0 NtQuerySstemInformation,NtQueryObject,GetCurrentProcess,NtQuerySystemInformation,OpenProcess,DuplicateHandle,NtQueryObject,NtQueryObject,NtQueryObject,FindCloseChangeNotification,FindCloseChangeNotification（CheckRemoteDebuggerPresent中会调用NtQueryInformationProcess函数） 004116E0 GetProcAddress,NtQueryInformationProcess 00408810 rdtsc 通过统计时间，判断当前环境是否是虚拟 0041B7A0 GetAdaptersInfo,GetAdaptersInfo 获取网络适配器信息 0040B4D0 LdrLoadDll,未公开的内核调用加载dll 大量的延迟函数： 419FD0 Sleep 4021D6 NtDelayExecution 403643 Sleep 4021D6 NtDelayExecution 4021D6 NtDelayExecution 408416 Sleep 4171A8 Sleep 4021D6 NtDelayExecution 可能用另一个用户启动线程： 0040DB40 GetStartupInfoW,GetCurrentProcess,OpenProcessToken,LookupPrivilegeValueW,AdjustTokenPrivileges,OpenProcess,OpenProcessToken,GetTokenInformation,AllocateAndInitializeSid,EqualSid,OpenProcessToken,RevertToSelf,DuplicateTokenEx,GetTokenInformation,GetTokenInformation,LookupAccountSidW,CreateProcessAsUserW,GetLastError,CloseHandle,CloseHandle,CloseHandle,AdjustTokenPrivileges,CloseHandle, 动态调用api ： 00405940 LoadLibraryW,GetProcAddress, 程序大致流程和逻辑 设置了一个计时器和消息处理，然后创建了heap，在最下面的if出call 405b30 来到405b30 遍历完目录之后进入41c6e0 综合上述信息可得v117的结果应该是通过rdtsc测量时间来检测虚拟化 那么从125行到172行一定是具体检测的逻辑，对于具体怎么测量我不是很清楚 变量v5处再次调用41a120应该是测量前后两次时间差作比较的 接下来又出现3个子调用 第一个 4195a0 408c30 存在循环和位运算，和上一个函数的入参密码相关 第二个 打开句柄调整token权限 第三个 408810是之前的rdtsc指令，getTickCount上下都存在多个rdtsc 执行完第三个函数之后的变量v106和v117存在大量相似的计算和比较 V106还额外执行了上图的3个函数，结合逻辑可以得出上述的推测应该是正确的，即通过106和117获取检测虚拟化，同时在第一个调用处尝试隐藏一些信息（4195a0每次进入循环虚拟机就卡死了，不知道具体是在做什么样的加密） 接下来到label58 4148c0 执行到返回 中间又是包含一堆数学计算 然后出现一个入参包括v24，即上面计算结果的一个复制对象句柄函数 进入41a120之后发现又是一堆rdtsc和位运算 跟v69执行同样的操作在下面还有个v72=sub_41a120(); 函数结尾处 停止监视通知更改句柄 该函数执行完后if判断通过 V7后面的操作应该就是判断是否重复执行了，所以该函数分析结束 返回到外层函数 40e940 生成sid 判断sid相等 40e5d0 StrStrlW GetSystemDirectoryW 大致逻辑下图 其中的41A840 返回a1不超过a2的第一个0地址 8460函数里面逻辑比价复杂 总体上应该就是一个获取目录比较确定目录同时包含加密解密的过程 在Function 00416870函数主体中一路分析 直到此处 发现第一个网络连接httpAPiCalled，具体调用时数据如下图 跟进dword_41f044的函数看到出现了http相关的api 可以推测这个函数所在的循环 会循环请求ip地址列表中的ip，由于没有网络安全方面的工具，无法检测目标ip的安全性 继续调试 这个函数明显发送了请求 并 且返回了请求失败 我在httpcall的地方留了断电 ，F9直接发现了第二个尝试请求的ip 继续F9 我在realrequest的地方F9至少等待了5次以上，不清楚这个 于是我打开wireshark对目标ip进行监视 192.168.88.135 51.77.112.255 TCP 66 49942 → 443 [SYN] Seq=0 Win=64240 Len=0 MSS=1460 WS=256 SACK_PERM=1 关于其连接知道的信息只有这么多了，未知的具体网络行为","categories":[{"name":"逆向分析","slug":"逆向分析","permalink":"https://blog.innnovation.cn/categories/%E9%80%86%E5%90%91%E5%88%86%E6%9E%90/"}],"tags":[{"name":"逆向","slug":"逆向","permalink":"https://blog.innnovation.cn/tags/%E9%80%86%E5%90%91/"},{"name":"病毒分析","slug":"病毒分析","permalink":"https://blog.innnovation.cn/tags/%E7%97%85%E6%AF%92%E5%88%86%E6%9E%90/"}]},{"title":"逆向工程学习录——Detour / Hook Functions","slug":"Detour  Hook Functions","date":"2020-12-28T23:12:00.000Z","updated":"2021-06-07T16:21:44.648Z","comments":true,"path":"2020/12/29/detour-hook-functions/","link":"","permalink":"https://blog.innnovation.cn/2020/12/29/detour-hook-functions/","excerpt":"","text":"在游戏逆向中我们需要在某个执行函数的地方添加自己的代码。案例：HackMe.exe 每按一次空格扣两滴血目标：hook扣血函数，改为double加血拉进Od后定位到扣血的函数如下图 00BB2740 55 push ebp 00BB2741 8BEC mov ebp,esp 00BB2743 81EC CC000000 sub esp,0xCC 00BB2749 53 push ebx 00BB274A 56 push esi ; HackMe. 00BB274B 57 push edi ; HackMe. 00BB274C 51 push ecx ; HackMe. 00BB274D 8DBD 34FFFFFF lea edi,dword ptr ss:[ebp-0xCC] 00BB2753 B9 33000000 mov ecx,0x33 00BB2758 B8 CCCCCCCC mov eax,0xCCCCCCCC 00BB275D F3:AB rep stos dword ptr es:[edi] 00BB275F 59 pop ecx ; kernel32.771F6359 00BB2760 894D F8 mov dword ptr ss:[ebp-0x8],ecx ; HackMe. 00BB2763 8B45 F8 mov eax,dword ptr ss:[ebp-0x8] 00BB2766 8B08 mov ecx,dword ptr ds:[eax] 00BB2768 2B4D 08 sub ecx,dword ptr ss:[ebp+0x8] 00BB276B 8B55 F8 mov edx,dword ptr ss:[ebp-0x8] 00BB276E 890A mov dword ptr ds:[edx],ecx ; HackMe. 00BB2770 8B45 F8 mov eax,dword ptr ss:[ebp-0x8] 00BB2773 8338 00 cmp dword ptr ds:[eax],0x0 00BB2776 7F 07 jg short HackMe.00BB277F 00BB2778 8B45 F8 mov eax,dword ptr ss:[ebp-0x8] 00BB277B C640 08 01 mov byte ptr ds:[eax+0x8],0x1 00BB277F 5F pop edi ; kernel32.771F6359 00BB2780 5E pop esi ; kernel32.771F6359 00BB2781 5B pop ebx ; kernel32.771F6359 00BB2782 8BE5 mov esp,ebp 00BB2784 5D pop ebp ; kernel32.771F6359 00BB2785 C2 0400 retn 0x4 地址00BB2768 sub ecx,dword ptr ss:[ebp+0x8]我们想要把该指令改为jmp [ourFunc]然后在ourFunc中在jmp回来。 代码实现 #include bool Hook(void* toHook, void* ourFunct, int len) &#123; if (len &lt; 5) &#123; return false; &#125; DWORD curProtection; VirtualProtect(toHook, len, PAGE_EXECUTE_READWRITE, &amp;curProtection); memset(toHook, 0x90, len); DWORD relativeAddress = ((DWORD)ourFunct - (DWORD)toHook) - 5; *(BYTE*)toHook = 0xE9; *(DWORD*)((DWORD)toHook + 1) = relativeAddress; DWORD temp; VirtualProtect(toHook, len, curProtection, &amp;temp); return true; &#125; DWORD jmpBackAddy; void __declspec(naked) ourFunct() &#123; __asm &#123; add ecx, ecx mov edx, [ebp - 8] jmp[jmpBackAddy] &#125; &#125; DWORD WINAPI MainThread(LPVOID param) &#123; int hookLength = 6; DWORD hookAddress = 0xBB2768; jmpBackAddy = hookAddress + hookLength; Hook((void*)hookAddress, ourFunct, hookLength); while (true) &#123; if (GetAsyncKeyState(VK_ESCAPE)) break; Sleep(50); &#125; FreeLibraryAndExitThread((HMODULE)param, 0); return 0; &#125; BOOL WINAPI DllMain(HINSTANCE hModule, DWORD dwReason, LPVOID lpReserved) &#123; switch (dwReason) &#123; case DLL_PROCESS_ATTACH: CreateThread(0, 0, MainThread, hModule, 0, 0); break; &#125; return TRUE; &#125; 由于jmp指令最少占5字节，所以我们必须覆写连同目标指令下一条指令连起来为6字节 运行HackMe 并注入dll跳转成功","categories":[{"name":"逆向分析","slug":"逆向分析","permalink":"https://blog.innnovation.cn/categories/%E9%80%86%E5%90%91%E5%88%86%E6%9E%90/"}],"tags":[{"name":"逆向","slug":"逆向","permalink":"https://blog.innnovation.cn/tags/%E9%80%86%E5%90%91/"},{"name":"Detour Hook","slug":"Detour-Hook","permalink":"https://blog.innnovation.cn/tags/Detour-Hook/"}]},{"title":"记一次用python做的多元线性回归分析","slug":"python多元线性回归分析","date":"2020-01-23T20:47:00.000Z","updated":"2021-06-08T02:10:30.854Z","comments":true,"path":"2020/01/24/python-duo-yuan-xian-xing-hui-gui-fen-xi/","link":"","permalink":"https://blog.innnovation.cn/2020/01/24/python-duo-yuan-xian-xing-hui-gui-fen-xi/","excerpt":"","text":"相关背景调查分析某鱼主播收到付费礼物收入和免费礼物收入模型如下： ; 抓取数据主播相关信息 主播名,主播编号,礼物总收入,付费礼物收入,免费礼物收入,礼物人数,弹幕人数,直播时长,人气峰值,订阅变化,活跃观众,订阅数,抓取时间,直播类型 弹幕相关信息 主播编号,用户名,用户id,用户全站等级,是否粉丝弹幕标记,徽章昵称,用户粉丝等级,弹幕内容,抓取时间,cid 清洗转换数据根据模型要求，从弹幕数据中计算出所需自变量的值合并到主播相关信息中并做简单清洗工作 import pandas as pd import numpy as np import re import os DataDF = pd.read_csv('清洗数据.csv',encoding = \"utf-8\",dtype = str) streamerId=DataDF['主播编号'] paidGiftIncome=np.log(DataDF['付费礼物收入'].astype(float)) freeGifgIncome=np.log(DataDF['免费礼物收入'].astype(float)) subscribeNum=np.log(DataDF['订阅数'].astype(int)) fansNum=np.log(DataDF['活跃观众'].astype(int)) streamType=DataDF['直播类型'].astype(int) topHot=np.log(DataDF['人气峰值'].astype(int)) userLevelVariance=[] fansLevelVariance=[] fansBarrageNum=[] nfansBarrageNum=[] path = 'danmu/' files=[] for f in DataDF['主播编号']: file=f+'.csv' files.append(file) df=pd.read_csv(path+file,encoding='utf-8',dtype=str,keep_default_na=False) dupDf=df.drop_duplicates('用户id','last') userLevelVariance.append(np.var(dupDf['用户全站等级'].astype(int))) fansLevelVariance.append(np.var(dupDf['用户粉丝等级'].astype(float))) mid=np.median(dupDf['用户全站等级'].astype(int)) fansBarrageNum.append(np.log(df[(df['用户全站等级'].astype(int)>= mid )].size)) nfansBarrageNum.append(np.log(df[(df['用户全站等级'].astype(int)&lt; mid )].size)) dataframe = pd.DataFrame(&#123;'主播id':streamerId,'ln关注数':subscribeNum,'ln粉丝数':fansNum,'直播类型':streamType,'ln人气峰值':topHot,'用户等级方差':userLevelVariance,'粉丝等级方差':fansLevelVariance,'ln(粉丝弹幕数量)':fansBarrageNum,'ln(非粉丝弹幕数量)':nfansBarrageNum,'收费礼物收入':paidGiftIncome,'免费礼物收入':freeGifgIncome&#125;) dataframe.to_csv(\"data.csv\",index=False,sep=',') 回归分析讲导出的data文件作为数据全集，划分数据集后使用sklearn进行回归分析 最佳拟合线:截距 [-4.6001933 -2.76872536] ,回归系数： [[-0.46457999 0.85992775 0.96507715 0.59494828 0.02850018 0.00734763-0.10940398 0.17530741][-0.01520268 0.93765167 0.14050881 -0.02137043 0.00608183 -0.022550790.3406813 -0.27023856]] import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from pandas import DataFrame,Series from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression df = pd.read_csv('data.csv') newDf = df[['ln关注数','ln粉丝数','直播类型','ln人气峰值','用户等级方差','粉丝等级方差','ln(粉丝弹幕数量)','ln(非粉丝弹幕数量)','收费礼物收入','免费礼物收入']] print('head:',newDf.head(),'\\nShape:',newDf.shape) X= newDf[['ln关注数','ln粉丝数','直播类型','ln人气峰值','用户等级方差','粉丝等级方差','ln(粉丝弹幕数量)','ln(非粉丝弹幕数量)']] Y= newDf[['收费礼物收入','免费礼物收入']] x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2) plt.rcParams['font.sans-serif']=['STSong'] plt.rcParams['axes.unicode_minus']=False plt.scatter(df.ln人气峰值,df.收费礼物收入,color = 'b',label = \"Exam Data\") plt.xlabel(\"x\") plt.ylabel(\"giftIncome\") plt.show() print(newDf.describe()) print(df[df.isnull()==True].count()) newDf.boxplot() plt.show() print(df.corr()) sns.pairplot(newDf, x_vars=['ln关注数','ln粉丝数','直播类型','ln人气峰值','用户等级方差','粉丝等级方差','ln(粉丝弹幕数量)','ln(非粉丝弹幕数量)'], y_vars=['收费礼物收入','免费礼物收入'],aspect=0.8,kind = 'reg') plt.show() model = LinearRegression() model.fit(x_train,y_train) a = model.intercept_ b = model.coef_ print(\"最佳拟合线:截距\",a,\",回归系数：\",b) score = model.score(x_test,y_test) print(score) Y_pred = model.predict(x_test) print(Y_pred) plt.plot(range(len(Y_pred)),Y_pred,'b',label=\"predict\") plt.show() plt.figure() plt.plot(range(len(Y_pred)),Y_pred,'b',label=\"predict\") plt.plot(range(len(Y_pred)),y_test,'r',label=\"test\") plt.legend(loc=\"upper right\") plt.xlabel(\"自变量\") plt.ylabel('礼物收入') plt.show()","categories":[{"name":"python","slug":"python","permalink":"https://blog.innnovation.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.innnovation.cn/tags/python/"},{"name":"线性回归","slug":"线性回归","permalink":"https://blog.innnovation.cn/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"}]}],"categories":[{"name":"Java","slug":"Java","permalink":"https://blog.innnovation.cn/categories/Java/"},{"name":"分布式","slug":"Java/分布式","permalink":"https://blog.innnovation.cn/categories/Java/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"操作系统","slug":"操作系统","permalink":"https://blog.innnovation.cn/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Spring","slug":"Java/Spring","permalink":"https://blog.innnovation.cn/categories/Java/Spring/"},{"name":"杂项","slug":"杂项","permalink":"https://blog.innnovation.cn/categories/%E6%9D%82%E9%A1%B9/"},{"name":"逆向分析","slug":"逆向分析","permalink":"https://blog.innnovation.cn/categories/%E9%80%86%E5%90%91%E5%88%86%E6%9E%90/"},{"name":"python","slug":"python","permalink":"https://blog.innnovation.cn/categories/python/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.innnovation.cn/tags/Java/"},{"name":"分布式","slug":"分布式","permalink":"https://blog.innnovation.cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"事务","slug":"事务","permalink":"https://blog.innnovation.cn/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"Seata","slug":"Seata","permalink":"https://blog.innnovation.cn/tags/Seata/"},{"name":"OS","slug":"OS","permalink":"https://blog.innnovation.cn/tags/OS/"},{"name":"Spring","slug":"Spring","permalink":"https://blog.innnovation.cn/tags/Spring/"},{"name":"WebFlux","slug":"WebFlux","permalink":"https://blog.innnovation.cn/tags/WebFlux/"},{"name":"杂项","slug":"杂项","permalink":"https://blog.innnovation.cn/tags/%E6%9D%82%E9%A1%B9/"},{"name":"GIT","slug":"GIT","permalink":"https://blog.innnovation.cn/tags/GIT/"},{"name":"逆向","slug":"逆向","permalink":"https://blog.innnovation.cn/tags/%E9%80%86%E5%90%91/"},{"name":"病毒分析","slug":"病毒分析","permalink":"https://blog.innnovation.cn/tags/%E7%97%85%E6%AF%92%E5%88%86%E6%9E%90/"},{"name":"Detour Hook","slug":"Detour-Hook","permalink":"https://blog.innnovation.cn/tags/Detour-Hook/"},{"name":"python","slug":"python","permalink":"https://blog.innnovation.cn/tags/python/"},{"name":"线性回归","slug":"线性回归","permalink":"https://blog.innnovation.cn/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"}]}